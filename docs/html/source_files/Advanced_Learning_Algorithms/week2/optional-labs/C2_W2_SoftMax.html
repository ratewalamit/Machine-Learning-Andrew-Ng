<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optional Lab - Softmax Function &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link href="../../../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Supervised_Machine_Learning_Regression_and_Classification/Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Optional Lab - Softmax Function</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/source_files/Advanced_Learning_Algorithms/week2/optional-labs/C2_W2_SoftMax.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Optional-Lab---Softmax-Function">
<h1>Optional Lab - Softmax Function<a class="headerlink" href="#Optional-Lab---Softmax-Function" title="Permalink to this headline"></a></h1>
<p>In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.</p>
<center><p><img alt="7601274629e846c8a35d3915ff1234a7" class="no-scaled-link" src="../../../../_images/C2_W2_Softmax_Header.PNG" style="width: 600px;" /></p>
<center/><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;./deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">Latex</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">from</span> <span class="nn">matplotlib.widgets</span> <span class="kn">import</span> <span class="n">Slider</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">dlc</span>
<span class="kn">from</span> <span class="nn">lab_utils_softmax</span> <span class="kn">import</span> <span class="n">plt_softmax</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<blockquote>
<div><p><strong>Note</strong>: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1, <span class="math notranslate nohighlight">\(\sum_{i=0}^{N-1}\)</span>, while lectures start with 1 and end with N, <span class="math notranslate nohighlight">\(\sum_{i=1}^{N}\)</span>. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus will break with the convention and will count 1 to N.</p>
</div></blockquote>
<section id="Softmax-Function">
<h2>Softmax Function<a class="headerlink" href="#Softmax-Function" title="Permalink to this headline"></a></h2>
<p>In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is generated by a linear function which is applied to a softmax function. The softmax function converts <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as
probabilities. The larger inputs will correspond to larger output probabilities.</p>
<center><p><img alt="0d8303215e064a93b489a779e008c779" class="no-scaled-link" src="../../../../_images/C2_W2_SoftmaxReg_NN.png" style="width: 600px;" /></p>
<p>The softmax function can be written:</p>
<div class="math notranslate nohighlight">
\[a_j = \frac{e^{z_j}}{ \sum_{k=1}^{N}{e^{z_k} }} \tag{1}\]</div>
<p>The output <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is a vector of length N, so for softmax regression, you could also write: <span class="math">\begin{align}
\mathbf{a}(x) =
\begin{bmatrix}
P(y = 1 | \mathbf{x}; \mathbf{w},b) \\
\vdots \\
P(y = N | \mathbf{x}; \mathbf{w},b)
\end{bmatrix}
=
\frac{1}{ \sum_{k=1}^{N}{e^{z_k} }}
\begin{bmatrix}
e^{z_1} \\
\vdots \\
e^{z_{N}} \\
\end{bmatrix} \tag{2}
\end{align}</span></p>
<div class="line-block">
<div class="line">Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</div>
<div class="line">Let’s create a NumPy implementation:</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">ez</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>              <span class="c1">#element-wise exponenial</span>
    <span class="n">sm</span> <span class="o">=</span> <span class="n">ez</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ez</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">sm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Below, vary the values of the <code class="docutils literal notranslate"><span class="pre">z</span></code> inputs using the sliders.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">plt_softmax</span><span class="p">(</span><span class="n">my_softmax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "cce96a6a945f4b63988352bac2bf0a85", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>As you are varying the values of the z’s above, there are a few things to note: * the exponential in the numerator of the softmax magnifies small differences in the values * the output values sum to one * the softmax spans all of the outputs. A change in <code class="docutils literal notranslate"><span class="pre">z0</span></code> for example will change the values of <code class="docutils literal notranslate"><span class="pre">a0</span></code>-<code class="docutils literal notranslate"><span class="pre">a3</span></code>. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.</p>
</section>
<section id="Cost">
<h2>Cost<a class="headerlink" href="#Cost" title="Permalink to this headline"></a></h2>
<center><p><img alt="e5e7a752b8e94f40b7ce34c708003795" class="no-scaled-link" src="../../../../_images/C2_W2_SoftMaxCost.png" style="width: 400px;" /></p>
<center/><dl>
<dt>The loss function associated with Softmax, the cross-entropy loss, is: :nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{equation}</dt><dd><dl>
<dt>L(mathbf{a},y)=begin{cases}</dt><dd><dl>
<dt>-log(a_1), &amp; text{if $y=1$}.\</dt><dd><blockquote>
<div><p>&amp;vdots\</p>
</div></blockquote>
<p>-log(a_N), &amp; text{if $y=N$}</p>
</dd>
</dl>
</dd>
</dl>
<p>end{cases} tag{3}</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>Where y is the target category for this example and <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is the output of a softmax function. In particular, the values in <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> are probabilities that sum to one. &gt;<strong>Recall:</strong> In this course, Loss is for one example while Cost covers all examples.</p>
<p>Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an ‘indicator function’ that will be 1 when the index matches the target and zero otherwise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{1}\{y == n\} = =\begin{cases}
    1, &amp; \text{if $y==n$}.\\
    0, &amp; \text{otherwise}.
  \end{cases}\end{split}\]</div>
<p>Now the cost is: <span class="math">\begin{align}
J(\mathbf{w},b) = -\frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{N}  1\left\{y^{(i)} == j\right\} \log \frac{e^{z^{(i)}_j}}{\sum_{k=1}^N e^{z^{(i)}_k} }\right] \tag{4}
\end{align}</span></p>
<p>Where <span class="math notranslate nohighlight">\(m\)</span> is the number of examples, <span class="math notranslate nohighlight">\(N\)</span> is the number of outputs. This is the average of all the losses.</p>
</section>
<section id="Tensorflow">
<h2>Tensorflow<a class="headerlink" href="#Tensorflow" title="Permalink to this headline"></a></h2>
<p>This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the ‘obvious’ method and the ‘preferred’ method. The former is the most straightforward while the latter is more numerically stable.</p>
<p>Let’s start by creating a dataset to train a multiclass classification model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make  dataset for example</span>
<span class="n">centers</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="The-Obvious-organization">
<h3>The <em>Obvious</em> organization<a class="headerlink" href="#The-Obvious-organization" title="Permalink to this headline"></a></h3>
<p>The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the <code class="docutils literal notranslate"><span class="pre">compile</span></code> directive.</p>
<p>The loss function is <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy</span></code>. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;softmax&#39;</span><span class="p">)</span>    <span class="c1"># &lt; softmax activation here</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 1/10
63/63 [==============================] - 0s 966us/step - loss: 0.8312
Epoch 2/10
63/63 [==============================] - 0s 1ms/step - loss: 0.3203
Epoch 3/10
63/63 [==============================] - 0s 1ms/step - loss: 0.1408
Epoch 4/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0847
Epoch 5/10
63/63 [==============================] - 0s 944us/step - loss: 0.0626
Epoch 6/10
63/63 [==============================] - 0s 974us/step - loss: 0.0515
Epoch 7/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0447
Epoch 8/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0402
Epoch 9/10
63/63 [==============================] - 0s 922us/step - loss: 0.0361
Epoch 10/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0332
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;keras.callbacks.History at 0x7fd399415490&gt;
</pre></div></div>
</div>
<p>Because the softmax is integrated into the output layer, the output is a vector of probabilities.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_nonpreferred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">p_nonpreferred</span> <span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;largest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">p_nonpreferred</span><span class="p">),</span> <span class="s2">&quot;smallest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">p_nonpreferred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[3.39e-03 1.02e-02 9.73e-01 1.30e-02]
 [9.97e-01 3.46e-03 9.35e-06 9.02e-06]]
largest value 0.9999994 smallest value 4.1378247e-09
</pre></div></div>
</div>
</section>
<section id="Preferred-66dea27cebeb4fc1b99a2006f15b5903">
<h3>Preferred <img alt="66dea27cebeb4fc1b99a2006f15b5903" src="../../../../_images/C2_W2_softmax_accurate.png" /><a class="headerlink" href="#Preferred-66dea27cebeb4fc1b99a2006f15b5903" title="Permalink to this headline"></a></h3>
<p>Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training. This is enabled by the ‘preferred’ organization shown here.</p>
<p>In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as <em>logits</em>. The loss function has an additional argument: <code class="docutils literal notranslate"><span class="pre">from_logits</span> <span class="pre">=</span> <span class="pre">True</span></code>. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preferred_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">)</span>   <span class="c1">#&lt;-- Note</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">preferred_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>  <span class="c1">#&lt;-- Note</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">preferred_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 1/10
63/63 [==============================] - 0s 1ms/step - loss: 1.0936
Epoch 2/10
63/63 [==============================] - 0s 1ms/step - loss: 0.4933
Epoch 3/10
63/63 [==============================] - 0s 1ms/step - loss: 0.2809
Epoch 4/10
63/63 [==============================] - 0s 1ms/step - loss: 0.1506
Epoch 5/10
63/63 [==============================] - 0s 966us/step - loss: 0.0916
Epoch 6/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0694
Epoch 7/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0592
Epoch 8/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0527
Epoch 9/10
63/63 [==============================] - 0s 919us/step - loss: 0.0489
Epoch 10/10
63/63 [==============================] - 0s 1ms/step - loss: 0.0457
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;keras.callbacks.History at 0x7fd23c552050&gt;
</pre></div></div>
</div>
<section id="Output-Handling">
<h4>Output Handling<a class="headerlink" href="#Output-Handling" title="Permalink to this headline"></a></h4>
<p>Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. Let’s look at the preferred model outputs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p_preferred</span> <span class="o">=</span> <span class="n">preferred_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;two example output vectors:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">p_preferred</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;largest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">p_preferred</span><span class="p">),</span> <span class="s2">&quot;smallest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">p_preferred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
two example output vectors:
 [[-2.72 -4.45  2.9  -1.37]
 [ 6.42  1.32 -1.32 -6.74]]
largest value 12.410254 smallest value -13.030992
</pre></div></div>
</div>
<p>The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax">softmax</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sm_preferred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">p_preferred</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;two example output vectors:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">sm_preferred</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;largest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">sm_preferred</span><span class="p">),</span> <span class="s2">&quot;smallest value&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">sm_preferred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
two example output vectors:
 [[3.55e-03 6.34e-04 9.82e-01 1.38e-02]
 [9.94e-01 6.05e-03 4.35e-04 1.92e-06]]
largest value 0.9999995 smallest value 1.5196424e-11
</pre></div></div>
</div>
<p>To select the most likely category, the softmax is not required. One can find the index of the largest output using <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html">np.argmax()</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p_preferred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, category: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p_preferred</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-2.72 -4.45  2.9  -1.37], category: 2
[ 6.42  1.32 -1.32 -6.74], category: 0
[ 4.64  1.5  -1.31 -5.3 ], category: 0
[-0.29  3.76 -3.11 -1.58], category: 1
[-0.64 -6.05  5.23 -5.2 ], category: 2
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="SparseCategorialCrossentropy-or-CategoricalCrossEntropy">
<h2>SparseCategorialCrossentropy or CategoricalCrossEntropy<a class="headerlink" href="#SparseCategorialCrossentropy-or-CategoricalCrossEntropy" title="Permalink to this headline"></a></h2>
<p>Tensorflow has two potential formats for target values and the selection of the loss defines which is expected. - SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. - CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values,
where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].</p>
</section>
<section id="Congratulations!">
<h2>Congratulations!<a class="headerlink" href="#Congratulations!" title="Permalink to this headline"></a></h2>
<p>In this lab you - Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks. - Learned the preferred model construction in Tensorflow: - No activation on the final layer (same as linear activation) - SparseCategoricalCrossentropy loss function - use from_logits=True - Recognized that unlike ReLU and Sigmoid, the softmax spans multiple outputs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>