<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced_Learning_Algorithms &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Supervised_Machine_Learning_Regression_and_Classification/Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Advanced_Learning_Algorithms</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/source_files/Advanced_Learning_Algorithms/weak1.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <hr style="height: 3px; background-color: black;"><section id="Advanced_Learning_Algorithms">
<h1>Advanced_Learning_Algorithms<a class="headerlink" href="#Advanced_Learning_Algorithms" title="Permalink to this headline"></a></h1>
<p><strong>(Convolutional Neural Networks)</strong></p>
<hr style="height: 3px; background-color: black;"><div style="margin-bottom: 4vh;"></div><section id="Module-1">
<h2>Module 1<a class="headerlink" href="#Module-1" title="Permalink to this headline"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">subprocess</span><span class="o">,</span><span class="nn">os</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="n">home_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">())</span>
<span class="n">proj_path</span><span class="o">=</span><span class="n">home_path</span><span class="o">+</span><span class="s2">&quot;/my_web/Machine-Learning-Andrew-Ng/source/source_files/Advanced_Learning_Algorithms&quot;</span>
<span class="n">week1</span><span class="o">=</span><span class="n">proj_path</span><span class="o">+</span><span class="s2">&quot;/week1&quot;</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">proj_path</span><span class="si">}</span><span class="s2">/week1/C1W3A1&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">proj_path</span><span class="si">}</span><span class="s2">/week1/optional-labs&quot;</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">proj_path</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<section id="Optional-Lab---Neurons-and-Layers">
<h3>Optional Lab - Neurons and Layers<a class="headerlink" href="#Optional-Lab---Neurons-and-Layers" title="Permalink to this headline"></a></h3>
<p>In this lab we will explore the inner workings of neurons/units and layers. In particular, the lab will draw parallels to the models you have mastered in Course 1, the regression/linear model and the logistic model. The lab will introduce Tensorflow and demonstrate how these models are implemented in that framework.</p>
<figure><p><img alt="d7fe83774d164fd0ae9bb24901f4e62b" src="../../_images/C2_W1_NeuronsAndLayers.png" /></p>
</figure></section>
</section>
<section id="Packages">
<h2>Packages<a class="headerlink" href="#Packages" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line"><strong>Tensorflow and Keras</strong></div>
<div class="line">Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by François Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.losses</span> <span class="kn">import</span> <span class="n">MeanSquaredError</span><span class="p">,</span> <span class="n">BinaryCrossentropy</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.activations</span> <span class="kn">import</span> <span class="n">sigmoid</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">dlc</span>
<span class="kn">from</span> <span class="nn">lab_neurons_utils</span> <span class="kn">import</span> <span class="n">plt_prob_1d</span><span class="p">,</span> <span class="n">sigmoidnp</span><span class="p">,</span> <span class="n">plt_linear</span><span class="p">,</span> <span class="n">plt_logistic</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week1/optional-labs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">FileNotFoundError</span>                         Traceback (most recent call last)
File <span class="ansi-green-fg">/usr/lib/python3/dist-packages/matplotlib/style/core.py:127</span>, in <span class="ansi-cyan-fg">use</span><span class="ansi-blue-fg">(style)</span>
<span class="ansi-green-intense-fg ansi-bold">    126</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 127</span>     rc <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">rc_params_from_file</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">style</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">use_default_template</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg ansi-bold" style="color: rgb(0,135,0)">False</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    128</span>     _apply_style(rc)

File <span class="ansi-green-fg">/usr/lib/python3/dist-packages/matplotlib/__init__.py:856</span>, in <span class="ansi-cyan-fg">rc_params_from_file</span><span class="ansi-blue-fg">(fname, fail_on_error, use_default_template)</span>
<span class="ansi-green-intense-fg ansi-bold">    842</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    843</span> <span style="color: rgb(175,0,0)">Construct a `RcParams` from file *fname*.</span>
<span class="ansi-green-intense-fg ansi-bold">    844</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">    854</span> <span style="color: rgb(175,0,0)">    parameters specified in the file. (Useful for updating dicts.)</span>
<span class="ansi-green-intense-fg ansi-bold">    855</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-fg">--&gt; 856</span> config_from_file <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_rc_params_in_file</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">fname</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">fail_on_error</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg">fail_on_error</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    858</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> use_default_template:

File <span class="ansi-green-fg">/usr/lib/python3/dist-packages/matplotlib/__init__.py:782</span>, in <span class="ansi-cyan-fg">_rc_params_in_file</span><span class="ansi-blue-fg">(fname, transform, fail_on_error)</span>
<span class="ansi-green-intense-fg ansi-bold">    781</span> rc_temp <span style="color: rgb(98,98,98)">=</span> {}
<span class="ansi-green-fg">--&gt; 782</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> _open_file_or_url(fname) <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> fd:
<span class="ansi-green-intense-fg ansi-bold">    783</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:

File <span class="ansi-green-fg">/usr/lib/python3.10/contextlib.py:135</span>, in <span class="ansi-cyan-fg">_GeneratorContextManager.__enter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    134</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 135</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span class="ansi-yellow-bg" style="color: rgb(0,135,0)">next</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg" style="color: rgb(0,135,0)">self</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">.</span><span class="ansi-yellow-bg">gen</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    136</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">StopIteration</span>:

File <span class="ansi-green-fg">/usr/lib/python3/dist-packages/matplotlib/__init__.py:759</span>, in <span class="ansi-cyan-fg">_open_file_or_url</span><span class="ansi-blue-fg">(fname)</span>
<span class="ansi-green-intense-fg ansi-bold">    758</span>     encoding <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">utf-8</span><span style="color: rgb(175,0,0)">&#34;</span>
<span class="ansi-green-fg">--&gt; 759</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> <span class="ansi-yellow-bg" style="color: rgb(0,135,0)">open</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">fname</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg">encoding</span><span class="ansi-yellow-bg">)</span> <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> f:
<span class="ansi-green-intense-fg ansi-bold">    760</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">yield</span> f

<span class="ansi-red-fg">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;./deeplearning.mplstyle&#39;

The above exception was the direct cause of the following exception:

<span class="ansi-red-fg">OSError</span>                                   Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[18], line 8</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">tensorflow</span><span class="ansi-bold" style="color: rgb(0,0,255)">.</span><span class="ansi-bold" style="color: rgb(0,0,255)">keras</span><span class="ansi-bold" style="color: rgb(0,0,255)">.</span><span class="ansi-bold" style="color: rgb(0,0,255)">losses</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> MeanSquaredError, BinaryCrossentropy
<span class="ansi-green-intense-fg ansi-bold">      7</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">tensorflow</span><span class="ansi-bold" style="color: rgb(0,0,255)">.</span><span class="ansi-bold" style="color: rgb(0,0,255)">keras</span><span class="ansi-bold" style="color: rgb(0,0,255)">.</span><span class="ansi-bold" style="color: rgb(0,0,255)">activations</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> sigmoid
<span class="ansi-green-fg">----&gt; 8</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">lab_utils_common</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> dlc
<span class="ansi-green-intense-fg ansi-bold">      9</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">lab_neurons_utils</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> plt_prob_1d, sigmoidnp, plt_linear, plt_logistic
<span class="ansi-green-intense-fg ansi-bold">     10</span> plt<span style="color: rgb(98,98,98)">.</span>style<span style="color: rgb(98,98,98)">.</span>use(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">week1/optional-labs/deeplearning.mplstyle</span><span style="color: rgb(175,0,0)">&#39;</span>)

File <span class="ansi-green-fg">~/my_web/Machine-Learning-Andrew-Ng/source/source_files/Advanced_Learning_Algorithms/week1/optional-labs/lab_utils_common.py:22</span>
<span class="ansi-green-intense-fg ansi-bold">     20</span> dlblue <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#0096ff</span><span style="color: rgb(175,0,0)">&#39;</span>; dlorange <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#FF9300</span><span style="color: rgb(175,0,0)">&#39;</span>; dldarkred<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#C00000</span><span style="color: rgb(175,0,0)">&#39;</span>; dlmagenta<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#FF40FF</span><span style="color: rgb(175,0,0)">&#39;</span>; dlpurple<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#7030A0</span><span style="color: rgb(175,0,0)">&#39;</span>; dldarkblue <span style="color: rgb(98,98,98)">=</span>  <span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">#0D5BDC</span><span style="color: rgb(175,0,0)">&#39;</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> dlcolors <span style="color: rgb(98,98,98)">=</span> [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]
<span class="ansi-green-fg">---&gt; 22</span> <span class="ansi-yellow-bg">plt</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">.</span><span class="ansi-yellow-bg">style</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">.</span><span class="ansi-yellow-bg">use</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#39;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">./deeplearning.mplstyle</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#39;</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     24</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">sigmoid</span>(z):
<span class="ansi-green-intense-fg ansi-bold">     25</span> <span style="color: rgb(188,188,188)">    </span><span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span> <span style="color: rgb(175,0,0)">    Compute the sigmoid of z</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">     36</span> <span style="color: rgb(175,0,0)">         sigmoid(z)</span>
<span class="ansi-green-intense-fg ansi-bold">     37</span> <span style="color: rgb(175,0,0)">    &#34;&#34;&#34;</span>

File <span class="ansi-green-fg">/usr/lib/python3/dist-packages/matplotlib/style/core.py:130</span>, in <span class="ansi-cyan-fg">use</span><span class="ansi-blue-fg">(style)</span>
<span class="ansi-green-intense-fg ansi-bold">    128</span>     _apply_style(rc)
<span class="ansi-green-intense-fg ansi-bold">    129</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">IOError</span> <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> err:
<span class="ansi-green-fg">--&gt; 130</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">IOError</span>(
<span class="ansi-green-intense-fg ansi-bold">    131</span>         <span style="color: rgb(175,0,0)">&#34;</span><span class="ansi-bold" style="color: rgb(175,95,135)">{!r}</span><span style="color: rgb(175,0,0)"> not found in the style library and input is not a </span><span style="color: rgb(175,0,0)">&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    132</span>         <span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">valid URL or path; see `style.available` for list of </span><span style="color: rgb(175,0,0)">&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    133</span>         <span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">available styles</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(98,98,98)">.</span>format(style)) <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">err</span>

<span class="ansi-red-fg">OSError</span>: &#39;./deeplearning.mplstyle&#39; not found in the style library and input is not a valid URL or path; see `style.available` for list of available styles
</pre></div></div>
</div>
</section>
<section id="Neuron-without-activation---Regression/Linear-Model">
<h2>Neuron without activation - Regression/Linear Model<a class="headerlink" href="#Neuron-without-activation---Regression/Linear-Model" title="Permalink to this headline"></a></h2>
<section id="DataSet">
<h3>DataSet<a class="headerlink" href="#DataSet" title="Permalink to this headline"></a></h3>
<p>We’ll use an example from Course 1, linear regression on house prices.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>           <span class="c1">#(size in 1000 square feet)</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">300.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">500.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>       <span class="c1">#(price in 1000s of dollars)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Data Points&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Price (in 1000s of dollars)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Size (1000 sqft)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_11_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_11_0.png" />
</div>
</div>
</section>
<section id="Regression/Linear-Model">
<h3>Regression/Linear Model<a class="headerlink" href="#Regression/Linear-Model" title="Permalink to this headline"></a></h3>
<p>The function implemented by a neuron with no activation is the same as in Course 1, linear regression:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x^{(i)}) = \mathbf{w}\cdot x^{(i)} + b \tag{1}\]</div>
<p>We can define a layer with one neuron or unit and compare it to the familiar linear regression function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s examine the weights.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[]
</pre></div></div>
</div>
<p>There are no weights as the weights are not yet instantiated. Let’s try the model on one example in <code class="docutils literal notranslate"><span class="pre">X_train</span></code>. This will trigger the instantiation of the weights. Note, the input to the layer must be 2-D, so we’ll reshape it.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a1</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tf.Tensor([[1.37]], shape=(1, 1), dtype=float32)
</pre></div></div>
</div>
<div class="line-block">
<div class="line">The result is a tensor (another name for an array) with a shape of (1,1) or one entry.</div>
<div class="line">Now let’s look at the weights and bias. These weights are randomly initialized to small numbers and the bias defaults to being initialized to zero.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span> <span class="n">linear_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;w = </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2">, b=</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
w = [[1.37]], b=[0.]
</pre></div></div>
</div>
<p>A linear regression model (1) with a single input feature will have a single weight and bias. This matches the dimensions of our <code class="docutils literal notranslate"><span class="pre">linear_layer</span></code> above.</p>
<p>The weights are initialized to random values so let’s set them to some known values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">200</span><span class="p">]])</span>
<span class="n">set_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>

<span class="c1"># set_weights takes a list of numpy arrays</span>
<span class="n">linear_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">set_w</span><span class="p">,</span> <span class="n">set_b</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[array([[200.]], dtype=float32), array([100.], dtype=float32)]
</pre></div></div>
</div>
<p>Let’s compare equation (1) to the layer output.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a1</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
<span class="n">alin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">set_w</span><span class="p">,</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">set_b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alin</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tf.Tensor([[300.]], shape=(1, 1), dtype=float32)
[[300.]]
</pre></div></div>
</div>
<p>They produce the same values! Now, we can use our linear layer to make predictions on our training data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction_tf</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">prediction_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">set_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">set_b</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_linear</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">prediction_tf</span><span class="p">,</span> <span class="n">prediction_np</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_27_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_27_0.png" />
</div>
</div>
</section>
</section>
<section id="Neuron-with-Sigmoid-activation">
<h2>Neuron with Sigmoid activation<a class="headerlink" href="#Neuron-with-Sigmoid-activation" title="Permalink to this headline"></a></h2>
<p>The function implemented by a neuron/unit with a sigmoid activation is the same as in Course 1, logistic regression:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x^{(i)}) = g(\mathbf{w}x^{(i)} + b) \tag{2}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[g(x) = sigmoid(x)\]</div>
<p>Let’s set <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to some known values and check the model.</p>
<section id="id1">
<h3>DataSet<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>We’ll use an example from Course 1, logistic regression.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 2-D Matrix</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 2-D Matrix</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">Y_train</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">neg</span> <span class="o">=</span> <span class="n">Y_train</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">X_train</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([3., 4., 5.], dtype=float32)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">Y_train</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">neg</span> <span class="o">=</span> <span class="n">Y_train</span> <span class="o">==</span> <span class="mi">0</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y=1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">neg</span><span class="p">],</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">neg</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y=0&quot;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span>
              <span class="n">edgecolors</span><span class="o">=</span><span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;one variable plot&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_32_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_32_0.png" />
</div>
</div>
</section>
<section id="Logistic-Neuron">
<h3>Logistic Neuron<a class="headerlink" href="#Logistic-Neuron" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line">We can implement a ‘logistic neuron’ by adding a sigmoid activation. The function of the neuron is then described by (2) above.</div>
<div class="line">This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The <a class="reference external" href="https://keras.io/guides/sequential_model/">Sequential</a> model is a convenient means of constructing these models.</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;L1&#39;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;sequential&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 L1 (Dense)                  (None, 1)                 2

=================================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">&#39;L1&#39;</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">logistic_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.11]] [0.]
(1, 1) (1,)
</pre></div></div>
</div>
<p>Let’s set the weight and bias to some known values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">set_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.5</span><span class="p">])</span>
<span class="c1"># set_weights takes a list of numpy arrays</span>
<span class="n">logistic_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">set_w</span><span class="p">,</span> <span class="n">set_b</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logistic_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[array([[2.]], dtype=float32), array([-4.5], dtype=float32)]
</pre></div></div>
</div>
<p>Let’s compare equation (2) to the layer output.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
<span class="n">alog</span> <span class="o">=</span> <span class="n">sigmoidnp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">set_w</span><span class="p">,</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">set_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">alog</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[0.01]]
[[0.01]]
</pre></div></div>
</div>
<p>They produce the same values! Now, we can use our logistic layer and NumPy model to make predictions on our training data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_logistic</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">set_w</span><span class="p">,</span> <span class="n">set_b</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">neg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_43_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_43_0.png" />
</div>
</div>
<p>The shading above reflects the output of the sigmoid which varies from 0 to 1.</p>
</section>
</section>
</section>
<section id="Congratulations!">
<h1>Congratulations!<a class="headerlink" href="#Congratulations!" title="Permalink to this headline"></a></h1>
<p>You built a very simple neural network and have explored the similarities of a neuron to the linear and logistic regression from Course 1.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
<section id="Optional-Lab---Simple-Neural-Network">
<h1>Optional Lab - Simple Neural Network<a class="headerlink" href="#Optional-Lab---Simple-Neural-Network" title="Permalink to this headline"></a></h1>
<p>In this lab we will build a small neural network using Tensorflow.</p>
<center><p><img alt="5f1ca7473b1b4eb4875981754edb8e46" class="no-scaled-link" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_CoffeeRoasting.png" style="width: 400px;" /></p>
<center/><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;./deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">dlc</span>
<span class="kn">from</span> <span class="nn">lab_coffee_utils</span> <span class="kn">import</span> <span class="n">load_coffee_data</span><span class="p">,</span> <span class="n">plt_roast</span><span class="p">,</span> <span class="n">plt_prob</span><span class="p">,</span> <span class="n">plt_layer</span><span class="p">,</span> <span class="n">plt_network</span><span class="p">,</span> <span class="n">plt_output_unit</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<section id="id2">
<h2>DataSet<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">Y</span> <span class="o">=</span> <span class="n">load_coffee_data</span><span class="p">();</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(200, 2) (200, 1)
</pre></div></div>
</div>
<p>Let’s plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. <a class="reference external" href="https://www.merchantsofgreencoffee.com/how-to-roast-green-coffee-in-your-oven/">Coffee Roasting at Home</a> suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius. Of course, as temperature rises, the duration should shrink.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_roast</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_53_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_53_0.png" />
</div>
</div>
<section id="Normalize-Data">
<h3>Normalize Data<a class="headerlink" href="#Normalize-Data" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line">Fitting the weights to the data (back-propagation, covered in next week’s lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. The procedure below uses a Keras <a class="reference external" href="https://keras.io/api/layers/preprocessing_layers/numerical/normalization/">normalization layer</a>. It has the following steps: - create a “Normalization Layer”. Note, as applied here, this is not a layer in
your model. - ‘adapt’ the data. This learns the mean and variance of the data set and saves the values internally. - normalize the data.</div>
<div class="line">It is important to apply normalization to any future data that utilizes the learned model.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature Max, Min pre normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duration    Max, Min pre normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">norm_l</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Normalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">norm_l</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># learns mean, variance</span>
<span class="n">Xn</span> <span class="o">=</span> <span class="n">norm_l</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature Max, Min post normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duration    Max, Min post normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Temperature Max, Min pre normalization: 284.99, 151.32
Duration    Max, Min pre normalization: 15.45, 11.51
Temperature Max, Min post normalization: 1.66, -1.69
Duration    Max, Min post normalization: 1.79, -1.70
</pre></div></div>
</div>
<p>Tile/copy our data to increase the training set size and reduce the number of training epochs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">Xn</span><span class="p">,(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Yt</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">Y</span><span class="p">,(</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Yt</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(200000, 2) (200000, 1)
</pre></div></div>
</div>
</section>
</section>
<section id="Tensorflow-Model">
<h2>Tensorflow Model<a class="headerlink" href="#Tensorflow-Model" title="Permalink to this headline"></a></h2>
<section id="Model">
<h3>Model<a class="headerlink" href="#Model" title="Permalink to this headline"></a></h3>
<center><p><img alt="8b8ef8b8021a45ee94393c05238c16b9" class="no-scaled-link" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_RoastingNetwork.PNG" style="width: 200px;" /></p>
<center/><p>Let’s build the “Coffee Roasting Network” described in lecture. There are two layers with sigmoid activations as shown below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>  <span class="c1"># applied to achieve consistent results</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer1&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;layer2&#39;</span><span class="p">)</span>
     <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Note 1:</strong> The <code class="docutils literal notranslate"><span class="pre">tf.keras.Input(shape=(2,)),</span></code> specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point. This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the <code class="docutils literal notranslate"><span class="pre">model.fit</span></code> statement.</div>
<div class="line"><strong>Note 2:</strong> Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.</div>
</div>
</div></blockquote>
<p>The <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> provides a description of the network:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;sequential&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 layer1 (Dense)              (None, 3)                 9

 layer2 (Dense)              (None, 1)                 4

=================================================================
Total params: 13
Trainable params: 13
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<p>The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays as shown below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L1_num_params</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span>   <span class="c1"># W1 parameters  + b1 parameters</span>
<span class="n">L2_num_params</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span>   <span class="c1"># W2 parameters  + b2 parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L1 params = &quot;</span><span class="p">,</span> <span class="n">L1_num_params</span><span class="p">,</span> <span class="s2">&quot;, L2 params = &quot;</span><span class="p">,</span> <span class="n">L2_num_params</span>  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
L1 params =  9 , L2 params =  4
</pre></div></div>
</div>
<p>Let’s examine the weights and biases Tensorflow has instantiated. The weights <span class="math notranslate nohighlight">\(W\)</span> should be of size (number of features in input, number of units in the layer) while the bias <span class="math notranslate nohighlight">\(b\)</span> size should match the number of units in the layer: - In the first layer with 3 units, we expect W to have a size of (2,3) and <span class="math notranslate nohighlight">\(b\)</span> should have 3 elements. - In the second layer with 1 unit, we expect W to have a size of (3,1) and <span class="math notranslate nohighlight">\(b\)</span> should have 1 element.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="n">W2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W1</span><span class="si">{</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">b1</span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W2</span><span class="si">{</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">b2</span><span class="si">{</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
W1(2, 3):
 [[ 0.08 -0.3   0.18]
 [-0.56 -0.15  0.89]]
b1(3,): [0. 0. 0.]
W2(3, 1):
 [[-0.43]
 [-0.88]
 [ 0.36]]
b2(1,): [0.]
</pre></div></div>
</div>
<p>The following statements will be described in detail in Week2. For now: - The <code class="docutils literal notranslate"><span class="pre">model.compile</span></code> statement defines a loss function and specifies a compile optimization. - The <code class="docutils literal notranslate"><span class="pre">model.fit</span></code> statement runs gradient descent and fits the weights to the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(),</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">Xt</span><span class="p">,</span><span class="n">Yt</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 1/10
6250/6250 [==============================] - 5s 768us/step - loss: 0.1782
Epoch 2/10
6250/6250 [==============================] - 5s 781us/step - loss: 0.1165
Epoch 3/10
6250/6250 [==============================] - 5s 792us/step - loss: 0.0426
Epoch 4/10
6250/6250 [==============================] - 5s 791us/step - loss: 0.0160
Epoch 5/10
6250/6250 [==============================] - 5s 787us/step - loss: 0.0104
Epoch 6/10
6250/6250 [==============================] - 5s 795us/step - loss: 0.0073
Epoch 7/10
6250/6250 [==============================] - 5s 787us/step - loss: 0.0052
Epoch 8/10
6250/6250 [==============================] - 5s 789us/step - loss: 0.0037
Epoch 9/10
6250/6250 [==============================] - 5s 780us/step - loss: 0.0027
Epoch 10/10
6250/6250 [==============================] - 5s 773us/step - loss: 0.0020
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;keras.callbacks.History at 0x7f5edc665990&gt;
</pre></div></div>
</div>
<section id="Updated-Weights">
<h4>Updated Weights<a class="headerlink" href="#Updated-Weights" title="Permalink to this headline"></a></h4>
<p>After fitting, the weights have been updated:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="n">W2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">b1:&quot;</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">b2:&quot;</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
W1:
 [[ -0.13  14.3  -11.1 ]
 [ -8.92  11.85  -0.25]]
b1: [-11.16   1.76 -12.1 ]
W2:
 [[-45.71]
 [-42.95]
 [-50.19]]
b2: [26.14]
</pre></div></div>
</div>
<p>Next, we will load some saved weights from a previous training run. This is so that this notebook remains robust to changes in Tensorflow over time. Different training runs can produce somewhat different results and the discussion below applies to a particular solution. Feel free to re-run the notebook with this cell commented out to see the difference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">8.94</span><span class="p">,</span>  <span class="mf">0.29</span><span class="p">,</span> <span class="mf">12.89</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.17</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.34</span><span class="p">,</span> <span class="mf">10.79</span><span class="p">]]</span> <span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">9.87</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.28</span><span class="p">,</span>  <span class="mf">1.01</span><span class="p">])</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">31.38</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">27.86</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">32.79</span><span class="p">]])</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">15.54</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span><span class="n">b1</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;layer2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">W2</span><span class="p">,</span><span class="n">b2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Predictions">
<h3>Predictions<a class="headerlink" href="#Predictions" title="Permalink to this headline"></a></h3>
<p><img alt="d59cc875f8cc495ca6dca878cf79703f" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_RoastingDecision.PNG" /></p>
<p>Once you have a trained model, you can then use it to make predictions. Recall that the output of our model is a probability. In this case, the probability of a good roast. To make a decision, one must apply the probability to a threshold. In this case, we will use 0.5</p>
<div class="line-block">
<div class="line">Let’s start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples. Recall, we have normalized the input features so we must normalize our test data as well.</div>
<div class="line">To make a prediction, you apply the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">200</span><span class="p">,</span><span class="mf">13.9</span><span class="p">],</span>  <span class="c1"># postive example</span>
    <span class="p">[</span><span class="mi">200</span><span class="p">,</span><span class="mi">17</span><span class="p">]])</span>   <span class="c1"># negative example</span>
<span class="n">X_testn</span> <span class="o">=</span> <span class="n">norm_l</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_testn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predictions = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
predictions =
 [[9.63e-01]
 [3.03e-08]]
</pre></div></div>
</div>
<section id="Epochs-and-batches">
<h4>Epochs and batches<a class="headerlink" href="#Epochs-and-batches" title="Permalink to this headline"></a></h4>
<p>In the <code class="docutils literal notranslate"><span class="pre">compile</span></code> statement above, the number of <code class="docutils literal notranslate"><span class="pre">epochs</span></code> was set to 10. This specifies that the entire data set should be applied during training 10 times. During training, you see output describing the progress of training that looks like this:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
6250/6250 [==============================] - 6s 910us/step - loss: 0.1782
</pre></div>
</div>
<p>The first line, <code class="docutils literal notranslate"><span class="pre">Epoch</span> <span class="pre">1/10</span></code>, describes which epoch the model is currently running. For efficiency, the training data set is broken into ‘batches’. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line <code class="docutils literal notranslate"><span class="pre">6250/6250</span> <span class="pre">[====</span></code> is describing which batch has been executed.</p>
<p>To convert the probabilities to a decision, we apply a threshold:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decisions = </span><span class="se">\n</span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
decisions =
[[1.]
 [0.]]
</pre></div></div>
</div>
<p>This can be accomplished more succinctly:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decisions = </span><span class="se">\n</span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
decisions =
[[1]
 [0]]
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="Layer-Functions">
<h2>Layer Functions<a class="headerlink" href="#Layer-Functions" title="Permalink to this headline"></a></h2>
<p>Let’s examine the functions of the units to determine their role in the coffee roasting decision. We will plot the output of each node for all values of the inputs (duration,temp). Each unit is a logistic function whose output can range from zero to one. The shading in the graph represents the output value. &gt; Note: In labs we typically number things starting at zero while the lectures may start with 1.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_layer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span><span class="n">W1</span><span class="p">,</span><span class="n">b1</span><span class="p">,</span><span class="n">norm_l</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_82_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_82_0.png" />
</div>
</div>
<p>The shading shows that each unit is responsible for a different “bad roast” region. unit 0 has larger values when the temperature is too low. unit 1 has larger values when the duration is too short and unit 2 has larger values for bad combinations of time/temp. It is worth noting that the network learned these functions on its own through the process of gradient descent. They are very much the same sort of functions a person might choose to make the same decisions.</p>
<p>The function plot of the final layer is a bit more difficult to visualize. It’s inputs are the output of the first layer. We know that the first layer uses sigmoids so their output range is between zero and one. We can create a 3-D plot that calculates the output for all possible combinations of the three inputs. This is shown below. Above, high output values correspond to ‘bad roast’ area’s. Below, the maximum output is in area’s where the three inputs are small values corresponding to ‘good
roast’ area’s.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_output_unit</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span><span class="n">b2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_85_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_85_0.png" />
</div>
</div>
<div class="line-block">
<div class="line">The final graph shows the whole network in action.</div>
<div class="line">The left graph is the raw output of the final layer represented by the blue shading. This is overlaid on the training data represented by the X’s and O’s.</div>
<div class="line">The right graph is the output of the network after a decision threshold. The X’s and O’s here correspond to decisions made by the network.</div>
<div class="line">The following takes a moment to run</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">netf</span><span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">norm_l</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt_network</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">netf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_87_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_87_0.png" />
</div>
</div>
</section>
<section id="id3">
<h2>Congratulations!<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>You have built a small neural network in Tensorflow. The network demonstrated the ability of neural networks to handle complex decisions by dividing the decisions between multiple units.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="id4">
<h1>Optional Lab - Simple Neural Network<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h1>
<p>In this lab, we will build a small neural network using Numpy. It will be the same “coffee roasting” network you implemented in Tensorflow.</p>
<center><p><img alt="6291a16a57ac41ce97b63cc6563a7dac" class="no-scaled-link" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_CoffeeRoasting.png" style="width: 400px;" /></p>
<center/><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;./deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">dlc</span><span class="p">,</span> <span class="n">sigmoid</span>
<span class="kn">from</span> <span class="nn">lab_coffee_utils</span> <span class="kn">import</span> <span class="n">load_coffee_data</span><span class="p">,</span> <span class="n">plt_roast</span><span class="p">,</span> <span class="n">plt_prob</span><span class="p">,</span> <span class="n">plt_layer</span><span class="p">,</span> <span class="n">plt_network</span><span class="p">,</span> <span class="n">plt_output_unit</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">autograph</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="id5">
<h2>DataSet<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<p>This is the same data set as the previous lab.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">Y</span> <span class="o">=</span> <span class="n">load_coffee_data</span><span class="p">();</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(200, 2) (200, 1)
</pre></div></div>
</div>
<p>Let’s plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. <a class="reference external" href="https://www.merchantsofgreencoffee.com/how-to-roast-green-coffee-in-your-oven/">Coffee Roasting at Home</a> suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius. Of course, as the temperature rises, the duration should shrink.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_roast</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_95_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_95_0.png" />
</div>
</div>
<section id="id6">
<h3>Normalize Data<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h3>
<p>To match the previous lab, we’ll normalize the data. Refer to that lab for more details</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature Max, Min pre normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duration    Max, Min pre normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">norm_l</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Normalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">norm_l</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># learns mean, variance</span>
<span class="n">Xn</span> <span class="o">=</span> <span class="n">norm_l</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature Max, Min post normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duration    Max, Min post normalization: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Xn</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Temperature Max, Min pre normalization: 284.99, 151.32
Duration    Max, Min pre normalization: 15.45, 11.51
Temperature Max, Min post normalization: 1.66, -1.69
Duration    Max, Min post normalization: 1.79, -1.70
</pre></div></div>
</div>
</section>
</section>
<section id="Numpy-Model-(Forward-Prop-in-NumPy)">
<h2>Numpy Model (Forward Prop in NumPy)<a class="headerlink" href="#Numpy-Model-(Forward-Prop-in-NumPy)" title="Permalink to this headline"></a></h2>
<center><p><img alt="10b5e45d7e5e4eb683b422a2e17850ee" class="no-scaled-link" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_RoastingNetwork.PNG" style="width: 200px;" /></p>
<center/><p>Let’s build the “Coffee Roasting Network” described in lecture. There are two layers with sigmoid activations.</p>
<p>As described in lecture, it is possible to build your own dense layer using NumPy. This can then be utilized to build a multi-layer neural network.</p>
<p><img alt="82d13f6e7a5948099d30c72ee4ad5a4d" class="no-scaled-link" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_dense2.PNG" style="width: 600px; height: 450px;" /></p>
<p>In the first optional lab, you constructed a neuron in NumPy and in Tensorflow and noted their similarity. A layer simply contains multiple neurons/units. As described in lecture, one can utilize a for loop to visit each unit (<code class="docutils literal notranslate"><span class="pre">j</span></code>) in the layer and perform the dot product of the weights for that unit (<code class="docutils literal notranslate"><span class="pre">W[:,j]</span></code>) and sum the bias for the unit (<code class="docutils literal notranslate"><span class="pre">b[j]</span></code>) to form <code class="docutils literal notranslate"><span class="pre">z</span></code>. An activation function <code class="docutils literal notranslate"><span class="pre">g(z)</span></code> can then be applied to that result. Let’s try that below to build a “dense layer” subroutine.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_dense</span><span class="p">(</span><span class="n">a_in</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes dense layer</span>
<span class="sd">    Args:</span>
<span class="sd">      a_in (ndarray (n, )) : Data, 1 example</span>
<span class="sd">      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units</span>
<span class="sd">      b    (ndarray (j, )) : bias vector, j units</span>
<span class="sd">      g    activation function (e.g. sigmoid, relu..)</span>
<span class="sd">    Returns</span>
<span class="sd">      a_out (ndarray (j,))  : j units|</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">units</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">a_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">units</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">units</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">W</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">a_in</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">a_out</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">a_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The following cell builds a two-layer neural network utilizing the <code class="docutils literal notranslate"><span class="pre">my_dense</span></code> subroutine above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_sequential</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">my_dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>  <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">my_dense</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can copy trained weights and biases from the previous lab in Tensorflow.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W1_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[[</span><span class="o">-</span><span class="mf">8.93</span><span class="p">,</span>  <span class="mf">0.29</span><span class="p">,</span> <span class="mf">12.9</span> <span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span>  <span class="o">-</span><span class="mf">7.32</span><span class="p">,</span> <span class="mf">10.81</span><span class="p">]]</span> <span class="p">)</span>
<span class="n">b1_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span><span class="o">-</span><span class="mf">9.82</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.28</span><span class="p">,</span>  <span class="mf">0.96</span><span class="p">]</span> <span class="p">)</span>
<span class="n">W2_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[[</span><span class="o">-</span><span class="mf">31.18</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">27.59</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">32.56</span><span class="p">]]</span> <span class="p">)</span>
<span class="n">b2_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span><span class="mf">15.41</span><span class="p">]</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<section id="id7">
<h3>Predictions<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<p><img alt="566d069bc43d4e0995b07443af9d668b" src="source_files/Advanced_Learning_Algorithms/images/C2_W1_RoastingDecision.PNG" /></p>
<p>Once you have a trained model, you can then use it to make predictions. Recall that the output of our model is a probability. In this case, the probability of a good roast. To make a decision, one must apply the probability to a threshold. In this case, we will use 0.5</p>
<p>Let’s start by writing a routine similar to Tensorflow’s <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code>. This will take a matrix <span class="math notranslate nohighlight">\(X\)</span> with all <span class="math notranslate nohighlight">\(m\)</span> examples in the rows and make a prediction by running the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_sequential</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can try this routine on two examples:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_tst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">200</span><span class="p">,</span><span class="mf">13.9</span><span class="p">],</span>  <span class="c1"># postive example</span>
    <span class="p">[</span><span class="mi">200</span><span class="p">,</span><span class="mi">17</span><span class="p">]])</span>   <span class="c1"># negative example</span>
<span class="n">X_tstn</span> <span class="o">=</span> <span class="n">norm_l</span><span class="p">(</span><span class="n">X_tst</span><span class="p">)</span>  <span class="c1"># remember to normalize</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">my_predict</span><span class="p">(</span><span class="n">X_tstn</span><span class="p">,</span> <span class="n">W1_tmp</span><span class="p">,</span> <span class="n">b1_tmp</span><span class="p">,</span> <span class="n">W2_tmp</span><span class="p">,</span> <span class="n">b2_tmp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To convert the probabilities to a decision, we apply a threshold:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">yhat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decisions = </span><span class="se">\n</span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
decisions =
[[1.]
 [0.]]
</pre></div></div>
</div>
<p>This can be accomplished more succinctly:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decisions = </span><span class="se">\n</span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
decisions =
[[1]
 [0]]
</pre></div></div>
</div>
</section>
</section>
<section id="Network-function">
<h2>Network function<a class="headerlink" href="#Network-function" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line">This graph shows the operation of the whole network and is identical to the Tensorflow result from the previous lab. The left graph is the raw output of the final layer represented by the blue shading. This is overlaid on the training data represented by the X’s and O’s.</div>
<div class="line">The right graph is the output of the network after a decision threshold. The X’s and O’s here correspond to decisions made by the network.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">netf</span><span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">my_predict</span><span class="p">(</span><span class="n">norm_l</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">W1_tmp</span><span class="p">,</span> <span class="n">b1_tmp</span><span class="p">,</span> <span class="n">W2_tmp</span><span class="p">,</span> <span class="n">b2_tmp</span><span class="p">)</span>
<span class="n">plt_network</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">netf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Advanced_Learning_Algorithms_weak1_116_0.png" src="../../_images/source_files_Advanced_Learning_Algorithms_weak1_116_0.png" />
</div>
</div>
</section>
<section id="id8">
<h2>Congratulations!<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h2>
<p>You have built a small neural network in NumPy. Hopefully this lab revealed the fairly simple and familiar functions which make up a layer in a neural network.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>