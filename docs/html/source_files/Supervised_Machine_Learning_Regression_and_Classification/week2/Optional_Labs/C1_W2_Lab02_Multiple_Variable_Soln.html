<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optional Lab: Multiple Variable Linear Regression &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link href="../../../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Optional Lab: Multiple Variable Linear Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/Optional_Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Optional-Lab:-Multiple-Variable-Linear-Regression">
<h1>Optional Lab: Multiple Variable Linear Regression<a class="headerlink" href="#Optional-Lab:-Multiple-Variable-Linear-Regression" title="Permalink to this heading"></a></h1>
<p>In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review. # Outline - `  1.1 Goals &lt;#toc_15456_1.1&gt;`__ - `  1.2 Tools &lt;#toc_15456_1.2&gt;`__ - `  1.3 Notation &lt;#toc_15456_1.3&gt;`__ - 2 Problem Statement - `  2.1 Matrix X containing our examples &lt;#toc_15456_2.1&gt;`__ - `  2.2 Parameter vector
w, b &lt;#toc_15456_2.2&gt;`__ - 3 Model Prediction With Multiple Variables - `  3.1 Single Prediction element by element &lt;#toc_15456_3.1&gt;`__ - `  3.2 Single Prediction, vector &lt;#toc_15456_3.2&gt;`__ - 4 Compute Cost With Multiple Variables - 5 Gradient Descent With Multiple Variables - `  5.1 Compute Gradient with Multiple Variables &lt;#toc_15456_5.1&gt;`__ - `  5.2 Gradient Descent With Multiple Variables &lt;#toc_15456_5.2&gt;`__ - 6
Congratulations</p>
<p>## 1.1 Goals - Extend our regression model routines to support multiple features - Extend data structures to support multiple features - Rewrite prediction, cost and gradient routines to support multiple features - Utilize NumPy <code class="docutils literal notranslate"><span class="pre">np.dot</span></code> to vectorize their implementations for speed and simplicity</p>
<p>## 1.2 Tools In this lab, we will make use of: - NumPy, a popular library for scientific computing - Matplotlib, a popular library for plotting data</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span><span class="o">,</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;./deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># reduced display precision on numpy arrays</span>
</pre></div>
</div>
</div>
<p>## 1.3 Notation Here is a summary of some of the notation you will encounter, updated for multiple features.</p>
<div class="line-block">
<div class="line">|General Notation | Description| Python (if applicable) | |: ————<a href="#id1"><span class="problematic" id="id2">|: ------------------------------------------------------------|</span></a>| | <span class="math notranslate nohighlight">\(a\)</span> | scalar, non bold || | <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> | vector, bold || | <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> | matrix, bold capital || | <strong>Regression</strong> | | | | | <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> | training example maxtrix | <code class="docutils literal notranslate"><span class="pre">X_train</span></code> |</div>
<div class="line">| <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> | training example targets | <code class="docutils literal notranslate"><span class="pre">y_train</span></code> | <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span>, <span class="math notranslate nohighlight">\(y^{(i)}\)</span> | <span class="math notranslate nohighlight">\(i_{th}\)</span>Training Example | <code class="docutils literal notranslate"><span class="pre">X[i]</span></code>, <code class="docutils literal notranslate"><span class="pre">y[i]</span></code>| | m | number of training examples | <code class="docutils literal notranslate"><span class="pre">m</span></code>| | n | number of features in each example | <code class="docutils literal notranslate"><span class="pre">n</span></code>| | <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> | parameter: weight, | <code class="docutils literal notranslate"><span class="pre">w</span></code> | | <span class="math notranslate nohighlight">\(b\)</span> | parameter: bias | <code class="docutils literal notranslate"><span class="pre">b</span></code> |</div>
<div class="line">| <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> | The result of the model evaluation at <span class="math notranslate nohighlight">\(\mathbf{x^{(i)}}\)</span> parameterized by <span class="math notranslate nohighlight">\(\mathbf{w},b\)</span>: <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)}+b\)</span> | <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> |</div>
</div>
<p># 2 Problem Statement</p>
<p>You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below. Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Size (sqft)</p></th>
<th class="head"><p>Number of Bedrooms</p></th>
<th class="head"><p>Number of floors</p></th>
<th class="head"><p>Age of Home</p></th>
<th class="head"><p>Price (1000s dollars)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2104</p></td>
<td><p>5</p></td>
<td><p>1</p></td>
<td><p>45</p></td>
<td><p>460</p></td>
</tr>
<tr class="row-odd"><td><p>1416</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
<td><p>40</p></td>
<td><p>232</p></td>
</tr>
<tr class="row-even"><td><p>852</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>35</p></td>
<td><p>178</p></td>
</tr>
</tbody>
</table>
<p>You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.</p>
<p>Please run the following code cell to create your <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2104</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">45</span><span class="p">],</span> <span class="p">[</span><span class="mi">1416</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="p">[</span><span class="mi">852</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">35</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">460</span><span class="p">,</span> <span class="mi">232</span><span class="p">,</span> <span class="mi">178</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>## 2.1 Matrix X containing our examples Similar to the table above, examples are stored in a NumPy matrix <code class="docutils literal notranslate"><span class="pre">X_train</span></code>. Each row of the matrix represents one example. When you have <span class="math notranslate nohighlight">\(m\)</span> training examples ( <span class="math notranslate nohighlight">\(m\)</span> is three in our example), and there are <span class="math notranslate nohighlight">\(n\)</span> features (four in our example), <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a matrix with dimensions (<span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(n\)</span>) (m rows, n columns).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} =
\begin{pmatrix}
 x^{(0)}_0 &amp; x^{(0)}_1 &amp; \cdots &amp; x^{(0)}_{n-1} \\
 x^{(1)}_0 &amp; x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_{n-1} \\
 \cdots \\
 x^{(m-1)}_0 &amp; x^{(m-1)}_1 &amp; \cdots &amp; x^{(m-1)}_{n-1}
\end{pmatrix}\end{split}\]</div>
<p>notation: - <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> is vector containing example i. <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> $ = (x^{(i)}_0, x^{(i)}<em>1, :nbsphinx-math:`cdots`,x^{(i)}</em>{n-1})$ - <span class="math notranslate nohighlight">\(x^{(i)}_j\)</span> is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.</p>
<p>Display the input data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># data is stored in numpy array/matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X Shape: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, X Type:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y Shape: </span><span class="si">{</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, y Type:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X Shape: (3, 4), X Type:&lt;class &#39;numpy.ndarray&#39;&gt;)
[[2104    5    1   45]
 [1416    3    2   40]
 [ 852    2    1   35]]
y Shape: (3,), y Type:&lt;class &#39;numpy.ndarray&#39;&gt;)
[460 232 178]
</pre></div></div>
</div>
<p>## 2.2 Parameter vector w, b</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is a vector with <span class="math notranslate nohighlight">\(n\)</span> elements.</p>
<ul>
<li><p>Each element contains the parameter associated with one feature.</p></li>
<li><p>in our dataset, n is 4.</p></li>
<li><p>notionally, we draw this as a column vector</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{w} = \begin{pmatrix}
w_0 \\
w_1 \\
\cdots\\
w_{n-1}
\end{pmatrix}\end{split}\]</div>
<p>* <span class="math notranslate nohighlight">\(b\)</span> is a scalar parameter.</p>
<p>For demonstration, <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> will be loaded with some initial selected values that are near the optimal. <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is a 1-D NumPy vector.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b_init</span> <span class="o">=</span> <span class="mf">785.1811367994083</span>
<span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.39133535</span><span class="p">,</span> <span class="mf">18.75376741</span><span class="p">,</span> <span class="o">-</span><span class="mf">53.36032453</span><span class="p">,</span> <span class="o">-</span><span class="mf">26.42131618</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;w_init shape: </span><span class="si">{</span><span class="n">w_init</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, b_init type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">b_init</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
w_init shape: (4,), b_init type: &lt;class &#39;float&#39;&gt;
</pre></div></div>
</div>
<p># 3 Model Prediction With Multiple Variables The model’s prediction with multiple variables is given by the linear model:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \tag{1}\]</div>
<p>or in vector notation:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b  \tag{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\cdot\)</span> is a vector <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code></p>
<p>To demonstrate the dot product, we will implement prediction using (1) and (2).</p>
<p>## 3.1 Single Prediction element by element Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_single_loop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    single predict using linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): Shape (n,) example with multiple features</span>
<span class="sd">      w (ndarray): Shape (n,) model parameters</span>
<span class="sd">      b (scalar):  model parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">      p (scalar):  prediction</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">p_i</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">p_i</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get a row from our training data</span>
<span class="n">x_vec</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_vec shape </span><span class="si">{</span><span class="n">x_vec</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, x_vec value: </span><span class="si">{</span><span class="n">x_vec</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># make a prediction</span>
<span class="n">f_wb</span> <span class="o">=</span> <span class="n">predict_single_loop</span><span class="p">(</span><span class="n">x_vec</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f_wb shape </span><span class="si">{</span><span class="n">f_wb</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, prediction: </span><span class="si">{</span><span class="n">f_wb</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x_vec shape (4,), x_vec value: [2104    5    1   45]
f_wb shape (), prediction: 459.9999976194083
</pre></div></div>
</div>
<p>Note the shape of <code class="docutils literal notranslate"><span class="pre">x_vec</span></code>. It is a 1-D NumPy vector with 4 elements, (4,). The result, <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> is a scalar.</p>
<p>## 3.2 Single Prediction, vector</p>
<p>Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.</p>
<p>Recall from the Python/Numpy lab that NumPy <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code>[<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html">link</a>] can be used to perform a vector dot product.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    single predict using linear regression</span>
<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): Shape (n,) example with multiple features</span>
<span class="sd">      w (ndarray): Shape (n,) model parameters</span>
<span class="sd">      b (scalar):             model parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">      p (scalar):  prediction</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get a row from our training data</span>
<span class="n">x_vec</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_vec shape </span><span class="si">{</span><span class="n">x_vec</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, x_vec value: </span><span class="si">{</span><span class="n">x_vec</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># make a prediction</span>
<span class="n">f_wb</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">x_vec</span><span class="p">,</span><span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f_wb shape </span><span class="si">{</span><span class="n">f_wb</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, prediction: </span><span class="si">{</span><span class="n">f_wb</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x_vec shape (4,), x_vec value: [2104    5    1   45]
f_wb shape (), prediction: 459.9999976194083
</pre></div></div>
</div>
<p>The results and shapes are the same as the previous version which used looping. Going forward, <code class="docutils literal notranslate"><span class="pre">np.dot</span></code> will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine.</p>
<p># 4 Compute Cost With Multiple Variables The equation for the cost function with multiple variables <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{4}\]</div>
<p>In contrast to previous labs, <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> are vectors rather than scalars supporting multiple features.</p>
<p>Below is an implementation of equations (3) and (4). Note that this uses a <em>standard pattern for this course</em> where a for loop over all <code class="docutils literal notranslate"><span class="pre">m</span></code> examples is used.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    compute cost</span>
<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n)): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)) : target values</span>
<span class="sd">      w (ndarray (n,)) : model parameters</span>
<span class="sd">      b (scalar)       : model parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">      cost (scalar): cost</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>           <span class="c1">#(n,)(n,) = scalar (see np.dot)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="p">(</span><span class="n">f_wb_i</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>       <span class="c1">#scalar</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>                      <span class="c1">#scalar</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost using our pre-chosen optimal parameters.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Cost at optimal w : </span><span class="si">{</span><span class="n">cost</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at optimal w : 1.5578904428966628e-12
</pre></div></div>
</div>
<p><strong>Expected Result</strong>: Cost at optimal w : 1.5578904045996674e-12</p>
<p># 5 Gradient Descent With Multiple Variables Gradient descent for multiple variables:</p>
<div class="math notranslate nohighlight">
\[\begin{align*} \text{repeat}&amp;\text{ until convergence:} \; \lbrace \newline\;
&amp; w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{5}  \; &amp; \text{for j = 0..n-1}\newline
&amp;b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}\]</div>
<p>where, n is the number of features, parameters <span class="math notranslate nohighlight">\(w_j\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, are updated simultaneously and where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{6}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{7}
\end{align}\end{split}\]</div>
<p>* m is the number of training examples in the data set</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the target value</p></li>
</ul>
<p>## 5.1 Compute Gradient with Multiple Variables An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an - outer loop over all m examples. - <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> for the example can be computed directly and accumulated - in a second loop over all n features: - <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w_j}\)</span> is computed for each <span class="math notranslate nohighlight">\(w_j\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>
<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n)): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)) : target values</span>
<span class="sd">      w (ndarray (n,)) : model parameters</span>
<span class="sd">      b (scalar)       : model parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>           <span class="c1">#(number of examples, number of features)</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">/</span> <span class="n">m</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute and display gradient</span>
<span class="n">tmp_dj_db</span><span class="p">,</span> <span class="n">tmp_dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_db at initial w,b: </span><span class="si">{</span><span class="n">tmp_dj_db</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_dw at initial w,b: </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">tmp_dj_dw</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at initial w,b: -1.6739251501955248e-06
dj_dw at initial w,b:
 [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]
</pre></div></div>
</div>
<div class="line-block">
<div class="line"><strong>Expected Result</strong>:</div>
<div class="line">dj_db at initial w,b: -1.6739251122999121e-06</div>
<div class="line">dj_dw at initial w,b:</div>
<div class="line">[-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]</div>
</div>
<p>## 5.2 Gradient Descent With Multiple Variables The routine below implements equation (5) above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n))   : Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,))    : target values</span>
<span class="sd">      w_in (ndarray (n,)) : initial model parameters</span>
<span class="sd">      b_in (scalar)       : initial model parameter</span>
<span class="sd">      cost_function       : function to compute cost</span>
<span class="sd">      gradient_function   : function to compute the gradient</span>
<span class="sd">      alpha (float)       : Learning rate</span>
<span class="sd">      num_iters (int)     : number of iterations to run gradient descent</span>

<span class="sd">    Returns:</span>
<span class="sd">      w (ndarray (n,)) : Updated values of parameters</span>
<span class="sd">      b (scalar)       : Updated value of parameter</span>
<span class="sd">      &quot;&quot;&quot;</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>  <span class="c1">#avoid modifying global w within function</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b_in</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span><span class="n">dj_dw</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>   <span class="c1">##None</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>               <span class="c1">##None</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>               <span class="c1">##None</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">J_history</span> <span class="c1">#return final w,b and J history for graphing</span>
</pre></div>
</div>
</div>
<p>In the next cell you will test the implementation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize parameters</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="c1"># some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">5.0e-7</span>
<span class="c1"># run gradient descent</span>
<span class="n">w_final</span><span class="p">,</span> <span class="n">b_final</span><span class="p">,</span> <span class="n">J_hist</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                                    <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span>
                                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b,w found by gradient descent: </span><span class="si">{</span><span class="n">b_final</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">w_final</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
<span class="n">m</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;prediction: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">w_final</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b_final</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">, target value: </span><span class="si">{</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost  2529.46
Iteration  100: Cost   695.99
Iteration  200: Cost   694.92
Iteration  300: Cost   693.86
Iteration  400: Cost   692.81
Iteration  500: Cost   691.77
Iteration  600: Cost   690.73
Iteration  700: Cost   689.71
Iteration  800: Cost   688.70
Iteration  900: Cost   687.69
b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]
prediction: 426.19, target value: 460
prediction: 286.17, target value: 232
prediction: 171.47, target value: 178
</pre></div></div>
</div>
<div class="line-block">
<div class="line"><strong>Expected Result</strong>:</div>
<div class="line">b,w found by gradient descent: -0.00,[ 0.2 0. -0.01 -0.07]</div>
<div class="line">prediction: 426.19, target value: 460</div>
<div class="line">prediction: 286.17, target value: 232</div>
<div class="line">prediction: 171.47, target value: 178</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot cost versus iteration</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">J_hist</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">100</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">J_hist</span><span class="p">[</span><span class="mi">100</span><span class="p">:])),</span> <span class="n">J_hist</span><span class="p">[</span><span class="mi">100</span><span class="p">:])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cost vs. iteration&quot;</span><span class="p">);</span>  <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cost vs. iteration (tail)&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>             <span class="p">;</span>  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iteration step&#39;</span><span class="p">)</span>   <span class="p">;</span>  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;iteration step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_week2_Optional_Labs_C1_W2_Lab02_Multiple_Variable_Soln_36_0.png" src="../../../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_week2_Optional_Labs_C1_W2_Lab02_Multiple_Variable_Soln_36_0.png" />
</div>
</div>
<p><em>These results are not inspiring</em>! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>