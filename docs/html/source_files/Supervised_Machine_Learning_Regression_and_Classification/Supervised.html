<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supervised_Machine_Learning &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Welcome to Machine-Learning-Andrew-Ng’s documentation!" href="../../index.html" />
    <link href="../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Supervised_Machine_Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Module---1">Module - 1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Optional-Lab---W1:-Brief-Introduction-to-Python-and-Jupyter-Notebooks">Optional Lab - W1: Brief Introduction to Python and Jupyter Notebooks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Goals">Goals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Python">Python</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Print-statement">Print statement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Practice-Quiz">Practice Quiz</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Quiz---1">Quiz - 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Quiz---2">Quiz - 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Quiz---3">Quiz - 3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Module---2">Module - 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Optional-Lab-W2:-Python,-NumPy-and-Vectorization">Optional Lab W2: Python, NumPy and Vectorization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Outline">Outline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#1.1-Goals">1.1 Goals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#1.2-Useful-References">1.2 Useful References</a></li>
<li class="toctree-l4"><a class="reference internal" href="#2-Python-and-NumPy">2 Python and NumPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#3-Vectors">3 Vectors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#4-Matrices">4 Matrices</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id4">Practice Quiz</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Quiz-1">Quiz-1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Quiz-2">Quiz-2</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Quiz-3">Quiz-3</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Assignment-W2:">Assignment W2:</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Practice-Lab:-Linear-Regression">Practice Lab: Linear Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#1---Packages">1 - Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="#2---Problem-Statement">2 - Problem Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="#3---Dataset">3 - Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#4---Refresher-on-linear-regression">4 - Refresher on linear regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#5---Compute-Cost">5 - Compute Cost</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Exercise-1">Exercise 1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#6---Gradient-descent">6 - Gradient descent</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Module---3">Module - 3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Optional-Lab-W3">Optional Lab W3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.1:-Classification">Optional Lab - 3.1: Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.2:-Logistic-Regression">Optional Lab - 3.2: Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.3:-Logistic-Regression,-Decision-Boundary">Optional Lab - 3.3: Logistic Regression, Decision Boundary</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.4:-Logistic-Regression,-Logistic-Loss">Optional Lab - 3.4: Logistic Regression, Logistic Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.5:-Cost-Function-for-Logistic-Regression">Optional Lab - 3.5: Cost Function for Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.6:-Gradient-Descent-for-Logistic-Regression">Optional Lab - 3.6: Gradient Descent for Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.7:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn">Optional Lab - 3.7: Ungraded Lab: Logistic Regression using Scikit-Learn</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.8:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn">Optional Lab - 3.8: Ungraded Lab: Logistic Regression using Scikit-Learn</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Optional-Lab---3.9---Regularized-Cost-and-Gradient">Optional Lab - 3.9 - Regularized Cost and Gradient</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id19">Practice Quiz</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id20">Quiz-1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">Quiz-2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Assignment-W3:">Assignment W3:</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id22">1 - Packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="#2---Logistic-Regression">2 - Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#3---Regularized-Logistic-Regression">3 - Regularized Logistic Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Supervised_Machine_Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/source_files/Supervised_Machine_Learning_Regression_and_Classification/Supervised.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <hr style="height: 3px; background-color: black;"><section id="Supervised_Machine_Learning">
<h1>Supervised_Machine_Learning<a class="headerlink" href="#Supervised_Machine_Learning" title="Permalink to this heading"></a></h1>
<p><strong>(Regression_and_Classification)</strong></p>
<hr style="height: 3px; background-color: black;"><div style="margin-bottom: 4vh;"></div><section id="Module---1">
<h2>Module - 1<a class="headerlink" href="#Module---1" title="Permalink to this heading"></a></h2>
<div style="margin-bottom: 8vh;"></div><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#ignore these lines, these are added to load some data</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week1&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week3&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/C1W2A1&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1&quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div style="margin-bottom: 5vh;"></div><section id="Optional-Lab---W1:-Brief-Introduction-to-Python-and-Jupyter-Notebooks">
<h3>Optional Lab - W1: Brief Introduction to Python and Jupyter Notebooks<a class="headerlink" href="#Optional-Lab---W1:-Brief-Introduction-to-Python-and-Jupyter-Notebooks" title="Permalink to this heading"></a></h3>
<p>Welcome to the first optional lab! Optional labs are available to: - provide information - like this notebook - reinforce lecture material with hands-on examples - provide working examples of routines used in the graded labs</p>
<section id="Goals">
<h4>Goals<a class="headerlink" href="#Goals" title="Permalink to this heading"></a></h4>
<p>In this lab, you will: - Get a brief introduction to Jupyter notebooks - Take a tour of Jupyter notebooks - Learn the difference between markdown cells and code cells - Practice some basic python</p>
<p>The easiest way to become familiar with Jupyter notebooks is to take the tour available above in the Help menu:</p>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/C1W1L1_Tour.PNG" style="width: 400px;" /></p>
<center/><figure/><p>Jupyter notebooks have two types of cells that are used in this course. Cells such as this which contain documentation called <code class="docutils literal notranslate"><span class="pre">Markdown</span> <span class="pre">Cells</span></code>. The name is derived from the simple formatting language used in the cells. You will not be required to produce markdown cells. Its useful to understand the <code class="docutils literal notranslate"><span class="pre">cell</span> <span class="pre">pulldown</span></code> shown in graphic below. Occasionally, a cell will end up in the wrong mode and you may need to restore it to the right state:</p>
<figure><p><img alt="missing" class="no-scaled-link" src="../../_images/C1W1L1_Markdown.PNG" style="width: 400px;" /></p>
<figure/><p>The other type of cell is the <code class="docutils literal notranslate"><span class="pre">code</span> <span class="pre">cell</span></code> where you will write your code:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[96]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is  a &#39;Code&#39; Cell</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This is  code cell&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
This is  code cell
</pre></div></div>
</div>
</section>
</section>
<section id="Python">
<h3>Python<a class="headerlink" href="#Python" title="Permalink to this heading"></a></h3>
<p>You can write your code in the code cells. To run the code, select the cell and either - hold the shift-key down and hit ‘enter’ or ‘return’ - click the ‘run’ arrow above</p>
<figure><p><img alt="258a7632e77744818dc18c96faafc48f" class="no-scaled-link" src="../../_images/C1W1L1_Run.PNG" style="width: 800px;" /></p>
<figure/><section id="Print-statement">
<h4>Print statement<a class="headerlink" href="#Print-statement" title="Permalink to this heading"></a></h4>
<div class="line-block">
<div class="line">Print statements will generally use the python f-string style.</div>
<div class="line">Try creating your own print in the following cell.</div>
<div class="line">Try both methods of running the cell.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[97]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print statements</span>
<span class="n">variable</span> <span class="o">=</span> <span class="s2">&quot;right in the strings!&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f strings allow you to embed variables </span><span class="si">{</span><span class="n">variable</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
f strings allow you to embed variables right in the strings!
</pre></div></div>
</div>
</section>
</section>
<section id="Practice-Quiz">
<h3>Practice Quiz<a class="headerlink" href="#Practice-Quiz" title="Permalink to this heading"></a></h3>
<section id="Quiz---1">
<h4>Quiz - 1<a class="headerlink" href="#Quiz---1" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss1.png" style="width: 800px;" /></p>
<center/><figure/></section>
<section id="Quiz---2">
<h4>Quiz - 2<a class="headerlink" href="#Quiz---2" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss21.png" style="width: 800px;" /></p>
<center/><figure/></section>
<section id="Quiz---3">
<h4>Quiz - 3<a class="headerlink" href="#Quiz---3" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss3.png" style="width: 800px;" /></p>
<center/><figure/><div style="margin-bottom: 8vh;"></div><hr style="height: 3px; background-color: black;"><div style="margin-bottom: 10vh;"></div></section>
</section>
</section>
<section id="Module---2">
<h2>Module - 2<a class="headerlink" href="#Module---2" title="Permalink to this heading"></a></h2>
<section id="Optional-Lab-W2:-Python,-NumPy-and-Vectorization">
<h3>Optional Lab W2: Python, NumPy and Vectorization<a class="headerlink" href="#Optional-Lab-W2:-Python,-NumPy-and-Vectorization" title="Permalink to this heading"></a></h3>
<p>A brief introduction to some of the scientific computing used in this course. In particular the NumPy scientific computing package and its use with python.</p>
<section id="Outline">
<h4>Outline<a class="headerlink" href="#Outline" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>1.1 Goals</p></li>
<li><p>1.2 Useful References</p></li>
<li><p>2 Python and NumPy</p></li>
<li><p>3 Vectors</p></li>
<li><p>3.1 Abstract</p></li>
<li><p>3.2 NumPy Arrays</p></li>
<li><p>3.3 Vector Creation</p></li>
<li><p>3.4 Operations on Vectors</p></li>
<li><p>4 Matrices</p></li>
<li><p>4.1 Abstract</p></li>
<li><p>4.2 NumPy Arrays</p></li>
<li><p>4.3 Matrix Creation</p></li>
<li><p>4.4 Operations on Matrices</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[98]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>    <span class="c1"># it is an unofficial standard to use np for numpy</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
</div>
</section>
<section id="1.1-Goals">
<h4>1.1 Goals<a class="headerlink" href="#1.1-Goals" title="Permalink to this heading"></a></h4>
<p>In this lab, you will: - Review the features of NumPy and Python that are used in Course 1</p>
</section>
<section id="1.2-Useful-References">
<h4>1.2 Useful References<a class="headerlink" href="#1.2-Useful-References" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>NumPy Documentation including a basic introduction: <a class="reference external" href="https://NumPy.org/doc/stable/">NumPy.org</a> - A challenging feature topic: <a class="reference external" href="https://NumPy.org/doc/stable/user/basics.broadcasting.html">NumPy Broadcasting</a></p></li>
</ul>
</section>
<section id="2-Python-and-NumPy">
<h4>2 Python and NumPy<a class="headerlink" href="#2-Python-and-NumPy" title="Permalink to this heading"></a></h4>
<p>Python is the programming language we will be using in this course. It has a set of numeric data types and arithmetic operations. NumPy is a library that extends the base capabilities of python to add a richer data set including more numeric types, vectors, matrices, and many matrix functions. NumPy and python work together fairly seamlessly. Python arithmetic operators work on NumPy data types and many NumPy functions will accept python data types.</p>
</section>
<section id="3-Vectors">
<h4>3 Vectors<a class="headerlink" href="#3-Vectors" title="Permalink to this heading"></a></h4>
<section id="3.1-Abstract">
<h5>3.1 Abstract<a class="headerlink" href="#3.1-Abstract" title="Permalink to this heading"></a></h5>
<div style="float: right;margin-left: 10px;"><p><img alt="Image" class="no-scaled-link" src="../../_images/C1_W2_Lab04_Vectors.PNG" style="width: 340px;" /></p>
</div><p><p>Vectors, as you will use them in this course, are ordered arrays of numbers. In notation, vectors are denoted with lower case bold letters such as <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. The elements of a vector are all the same type. A vector does not, for example, contain both characters and numbers. The number of elements in the array is often referred to as the <em>dimension</em> though mathematicians may prefer <em>rank</em>. The vector shown has a dimension of <span class="math notranslate nohighlight">\(n\)</span>. The elements of a vector can be referenced with an
index. In math settings, indexes typically run from 1 to n. In computer science and these labs, indexing will typically run from 0 to n-1. In notation, elements of a vector, when referenced individually will indicate the index in a subscript, for example, the <span class="math notranslate nohighlight">\(0^{th}\)</span> element, of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is <span class="math notranslate nohighlight">\(x_0\)</span>. Note, the x is not bold in this case.</p>
</p></section>
<section id="3.2-NumPy-Arrays">
<h5>3.2 NumPy Arrays<a class="headerlink" href="#3.2-NumPy-Arrays" title="Permalink to this heading"></a></h5>
<p>NumPy’s basic data structure is an indexable, n-dimensional <em>array</em> containing elements of the same type (<code class="docutils literal notranslate"><span class="pre">dtype</span></code>). Right away, you may notice we have overloaded the term ‘dimension’. Above, it was the number of elements in the vector, here, dimension refers to the number of indexes of an array. A one-dimensional or 1-D array has one index. In Course 1, we will represent vectors as NumPy 1-D arrays.</p>
<ul class="simple">
<li><p>1-D array, shape (n,): n elements indexed [0] through [n-1]</p></li>
</ul>
</section>
<section id="3.3-Vector-Creation">
<h5>3.3 Vector Creation<a class="headerlink" href="#3.3-Vector-Creation" title="Permalink to this heading"></a></h5>
<p>Data creation routines in NumPy will generally have a first parameter which is the shape of the object. This can either be a single value for a 1-D result or a tuple (n,m,…) specifying the shape of the result. Below are examples of creating vectors using these routines.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[99]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NumPy routines which allocate memory and fill arrays with value</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.zeros(4) :   a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,));</span>             <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.zeros(4,) :  a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.random.random_sample(4): a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64
np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64
np.random.random_sample(4): a = [0.08756251 0.03058948 0.35713493 0.58978199], a shape = (4,), a data type = float64
</pre></div></div>
</div>
<p>Some data creation routines do not take a shape tuple:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[100]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.</span><span class="p">);</span>              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.arange(4.):     a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.random.rand(4): a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64
np.random.rand(4): a = [0.05222273 0.06566367 0.04350119 0.39515076], a shape = (4,), a data type = float64
</pre></div></div>
</div>
<p>values can be specified manually as well.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[101]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NumPy routines which allocate memory and fill with user specified values</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]);</span>  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.array([5,4,3,2]):  a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">,     a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]);</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.array([5.,4,3,2]): a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">, a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a data type = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int64
np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64
</pre></div></div>
</div>
<p>These have all created a one-dimensional vector <code class="docutils literal notranslate"><span class="pre">a</span></code> with four elements. <code class="docutils literal notranslate"><span class="pre">a.shape</span></code> returns the dimensions. Here we see a.shape = <code class="docutils literal notranslate"><span class="pre">(4,)</span></code> indicating a 1-d array with 4 elements.</p>
</section>
<section id="3.4-Operations-on-Vectors">
<h5>3.4 Operations on Vectors<a class="headerlink" href="#3.4-Operations-on-Vectors" title="Permalink to this heading"></a></h5>
<p>Let’s explore some operations using vectors.</p>
<p>3.4.1 Indexing</p>
<div class="line-block">
<div class="line">Elements of vectors can be accessed via indexing and slicing. NumPy provides a very complete set of indexing and slicing capabilities. We will explore only the basics needed for the course here. Reference <a class="reference external" href="https://NumPy.org/doc/stable/reference/arrays.indexing.html">Slicing and Indexing</a> for more details.</div>
<div class="line"><strong>Indexing</strong> means referring to <em>an element</em> of an array by its position within the array.</div>
<div class="line"><strong>Slicing</strong> means getting a <em>subset</em> of elements from an array based on their indices.</div>
<div class="line">NumPy starts indexing at zero so the 3rd element of an vector <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is <code class="docutils literal notranslate"><span class="pre">a[2]</span></code>.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[102]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#vector indexing operations on 1-D vectors</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1">#access an element</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a[2].shape: </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> a[2]  = </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">, Accessing an element returns a scalar&quot;</span><span class="p">)</span>

<span class="c1"># access the last element, negative indexes count from the end</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a[-1] = </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#indexs must be within the range of the vector or they will produce and error</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The error message you&#39;ll see is:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0 1 2 3 4 5 6 7 8 9]
a[2].shape: () a[2]  = 2, Accessing an element returns a scalar
a[-1] = 9
The error message you&#39;ll see is:
index 10 is out of bounds for axis 0 with size 10
</pre></div></div>
</div>
<p>3.4.2 Slicing</p>
<p>Slicing creates an array of indices using a set of three values (<code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code>). A subset of values is also valid. Its use is best explained by example:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[103]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#vector slicing operations</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a         = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#access 5 consecutive elements (start:stop:step)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">1</span><span class="p">];</span>     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[2:7:1] = &quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="c1"># access 3 elements separated by two</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">2</span><span class="p">];</span>     <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[2:7:2] = &quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="c1"># access all elements index 3 and above</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">:];</span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[3:]    = &quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="c1"># access all elements below index 3</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:</span><span class="mi">3</span><span class="p">];</span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[:3]    = &quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="c1"># access all elements</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:];</span>         <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[:]     = &quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a         = [0 1 2 3 4 5 6 7 8 9]
a[2:7:1] =  [2 3 4 5 6]
a[2:7:2] =  [2 4 6]
a[3:]    =  [3 4 5 6 7 8 9]
a[:3]    =  [0 1 2]
a[:]     =  [0 1 2 3 4 5 6 7 8 9]
</pre></div></div>
</div>
<p>3.4.3 Single vector operations</p>
<p>There are a number of useful operations that involve operations on a single vector.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[104]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a             : </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># negate elements of a</span>
<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = -a        : </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># sum all elements of a, returns a scalar</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = np.sum(a) : </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = np.mean(a): </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = a**2      : </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a             : [1 2 3 4]
b = -a        : [-1 -2 -3 -4]
b = np.sum(a) : 10
b = np.mean(a): 2.5
b = a**2      : [ 1  4  9 16]
</pre></div></div>
</div>
<p>3.4.4 Vector Vector element-wise operations</p>
<p>Most of the NumPy arithmetic, logical and comparison operations apply to vectors as well. These operators work on an element-by-element basis. For example</p>
<div class="math notranslate nohighlight">
\[\mathbf{a} + \mathbf{b} = \sum_{i=0}^{n-1} a_i + b_i\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[105]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Binary operators work element wise: </span><span class="si">{</span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Binary operators work element wise: [0 0 6 8]
</pre></div></div>
</div>
<p>Of course, for this to work correctly, the vectors must be of the same size:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[106]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#try a mismatched vector operation</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">c</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The error message you&#39;ll see is:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The error message you&#39;ll see is:
operands could not be broadcast together with shapes (4,) (2,)
</pre></div></div>
</div>
<p>3.4.5 Scalar Vector operations</p>
<p>Vectors can be ‘scaled’ by scalar values. A scalar value is just a number. The scalar multiplies all the elements of the vector.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[107]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># multiply a by a scalar</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">a</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b = 5 * a : </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
b = 5 * a : [ 5 10 15 20]
</pre></div></div>
</div>
<p>3.4.6 Vector Vector dot product</p>
<p>The dot product is a mainstay of Linear Algebra and NumPy. This is an operation used extensively in this course and should be well understood. The dot product is shown below.</p>
<p><img alt="ca43fa2736c6439297695997af222782" class="no-scaled-link" src="../../_images/C1_W2_Lab04_dot_notrans.gif" style="width: 800px;" /></p>
<p>The dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same.</p>
<p>Let’s implement our own version of the dot product below:</p>
<p><strong>Using a for loop</strong>, implement a function which returns the dot product of two vectors. The function to return given inputs <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[x = \sum_{i=0}^{n-1} a_i b_i\]</div>
<p>Assume both <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are the same shape.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[108]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">   Compute the dot product of two vectors</span>

<span class="sd">    Args:</span>
<span class="sd">      a (ndarray (n,)):  input vector</span>
<span class="sd">      b (ndarray (n,)):  input vector with same dimension as a</span>

<span class="sd">    Returns:</span>
<span class="sd">      x (scalar):</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[109]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test 1-D</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;my_dot(a, b) = </span><span class="si">{</span><span class="n">my_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
my_dot(a, b) = 24
</pre></div></div>
</div>
<p>Note, the dot product is expected to return a scalar value.</p>
<p>Let’s try the same operations using <code class="docutils literal notranslate"><span class="pre">np.dot</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[110]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test 1-D</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy 1-D np.dot(a, b) = </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">, np.dot(a, b).shape = </span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy 1-D np.dot(b, a) = </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">, np.dot(a, b).shape = </span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = ()
NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = ()
</pre></div></div>
</div>
<p>Above, you will note that the results for 1-D matched our implementation.</p>
<p>3.4.7 The Need for Speed: vector vs for loop</p>
<p>We utilized the NumPy library because it improves speed memory efficiency. Let’s demonstrate:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[111]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10000000</span><span class="p">)</span>  <span class="c1"># very large arrays</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10000000</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># capture start time</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># capture end time</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.dot(a, b) =  </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vectorized version duration: </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ms &quot;</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># capture start time</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">my_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># capture end time</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;my_dot(a, b) =  </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loop version duration: </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ms &quot;</span><span class="p">)</span>

<span class="k">del</span><span class="p">(</span><span class="n">a</span><span class="p">);</span><span class="k">del</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>  <span class="c1">#remove these big arrays from memory</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
np.dot(a, b) =  2501072.5817
Vectorized version duration: 10.0081 ms
my_dot(a, b) =  2501072.5817
loop version duration: 2157.2831 ms
</pre></div></div>
</div>
<p>So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU’s and modern CPU’s implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large.</p>
<p>3.4.8 Vector Vector operations in Course 1</p>
<p>Vector Vector operations will appear frequently in course 1. Here is why: - Going forward, our examples will be stored in an array, <code class="docutils literal notranslate"><span class="pre">X_train</span></code> of dimension (m,n). This will be explained more in context, but here it is important to note it is a 2 Dimensional array or matrix (see next section on matrices). - <code class="docutils literal notranslate"><span class="pre">w</span></code> will be a 1-dimensional vector of shape (n,). - we will perform operations by looping through the examples, extracting each example to work on individually by indexing X. For
example:<code class="docutils literal notranslate"><span class="pre">X[i]</span></code> - <code class="docutils literal notranslate"><span class="pre">X[i]</span></code> returns a value of shape (n,), a 1-dimensional vector. Consequently, operations involving <code class="docutils literal notranslate"><span class="pre">X[i]</span></code> are often vector-vector.</p>
<p>That is a somewhat lengthy explanation, but aligning and understanding the shapes of your operands is important when performing vector operations.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[112]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># show common Course 1 example</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;X[1] has shape </span><span class="si">{</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;w has shape </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c has shape </span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X[1] has shape (1,)
w has shape (1,)
c has shape ()
</pre></div></div>
</div>
</section>
</section>
<section id="4-Matrices">
<h4>4 Matrices<a class="headerlink" href="#4-Matrices" title="Permalink to this heading"></a></h4>
<section id="4.1-Abstract">
<h5>4.1 Abstract<a class="headerlink" href="#4.1-Abstract" title="Permalink to this heading"></a></h5>
<p>Matrices, are two dimensional arrays. The elements of a matrix are all of the same type. In notation, matrices are denoted with capitol, bold letter such as <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In this and other labs, <code class="docutils literal notranslate"><span class="pre">m</span></code> is often the number of rows and <code class="docutils literal notranslate"><span class="pre">n</span></code> the number of columns. The elements of a matrix can be referenced with a two dimensional index. In math settings, numbers in the index typically run from 1 to n. In computer science and these labs, indexing will run from 0 to n-1.</p>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/C1_W2_Lab04_Matrices.PNG" style="width: 900px;" /></p>
<center/><figcaption><p>Generic Matrix Notation, 1st index is row, 2nd is column</p>
</figcaption><figure/></section>
<section id="4.2-NumPy-Arrays">
<h5>4.2 NumPy Arrays<a class="headerlink" href="#4.2-NumPy-Arrays" title="Permalink to this heading"></a></h5>
<p>NumPy’s basic data structure is an indexable, n-dimensional <em>array</em> containing elements of the same type (<code class="docutils literal notranslate"><span class="pre">dtype</span></code>). These were described earlier. Matrices have a two-dimensional (2-D) index [m,n].</p>
<p>In Course 1, 2-D matrices are used to hold training data. Training data is <span class="math notranslate nohighlight">\(m\)</span> examples by <span class="math notranslate nohighlight">\(n\)</span> features creating an (m,n) array. Course 1 does not do operations directly on matrices but typically extracts an example as a vector and operates on that. Below you will review: - data creation - slicing and indexing</p>
</section>
<section id="4.3-Matrix-Creation">
<h5>4.3 Matrix Creation<a class="headerlink" href="#4.3-Matrix-Creation" title="Permalink to this heading"></a></h5>
<p>The same functions that created 1-D vectors will create 2-D or n-D arrays. Here are some examples</p>
<p>Below, the shape tuple is provided to achieve a 2-D result. Notice how NumPy uses brackets to denote each dimension. Notice further than NumPy, when printing, will print one row per line.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[113]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a shape = (1, 5), a = [[0. 0. 0. 0. 0.]]
a shape = (2, 1), a = [[0.]
 [0.]]
a shape = (1, 1), a = [[0.44236513]]
</pre></div></div>
</div>
<p>One can also manually specify data. Dimensions are specified with additional brackets matching the format in the printing above.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[114]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NumPy routines which allocate memory and fill with user specified values</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]);</span>   <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, np.array: a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">],</span>   <span class="c1"># One can also</span>
              <span class="p">[</span><span class="mi">4</span><span class="p">],</span>   <span class="c1"># separate values</span>
              <span class="p">[</span><span class="mi">3</span><span class="p">]]);</span> <span class="c1">#into separate rows</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; a shape = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, np.array: a = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
 a shape = (3, 1), np.array: a = [[5]
 [4]
 [3]]
 a shape = (3, 1), np.array: a = [[5]
 [4]
 [3]]
</pre></div></div>
</div>
</section>
<section id="4.4-Operations-on-Matrices">
<h5>4.4 Operations on Matrices<a class="headerlink" href="#4.4-Operations-on-Matrices" title="Permalink to this heading"></a></h5>
<p>Let’s explore some operations using matrices.</p>
<p>4.4.1 Indexing</p>
<p>Matrices include a second index. The two indexes describe [row, column]. Access can either return an element or a row/column. See below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[115]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#vector indexing operations on matrices</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1">#reshape is a convenient way to create matrices</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.shape: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="s2">a= </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#access an element</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">a[2,0].shape:   </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a[2,0] = </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,     type(a[2,0]) = </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2"> Accessing an element returns a scalar</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#access a row</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a[2].shape:   </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, a[2]   = </span><span class="si">{</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">, type(a[2])   = </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a.shape: (3, 2),
a= [[0 1]
 [2 3]
 [4 5]]

a[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = &lt;class &#39;numpy.int64&#39;&gt; Accessing an element returns a scalar

a[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>It is worth drawing attention to the last example. Accessing a matrix by just specifying the row will return a <em>1-D vector</em>.</p>
<div class="line-block">
<div class="line"><strong>Reshape</strong></div>
<div class="line">The previous example used <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html">reshape</a> to shape the array.</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">=</span> <span class="pre">np.arange(6).reshape(-1,</span> <span class="pre">2)</span></code></div>
<div class="line">This line of code first created a <em>1-D Vector</em> of six elements. It then reshaped that vector into a <em>2-D</em> array using the reshape command. This could have been written:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">=</span> <span class="pre">np.arange(6).reshape(3,</span> <span class="pre">2)</span></code></div>
<div class="line">To arrive at the same 3 row, 2 column array. The -1 argument tells the routine to compute the number of rows given the size of the array and the number of columns.</div>
</div>
<p>4.4.2 Slicing</p>
<p>Slicing creates an array of indices using a set of three values (<code class="docutils literal notranslate"><span class="pre">start:stop:step</span></code>). A subset of values is also valid. Its use is best explained by example:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[116]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#vector 2-D slicing operations</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a = </span><span class="se">\n</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#access 5 consecutive elements (start:stop:step)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[0, 2:7:1] = &quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;,  a[0, 2:7:1].shape =&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;a 1-D array&quot;</span><span class="p">)</span>

<span class="c1">#access 5 consecutive elements (start:stop:step) in two rows</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[:, 2:7:1] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;,  a[:, 2:7:1].shape =&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;a 2-D array&quot;</span><span class="p">)</span>

<span class="c1"># access all elements</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[:,:] = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:,:],</span> <span class="s2">&quot;,  a[:,:].shape =&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:,:]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># access all elements in one row (very common usage)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[1,:] = &quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="s2">&quot;,  a[1,:].shape =&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;a 1-D array&quot;</span><span class="p">)</span>
<span class="c1"># same as</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a[1]   = &quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>   <span class="s2">&quot;,  a[1].shape   =&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;a 1-D array&quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
a =
[[ 0  1  2  3  4  5  6  7  8  9]
 [10 11 12 13 14 15 16 17 18 19]]
a[0, 2:7:1] =  [2 3 4 5 6] ,  a[0, 2:7:1].shape = (5,) a 1-D array
a[:, 2:7:1] =
 [[ 2  3  4  5  6]
 [12 13 14 15 16]] ,  a[:, 2:7:1].shape = (2, 5) a 2-D array
a[:,:] =
 [[ 0  1  2  3  4  5  6  7  8  9]
 [10 11 12 13 14 15 16 17 18 19]] ,  a[:,:].shape = (2, 10)
a[1,:] =  [10 11 12 13 14 15 16 17 18 19] ,  a[1,:].shape = (10,) a 1-D array
a[1]   =  [10 11 12 13 14 15 16 17 18 19] ,  a[1].shape   = (10,) a 1-D array
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="id4">
<h3>Practice Quiz<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h3>
<section id="Quiz-1">
<h4>Quiz-1<a class="headerlink" href="#Quiz-1" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss11.png" style="width: 800px;" /></p>
<center/><figure/></section>
<section id="Quiz-2">
<h4>Quiz-2<a class="headerlink" href="#Quiz-2" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss2.png" style="width: 800px;" /></p>
<center/><figure/></section>
<section id="Quiz-3">
<h4>Quiz-3<a class="headerlink" href="#Quiz-3" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/ss31.png" style="width: 800px;" /></p>
<center/><figure/></section>
</section>
<section id="Assignment-W2:">
<h3>Assignment W2:<a class="headerlink" href="#Assignment-W2:" title="Permalink to this heading"></a></h3>
<section id="Practice-Lab:-Linear-Regression">
<h4>Practice Lab: Linear Regression<a class="headerlink" href="#Practice-Lab:-Linear-Regression" title="Permalink to this heading"></a></h4>
<p>Welcome to your first practice lab! In this lab, you will implement linear regression with one variable to predict profits for a restaurant franchise.</p>
<section id="id8">
<h5>Outline<a class="headerlink" href="#id8" title="Permalink to this heading"></a></h5>
<ul class="simple">
<li><p>1 - Packages</p></li>
<li><p>2 - Linear regression with one variable</p></li>
<li><p>2.1 Problem Statement</p></li>
<li><p>3 Dataset</p></li>
<li><p>4 Refresher on linear regression</p></li>
<li><p>5 Compute Cost</p>
<ul>
<li><p>Exercise 1</p></li>
</ul>
</li>
<li><p>6 Gradient descent</p>
<ul>
<li><p>Exercise 2</p></li>
<li><p>6.1 Learning parameters using batch gradient descent</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="1---Packages">
<h4>1 - Packages<a class="headerlink" href="#1---Packages" title="Permalink to this heading"></a></h4>
<p>First, let’s run the cell below to import all the packages that you will need during this assignment. - <a class="reference external" href="www.numpy.org">numpy</a> is the fundamental package for working with matrices in Python. - <a class="reference external" href="http://matplotlib.org">matplotlib</a> is a famous library to plot graphs in Python. - <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> contains helper functions for this assignment. You do not need to modify code in this file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="c1">#add modules from the path</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/C1W2A1&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1">#to show graphs inline</span>
</pre></div>
</div>
</div>
</section>
<section id="2---Problem-Statement">
<h4>2 - Problem Statement<a class="headerlink" href="#2---Problem-Statement" title="Permalink to this heading"></a></h4>
<p>Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. - You would like to expand your business to cities that may give your restaurant higher profits. - The chain already has restaurants in various cities and you have data for profits and populations from the cities. - You also have data on cities that are candidates for a new restaurant. - For these cities, you have the city population.</p>
<p>Can you use the data to help you identify which cities may potentially give your business higher profits?</p>
</section>
<section id="3---Dataset">
<h4>3 - Dataset<a class="headerlink" href="#3---Dataset" title="Permalink to this heading"></a></h4>
<div class="line-block">
<div class="line">You will start by loading the dataset for this task. - The <code class="docutils literal notranslate"><span class="pre">load_data()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">x_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> - <code class="docutils literal notranslate"><span class="pre">x_train</span></code> is the population of a city - <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is the profit of a restaurant in that city. A negative value for profit indicates a loss.</div>
<div class="line">- Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the dataset</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
<section id="View-the-variables">
<h5>View the variables<a class="headerlink" href="#View-the-variables" title="Permalink to this heading"></a></h5>
<div class="line-block">
<div class="line">Before starting on any task, it is useful to get more familiar with your dataset.</div>
<div class="line">- A good place to start is to just print out each variable and see what it contains.</div>
</div>
<p>The code below prints the variable <code class="docutils literal notranslate"><span class="pre">x_train</span></code> and the type of the variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[119]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print x_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of x_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements of x_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Type of x_train: &lt;class &#39;numpy.ndarray&#39;&gt;
First five elements of x_train are:
 [6.1101 5.5277 8.5186 7.0032 5.8598]
</pre></div></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x_train</span></code> is a numpy array that contains decimal values that are all greater than zero. - These values represent the city population times 10,000 - For example, 6.1101 means that the population for that city is 61,101</p>
<p>Now, let’s print <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[120]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print y_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements of y_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
First five elements of y_train are:
 [17.592   9.1302 13.662  11.854   6.8233]
</pre></div></div>
</div>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is a numpy array that has decimal values, some negative, some positive. - These represent your restaurant’s average monthly profits in each city, in units of $10,000. - For example, 17.592 represents $175,920 in average monthly profits for that city. - -2.6807 represents -$26,807 in average monthly loss for that city.</p>
</section>
<section id="Check-the-dimensions-of-your-variables">
<h5>Check the dimensions of your variables<a class="headerlink" href="#Check-the-dimensions-of-your-variables" title="Permalink to this heading"></a></h5>
<p>Another useful way to get familiar with your data is to view its dimensions.</p>
<p>Please print the shape of <code class="docutils literal notranslate"><span class="pre">x_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples you have in your dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[121]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of x_train is:&#39;</span><span class="p">,</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Number of training examples (m):&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of x_train is: (97,)
The shape of y_train is:  (97,)
Number of training examples (m): 97
</pre></div></div>
</div>
<p>The city population array has 97 data points, and the monthly average profits also has 97 data points. These are NumPy 1D arrays.</p>
</section>
<section id="Visualize-your-data">
<h5>Visualize your data<a class="headerlink" href="#Visualize-your-data" title="Permalink to this heading"></a></h5>
<p>It is often useful to understand the data by visualizing it. - For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). - Many other problems that you will encounter in real life have more than two properties (for example, population, average household income, monthly profits, monthly sales).When you have more than two properties, you can still use a scatter plot to see the relationship between each pair of properties.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a scatter plot of the data. To change the markers to red &quot;x&quot;,</span>
<span class="c1"># we used the &#39;marker&#39; and &#39;c&#39; parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Profits vs. Population per city&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Profit in $10,000&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Population of City in 10,000s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_108_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_108_0.png" />
</div>
</div>
<p>Your goal is to build a linear regression model to fit this data. - With this model, you can then input a new city’s population, and have the model estimate your restaurant’s potential monthly profits for that city.</p>
</section>
</section>
<section id="4---Refresher-on-linear-regression">
<h4>4 - Refresher on linear regression<a class="headerlink" href="#4---Refresher-on-linear-regression" title="Permalink to this heading"></a></h4>
<p>In this practice lab, you will fit the linear regression parameters <span class="math notranslate nohighlight">\((w,b)\)</span> to your dataset. - The model function for linear regression, which is a function that maps from <code class="docutils literal notranslate"><span class="pre">x</span></code> (city population) to <code class="docutils literal notranslate"><span class="pre">y</span></code> (your restaurant’s monthly profit for that city) is represented as</p>
<div class="math notranslate nohighlight">
\[f_{w,b}(x) = wx + b\]</div>
<ul class="simple">
<li><p>To train a linear regression model, you want to find the best <span class="math notranslate nohighlight">\((w,b)\)</span> parameters that fit your dataset.</p>
<ul>
<li><p>To compare how one choice of <span class="math notranslate nohighlight">\((w,b)\)</span> is better or worse than another choice, you can evaluate it with a cost function <span class="math notranslate nohighlight">\(J(w,b)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(J\)</span> is a function of <span class="math notranslate nohighlight">\((w,b)\)</span>. That is, the value of the cost <span class="math notranslate nohighlight">\(J(w,b)\)</span> depends on the value of <span class="math notranslate nohighlight">\((w,b)\)</span>.</p></li>
</ul>
</li>
<li><p>The choice of <span class="math notranslate nohighlight">\((w,b)\)</span> that fits your data the best is the one that has the smallest cost <span class="math notranslate nohighlight">\(J(w,b)\)</span>.</p></li>
</ul>
</li>
<li><p>To find the values <span class="math notranslate nohighlight">\((w,b)\)</span> that gets the smallest possible cost <span class="math notranslate nohighlight">\(J(w,b)\)</span>, you can use a method called <strong>gradient descent</strong>.</p>
<ul>
<li><p>With each step of gradient descent, your parameters <span class="math notranslate nohighlight">\((w,b)\)</span> come closer to the optimal values that will achieve the lowest cost <span class="math notranslate nohighlight">\(J(w,b)\)</span>.</p></li>
</ul>
</li>
<li><p>The trained linear regression model can then take the input feature <span class="math notranslate nohighlight">\(x\)</span> (city population) and output a prediction <span class="math notranslate nohighlight">\(f_{w,b}(x)\)</span> (predicted monthly profit for a restaurant in that city).</p></li>
</ul>
</section>
<section id="5---Compute-Cost">
<h4>5 - Compute Cost<a class="headerlink" href="#5---Compute-Cost" title="Permalink to this heading"></a></h4>
<p>Gradient descent involves repeated steps to adjust the value of your parameter <span class="math notranslate nohighlight">\((w,b)\)</span> to gradually get a smaller and smaller cost <span class="math notranslate nohighlight">\(J(w,b)\)</span>. - At each step of gradient descent, it will be helpful for you to monitor your progress by computing the cost <span class="math notranslate nohighlight">\(J(w,b)\)</span> as <span class="math notranslate nohighlight">\((w,b)\)</span> gets updated. - In this section, you will implement a function to calculate <span class="math notranslate nohighlight">\(J(w,b)\)</span> so that you can check the progress of your gradient descent implementation.</p>
<section id="Cost-function">
<h5>Cost function<a class="headerlink" href="#Cost-function" title="Permalink to this heading"></a></h5>
<p>As you may recall from the lecture, for one variable, the cost function for linear regression <span class="math notranslate nohighlight">\(J(w,b)\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\]</div>
<ul class="simple">
<li><p>You can think of <span class="math notranslate nohighlight">\(f_{w,b}(x^{(i)})\)</span> as the model’s prediction of your restaurant’s profit, as opposed to <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, which is the actual profit that is recorded in the data.</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> is the number of training examples in the dataset</p></li>
</ul>
</section>
<section id="Model-prediction">
<h5>Model prediction<a class="headerlink" href="#Model-prediction" title="Permalink to this heading"></a></h5>
<ul class="simple">
<li><p>For linear regression with one variable, the prediction of the model <span class="math notranslate nohighlight">\(f_{w,b}\)</span> for an example <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is representented as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[f_{w,b}(x^{(i)}) = wx^{(i)} + b\]</div>
<p>This is the equation for a line, with an intercept <span class="math notranslate nohighlight">\(b\)</span> and a slope <span class="math notranslate nohighlight">\(w\)</span></p>
</section>
<section id="Implementation">
<h5>Implementation<a class="headerlink" href="#Implementation" title="Permalink to this heading"></a></h5>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost()</span></code> function below to compute the cost <span class="math notranslate nohighlight">\(J(w,b)\)</span>.</p>
</section>
</section>
<section id="Exercise-1">
<h4>Exercise 1<a class="headerlink" href="#Exercise-1" title="Permalink to this heading"></a></h4>
<p>Complete the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> below to:</p>
<ul>
<li><p>Iterate over the training examples, and for each example, compute:</p>
<ul>
<li><p>The prediction of the model for that example</p>
<div class="math notranslate nohighlight">
\[f_{wb}(x^{(i)}) =  wx^{(i)} + b\]</div>
</li>
<li><p>The cost for that example</p>
<div class="math notranslate nohighlight">
\[cost^{(i)} =  (f_{wb} - y^{(i)})^2\]</div>
</li>
</ul>
</li>
<li><p>Return the total cost over all examples</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} cost^{(i)}\]</div>
<ul class="simple">
<li><p>Here, <span class="math notranslate nohighlight">\(m\)</span> is the number of training examples and <span class="math notranslate nohighlight">\(\sum\)</span> is the summation operator</p></li>
</ul>
</li>
</ul>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED FUNCTION: compute_cost</span>

<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost function for linear regression.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (ndarray): Shape (m,) Input to the model (Population of cities)</span>
<span class="sd">        y (ndarray): Shape (m,) Label (Actual profits for the cities)</span>
<span class="sd">        w, b (scalar): Parameters of the model</span>

<span class="sd">    Returns</span>
<span class="sd">        total_cost (float): The cost of using w,b as the parameters for linear regression</span>
<span class="sd">               to fit the data points in x and y</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># You need to return this variable correctly</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">cost</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">b</span>
        <span class="n">cost</span> <span class="o">+=</span> <span class="p">(</span><span class="n">f_wb</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul class="simple">
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">i</span>
</pre></div>
</div>
<ul>
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">x</span></code> using a for loop and add the <code class="docutils literal notranslate"><span class="pre">cost</span></code> from each iteration to a variable (<code class="docutils literal notranslate"><span class="pre">cost_sum</span></code>) initialized outside the loop.</p>
<ul class="simple">
<li><p>Then, you can return the <code class="docutils literal notranslate"><span class="pre">total_cost</span></code> as <code class="docutils literal notranslate"><span class="pre">cost_sum</span></code> divided by <code class="docutils literal notranslate"><span class="pre">2m</span></code>.</p></li>
</ul>
<details><p>Click for more hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="c1"># number of training examples</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="c1"># You need to return this variable correctly</span>
  <span class="n">total_cost</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="c1">### START CODE HERE ###</span>
  <span class="c1"># Variable to keep track of sum of cost from each example</span>
  <span class="n">cost_sum</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="c1"># Loop over training examples</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
      <span class="c1"># Your code here to get the prediction f_wb for the ith example</span>
      <span class="n">f_wb</span> <span class="o">=</span>
      <span class="c1"># Your code here to get the cost associated with the ith example</span>
      <span class="n">cost</span> <span class="o">=</span>

      <span class="c1"># Add to sum of cost for each example</span>
      <span class="n">cost_sum</span> <span class="o">=</span> <span class="n">cost_sum</span> <span class="o">+</span> <span class="n">cost</span>

  <span class="c1"># Get the total cost as the sum divided by (2*m)</span>
  <span class="n">total_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">cost_sum</span>
  <span class="c1">### END CODE HERE ###</span>

  <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</li>
</ul>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
</li>
</ul>
<details><p>Hint to calculate f_wb    For scalars <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (x[i], w and b are all scalars), you can calculate the equation <span class="math notranslate nohighlight">\(h = ab + c\)</span> in code as h = a * b + c</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = w * x[i] + b</p>
</details></details><details><p>Hint to calculate cost     You can calculate the square of a variable z as z**2</p>
<details><p>    More hints to calculate cost     You can compute cost as cost = (f_wb - y[i]) ** 2</p>
</details></details></details><p>You can check if your implementation was correct by running the following test code:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute cost with some initial values for paramaters w, b</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Cost at initial w (zeros): </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Public tests</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;class &#39;numpy.float64&#39;&gt;
Cost at initial w (zeros): 75.203
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at initial w (zeros): 75.203</p>
</td></tr></table></section>
<section id="6---Gradient-descent">
<h4>6 - Gradient descent<a class="headerlink" href="#6---Gradient-descent" title="Permalink to this heading"></a></h4>
<p>In this section, you will implement the gradient for parameters <span class="math notranslate nohighlight">\(w, b\)</span> for linear regression.</p>
<p>As described in the lecture videos, the gradient descent algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; \phantom {0000} b := b -  \alpha \frac{\partial J(w,b)}{\partial b} \newline       \; &amp; \phantom {0000} w := w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{1}  \; &amp;
\newline &amp; \rbrace\end{align*}\]</div>
<div class="line-block">
<div class="line">where, parameters <span class="math notranslate nohighlight">\(w, b\)</span> are both updated simultaniously and where</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial w}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \tag{3}\]</div>
<p>* m is the number of training examples in the dataset</p>
</div></blockquote>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{w,b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, is the target value</p></li>
</ul>
<p>You will implement a function called <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> which calculates <span class="math notranslate nohighlight">\(\frac{\partial J(w)}{\partial w}\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial J(w)}{\partial b}\)</span></p>
<section id="Exercise-2">
<h5>Exercise 2<a class="headerlink" href="#Exercise-2" title="Permalink to this heading"></a></h5>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function to:</p>
<ul>
<li><p>Iterate over the training examples, and for each example, compute:</p>
<ul>
<li><p>The prediction of the model for that example</p>
<div class="math notranslate nohighlight">
\[f_{wb}(x^{(i)}) =  wx^{(i)} + b\]</div>
</li>
<li><p>The gradient for the parameters <span class="math notranslate nohighlight">\(w, b\)</span> from that example</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)})\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}\]</div>
</li>
</ul>
</li>
<li><p>Return the total gradient update from all the examples</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} \frac{\partial J(w,b)}{\partial b}^{(i)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(w,b)}{\partial w}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} \frac{\partial J(w,b)}{\partial w}^{(i)}\]</div>
<ul class="simple">
<li><p>Here, <span class="math notranslate nohighlight">\(m\)</span> is the number of training examples and <span class="math notranslate nohighlight">\(\sum\)</span> is the summation operator</p></li>
</ul>
</li>
</ul>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED FUNCTION: compute_gradient</span>
<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>
<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): Shape (m,) Input to the model (Population of cities)</span>
<span class="sd">      y (ndarray): Shape (m,) Label (Actual profits for the cities)</span>
<span class="sd">      w, b (scalar): Parameters of the model</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w</span>
<span class="sd">      dj_db (scalar): The gradient of the cost w.r.t. the parameter b</span>
<span class="sd">     &quot;&quot;&quot;</span>

    <span class="c1"># Number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># You need to return the following variables correctly</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">b</span>
        <span class="n">dj_db</span> <span class="o">+=</span> <span class="n">f_wb</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">dj_dw</span> <span class="o">+=</span> <span class="p">(</span><span class="n">f_wb</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">dj_dw</span> <span class="o">/=</span> <span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">/=</span> <span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_dw</span><span class="p">,</span> <span class="n">dj_db</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows: <code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">0</span>&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(m):</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">+</span> <span class="pre">2*i</span></code></p>
<ul class="simple">
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">x</span></code> using a for loop and for each example, keep adding the gradient from that example to the variables <code class="docutils literal notranslate"><span class="pre">dj_dw</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_db</span></code> which are initialized outside the loop.</p></li>
</ul>
</li>
<li><div class="line-block">
<div class="line">Then, you can return <code class="docutils literal notranslate"><span class="pre">dj_dw</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_db</span></code> both divided by <code class="docutils literal notranslate"><span class="pre">m</span></code>.</div>
</div>
<details><p>Click for more hints</p>
</li>
<li><p>Here’s how you can structure the overall implementation for this function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>
<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): Shape (m,) Input to the model (Population of cities)</span>
<span class="sd">      y (ndarray): Shape (m,) Label (Actual profits for the cities)</span>
<span class="sd">      w, b (scalar): Parameters of the model</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w</span>
<span class="sd">      dj_db (scalar): The gradient of the cost w.r.t. the parameter b</span>
<span class="sd">     &quot;&quot;&quot;</span>

    <span class="c1"># Number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># You need to return the following variables correctly</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Loop over examples</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="c1"># Your code here to get prediction f_wb for the ith example</span>
        <span class="n">f_wb</span> <span class="o">=</span>

        <span class="c1"># Your code here to get the gradient for w from the ith example</span>
        <span class="n">dj_dw_i</span> <span class="o">=</span>

        <span class="c1"># Your code here to get the gradient for b from the ith example</span>
        <span class="n">dj_db_i</span> <span class="o">=</span>

        <span class="c1"># Update dj_db : In Python, a += 1  is the same as a = a + 1</span>
        <span class="n">dj_db</span> <span class="o">+=</span> <span class="n">dj_db_i</span>

        <span class="c1"># Update dj_dw</span>
        <span class="n">dj_dw</span> <span class="o">+=</span> <span class="n">dj_dw_i</span>

    <span class="c1"># Divide both dj_dw and dj_db by m</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">/</span> <span class="n">m</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_dw</span><span class="p">,</span> <span class="n">dj_db</span>
</pre></div>
</div>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
<details><p>Hint to calculate f_wb     You did this in the previous exercise! For scalars <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (x[i], w and b are all scalars), you can calculate the equation <span class="math notranslate nohighlight">\(h = ab + c\)</span> in code as h = a * b + c</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = w * x[i] + b</p>
</details></details><details><p>Hint to calculate dj_dw_i     For scalars <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (f_wb, y[i] and x[i] are all scalars), you can calculate the equation <span class="math notranslate nohighlight">\(h = (a - b)c\)</span> in code as h = (a-b)*c</p>
<details><p>    More hints to calculate f     You can compute dj_dw_i as dj_dw_i = (f_wb - y[i]) * x[i]</p>
</details></details><details><p>Hint to calculate dj_db_i     You can compute dj_db_i as dj_db_i = f_wb - y[i]</p>
</details></details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display gradient with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">tmp_dj_dw</span><span class="p">,</span> <span class="n">tmp_dj_db</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at initial w, b (zeros):&#39;</span><span class="p">,</span> <span class="n">tmp_dj_dw</span><span class="p">,</span> <span class="n">tmp_dj_db</span><span class="p">)</span>

<span class="n">compute_gradient_test</span><span class="p">(</span><span class="n">compute_gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Gradient at initial w, b (zeros): -65.32884974555672 -5.83913505154639
Using X with shape (4, 1)
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p>Now let’s run the gradient descent algorithm implemented above on our dataset.</p>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Gradient at initial , b (zeros)</p>
</td><td><p>-65.32884975 -5.83913505154639</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost and gradient with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">tmp_dj_dw</span><span class="p">,</span> <span class="n">tmp_dj_db</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient at test w, b:&#39;</span><span class="p">,</span> <span class="n">tmp_dj_dw</span><span class="p">,</span> <span class="n">tmp_dj_db</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Gradient at test w, b: -47.41610118114435 -4.007175051546391
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Gradient at test w</p>
</td><td><p>-47.41610118 -4.007175051546391</p>
</td></tr></table></section>
<section id="6.1-Learning-parameters-using-batch-gradient-descent">
<h5>6.1 Learning parameters using batch gradient descent<a class="headerlink" href="#6.1-Learning-parameters-using-batch-gradient-descent" title="Permalink to this heading"></a></h5>
<p>You will now find the optimal parameters of a linear regression model by using batch gradient descent. Recall batch refers to running all the examples in one iteration. - You don’t need to implement anything for this part. Simply run the cells below.</p>
<ul class="simple">
<li><p>A good way to verify that gradient descent is working correctly is to look at the value of <span class="math notranslate nohighlight">\(J(w,b)\)</span> and check that it is decreasing with each step.</p></li>
<li><p>Assuming you have implemented the gradient and computed the cost correctly and you have an appropriate value for the learning rate alpha, <span class="math notranslate nohighlight">\(J(w,b)\)</span> should never increase and should converge to a steady value by the end of the algorithm.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      x :    (ndarray): Shape (m,)</span>
<span class="sd">      y :    (ndarray): Shape (m,)</span>
<span class="sd">      w_in, b_in : (scalar) Initial values of parameters of the model</span>
<span class="sd">      cost_function: function to compute cost</span>
<span class="sd">      gradient_function: function to compute the gradient</span>
<span class="sd">      alpha : (float) Learning rate</span>
<span class="sd">      num_iters : (int) number of iterations to run gradient descent</span>
<span class="sd">    Returns</span>
<span class="sd">      w : (ndarray): Shape (1,) Updated values of parameters of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">      b : (scalar)                Updated value of parameter of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration — primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>  <span class="c1">#avoid modifying global w within function</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b_in</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_dw</span><span class="p">,</span> <span class="n">dj_db</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">cost</span> <span class="o">=</span>  <span class="n">cost_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span> <span class="n">w_history</span> <span class="c1">#return w and J,w history for graphing</span>
</pre></div>
</div>
</div>
<p>Now let’s run the gradient descent algorithm above to learn the parameters for our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize fitting parameters. Recall that the shape of w is (n,)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">15000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">x_train</span> <span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w,b found by gradient descent:&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost   508.01
Iteration 1500: Cost     4.50
Iteration 3000: Cost     4.48
Iteration 4500: Cost     4.48
Iteration 6000: Cost     4.48
Iteration 7500: Cost     4.48
Iteration 9000: Cost     4.48
Iteration 10500: Cost     4.48
Iteration 12000: Cost     4.48
Iteration 13500: Cost     4.48
w,b found by gradient descent: 1.193033644188364 -3.895780878299615
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>w, b found by gradient descent</p>
</td><td><p>1.16636235 -3.63029143940436</p>
</td></tr></table><p>We will now use the final parameters from gradient descent to plot the linear fit.</p>
<p>Recall that we can get the prediction for a single example <span class="math notranslate nohighlight">\(f(x^{(i)})= wx^{(i)}+b\)</span>.</p>
<p>To calculate the predictions on the entire dataset, we can loop through all the training examples and calculate the prediction for each example. This is shown in the code block below.</p>
</section>
<section id="Assignment2:-My-solution">
<h5>Assignment2: My solution<a class="headerlink" href="#Assignment2:-My-solution" title="Permalink to this heading"></a></h5>
<details><p>My Solution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#my solution: Dictate learning rate automatically,costrain parameter within boundry</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="c1">#add modules from the path</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;/home/amitk/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/C1W2A1&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1">#to show graphs inline</span>
<span class="c1"># load the dataset</span>
<span class="c1">#x_train, y_train = load_data()</span>
<span class="n">x_train</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y_train</span><span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">x_train</span><span class="o">-</span><span class="mi">8</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>


<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">cf</span><span class="o">=</span> <span class="p">(</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">#constraining parameters</span>
        <span class="n">b</span><span class="o">=-</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>

        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="c1">#print(cost_i[i],alpha)</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">10000</span>
<span class="n">Win</span><span class="o">=</span><span class="mi">20</span>
<span class="n">Bin</span><span class="o">=</span><span class="mi">5</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">#print(compute_gradient(x_train,y_train,0.2,0.2))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1">#plt.xlim(0,100)</span>
<span class="c1">#plt.show()</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">predictedamit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">predictedamit</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">wf</span> <span class="o">*</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">bf</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="c1"># Plot the linear fit</span>
<span class="c1">#plt.plot(x_train, predicted, c = &quot;b&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">predictedamit</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predcited model&quot;</span><span class="p">)</span>

<span class="c1"># Create a scatter plot of the data.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</details><details><p>My Solution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Area of triangle&#39;</span><span class="p">)</span>
</pre></div>
</div>
<details><p>See hints</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Area of triangle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</details></details><details><p>Hint to calculate f_wb    For scalars <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (x[i], w and b are all scalars), you can calculate the equation <span class="math notranslate nohighlight">\(h = ab + c\)</span> in code as h = a * b + c</p>
<details><p>    More hints to calculate f       You can compute f_wb as f_wb = w * x[i] + b</p>
</details></details><details><p>Hint to calculate cost     You can calculate the square of a variable z as z**2</p>
<details><p>    More hints to calculate cost     You can compute cost as cost = (f_wb - y[i]) ** 2</p>
</details></details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[69]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">predicted</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
</div>
<p>We will now plot the predicted values to see the linear fit.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the linear fit</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, predictedamit, c = &quot;g&quot;)</span>

<span class="c1"># Create a scatter plot of the data.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Profits vs. Population per city&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Profit in $10,000&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Population of City in 10,000s&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 0, &#39;Population of City in 10,000s&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_139_1.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_139_1.png" />
</div>
</div>
<p>Your final values of <span class="math notranslate nohighlight">\(w,b\)</span> can also be used to make predictions on profits. Let’s predict what the profit would be in areas of 35,000 and 70,000 people.</p>
<ul class="simple">
<li><p>The model takes in population of a city in 10,000s as input.</p></li>
<li><p>Therefore, 35,000 people can be translated into an input to the model as <code class="docutils literal notranslate"><span class="pre">np.array([3.5])</span></code></p></li>
<li><p>Similarly, 70,000 people can be translated into an input to the model as <code class="docutils literal notranslate"><span class="pre">np.array([7.])</span></code></p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[132]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predict1</span> <span class="o">=</span> <span class="mf">3.5</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For population = 35,000, we predict a profit of $</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predict1</span><span class="o">*</span><span class="mi">10000</span><span class="p">))</span>

<span class="n">predict2</span> <span class="o">=</span> <span class="mf">7.0</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;For population = 70,000, we predict a profit of $</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">predict2</span><span class="o">*</span><span class="mi">10000</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
For population = 35,000, we predict a profit of $4519.77
For population = 70,000, we predict a profit of $45342.45
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>For population = 35,000, we predict a profit of</p>
</td><td><p>$4519.77</p>
</td></tr><tr><td><p>For population = 70,000, we predict a profit of</p>
</td><td><p>$45342.45</p>
</td></tr></table><div style="margin-bottom: 8vh;"></div><hr style="height: 3px; background-color: black;"><div style="margin-bottom: 10vh;"></div></section>
</section>
</section>
</section>
<section id="Module---3">
<h2>Module - 3<a class="headerlink" href="#Module---3" title="Permalink to this heading"></a></h2>
<section id="Optional-Lab-W3">
<h3>Optional Lab W3<a class="headerlink" href="#Optional-Lab-W3" title="Permalink to this heading"></a></h3>
<section id="Optional-Lab---3.1:-Classification">
<h4>Optional Lab - 3.1: Classification<a class="headerlink" href="#Optional-Lab---3.1:-Classification" title="Permalink to this heading"></a></h4>
<p>In this lab, you will contrast regression and classification.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[133]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">dlc</span><span class="p">,</span> <span class="n">plot_data</span>
<span class="kn">from</span> <span class="nn">plt_one_addpt_onclick</span> <span class="kn">import</span> <span class="n">plt_one_addpt_onclick</span>
<br/></pre></div>
</div>
</div>
<section id="Classification-Problems">
<h5>Classification Problems<a class="headerlink" href="#Classification-Problems" title="Permalink to this heading"></a></h5>
<p><img alt="10ff97d0b1d84fc98a42b3aaa90adb9e" src="../../_images/C1_W3_Classification.png" /> Examples of classification problems are things like: identifying email as Spam or Not Spam or determining if a tumor is malignant or benign. In particular, these are examples of <em>binary</em> classification where there are two possible outcomes. Outcomes can be described in pairs of ‘positive’/’negative’ such as ‘yes’/’no, ‘true’/’false’ or ‘1’/’0’.</p>
<p>Plots of classification data sets often use symbols to indicate the outcome of an example. In the plots below, ‘X’ is used to represent the positive values while ‘O’ represents negative outcomes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[134]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">X_train2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y_train2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[135]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">neg</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="c1">#plot 1, single variable</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">pos</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y=1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">neg</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">neg</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;y=0&quot;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span>
              <span class="n">edgecolors</span><span class="o">=</span><span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.08</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;one variable plot&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1">#plot 2, two variables</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train2</span><span class="p">,</span> <span class="n">y_train2</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;two variable plot&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_150_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_150_0.png" />
</div>
</div>
<p>Note in the plots above: - In the single variable plot, positive results are shown both a red ‘X’s and as y=1. Negative results are blue ‘O’s and are located at y=0. - Recall in the case of linear regression, y would not have been limited to two values but could have been any value. - In the two-variable plot, the y axis is not available. Positive results are shown as red ‘X’s, while negative results use the blue ‘O’ symbol. - Recall in the case of linear regression with multiple variables, y
would not have been limited to two values and a similar plot would have been three-dimensional.</p>
</section>
<section id="Linear-Regression-approach">
<h5>Linear Regression approach<a class="headerlink" href="#Linear-Regression-approach" title="Permalink to this heading"></a></h5>
<p>In the previous week, you applied linear regression to build a prediction model. Let’s try that approach here using the simple example that was described in the lecture. The model will predict if a tumor is benign or malignant based on tumor size. Try the following: - Click on ‘Run Linear Regression’ to find the best linear regression model for the given data. - Note the resulting linear model does <strong>not</strong> match the data well. One option to improve the results is to apply a <em>threshold</em>. - Tick
the box on the ‘Toggle 0.5 threshold’ to show the predictions if a threshold is applied. - These predictions look good, the predictions match the data - <em>Important</em>: Now, add further ‘malignant’ data points on the far right, in the large tumor size range (near 10), and re-run linear regression. - Now, the model predicts the larger tumor, but data point at x=3 is being incorrectly predicted! - to clear/renew the plot, rerun the cell containing the plot command.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[136]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b_in</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">addpt</span> <span class="o">=</span> <span class="n">plt_one_addpt_onclick</span><span class="p">(</span> <span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">logistic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_153_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Supervised_153_0.png" />
</div>
</div>
<p>The example above demonstrates that the linear model is insufficient to model categorical data. The model can be extended as described in the following lab.</p>
<p>In this lab you: - explored categorical data sets and plotting - determined that linear regression was insufficient for a classification problem.</p>
</section>
</section>
<section id="Optional-Lab---3.2:-Logistic-Regression">
<h4>Optional Lab - 3.2: Logistic Regression<a class="headerlink" href="#Optional-Lab---3.2:-Logistic-Regression" title="Permalink to this heading"></a></h4>
<p>In this ungraded lab, you will - explore the sigmoid function (also known as the logistic function) - explore logistic regression; which uses the sigmoid function</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">plt_one_addpt_onclick</span> <span class="kn">import</span> <span class="n">plt_one_addpt_onclick</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">draw_vthresh</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Sigmoid-or-Logistic-Function">
<h5>Sigmoid or Logistic Function<a class="headerlink" href="#Sigmoid-or-Logistic-Function" title="Permalink to this heading"></a></h5>
<p><a href="#id9"><span class="problematic" id="id10">|</span></a>e04c3e9635dc44eda42b419a9fd5b907|As discussed in the lecture videos, for a classification task, we can start by using our linear regression model, <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b\)</span>, to predict <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. - However, we would like the predictions of our classification model to be between 0 and 1 since our output variable <span class="math notranslate nohighlight">\(y\)</span> is either 0 or 1. - This can be accomplished by using a “sigmoid function” which maps all input values
to values between 0 and 1.</p>
<p>Let’s implement the sigmoid function and see this for ourselves.</p>
</section>
<section id="Formula-for-Sigmoid-function">
<h5>Formula for Sigmoid function<a class="headerlink" href="#Formula-for-Sigmoid-function" title="Permalink to this heading"></a></h5>
<p>The formula for a sigmoid function is as follows -</p>
<p><span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\tag{1}\)</span></p>
<p>In the case of logistic regression, z (the input to the sigmoid function), is the output of a linear regression model. - In the case of a single example, <span class="math notranslate nohighlight">\(z\)</span> is scalar. - in the case of multiple examples, <span class="math notranslate nohighlight">\(z\)</span> may be a vector consisting of <span class="math notranslate nohighlight">\(m\)</span> values, one for each example. - The implementation of the sigmoid function should cover both of these potential input formats. Let’s implement this in Python.</p>
<p>NumPy has a function called <code class="docutils literal notranslate"><span class="pre">`exp()</span></code> &lt;<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">https://numpy.org/doc/stable/reference/generated/numpy.exp.html</a>&gt;`__, which offers a convenient way to calculate the exponential ( <span class="math notranslate nohighlight">\(e^{z}\)</span>) of all elements in the input array (<code class="docutils literal notranslate"><span class="pre">z</span></code>).</p>
<p>It also works with a single number as an input, as shown below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Input is an array.</span>
<span class="n">input_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">exp_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">input_array</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to exp:&quot;</span><span class="p">,</span> <span class="n">input_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output of exp:&quot;</span><span class="p">,</span> <span class="n">exp_array</span><span class="p">)</span>

<span class="c1"># Input is a single number</span>
<span class="n">input_val</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">exp_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">input_val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to exp:&quot;</span><span class="p">,</span> <span class="n">input_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output of exp:&quot;</span><span class="p">,</span> <span class="n">exp_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Input to exp: [1 2 3]
Output of exp: [ 2.72  7.39 20.09]
Input to exp: 1
Output of exp: 2.718281828459045
</pre></div></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function is implemented in python as shown in the cell below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Args:</span>
<span class="sd">        z (ndarray): A scalar, numpy array of any size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        g (ndarray): sigmoid(z), with the same shape as z</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
<p>Let’s see what the output of this function is for various value of <code class="docutils literal notranslate"><span class="pre">z</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate an array of evenly spaced values between -10 and 10</span>
<span class="n">z_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>

<span class="c1"># Use the function implemented above to get the sigmoid values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_tmp</span><span class="p">)</span>

<span class="c1"># Code for pretty printing the two arrays next to each other</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input (z), Output (sigmoid(z))&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">z_tmp</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Input (z), Output (sigmoid(z))
[[-1.000e+01  4.540e-05]
 [-9.000e+00  1.234e-04]
 [-8.000e+00  3.354e-04]
 [-7.000e+00  9.111e-04]
 [-6.000e+00  2.473e-03]
 [-5.000e+00  6.693e-03]
 [-4.000e+00  1.799e-02]
 [-3.000e+00  4.743e-02]
 [-2.000e+00  1.192e-01]
 [-1.000e+00  2.689e-01]
 [ 0.000e+00  5.000e-01]
 [ 1.000e+00  7.311e-01]
 [ 2.000e+00  8.808e-01]
 [ 3.000e+00  9.526e-01]
 [ 4.000e+00  9.820e-01]
 [ 5.000e+00  9.933e-01]
 [ 6.000e+00  9.975e-01]
 [ 7.000e+00  9.991e-01]
 [ 8.000e+00  9.997e-01]
 [ 9.000e+00  9.999e-01]
 [ 1.000e+01  1.000e+00]]
</pre></div></div>
</div>
<p>The values in the left column are <code class="docutils literal notranslate"><span class="pre">z</span></code>, and the values in the right column are <code class="docutils literal notranslate"><span class="pre">sigmoid(z)</span></code>. As you can see, the input values to the sigmoid range from -10 to 10, and the output values range from 0 to 1.</p>
<p>Now, let’s try to plot this function using the <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> library.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot z vs sigmoid(z)</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z_tmp</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sigmoid function&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;sigmoid(z)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="n">draw_vthresh</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0d41de41e5bc45b4aec2927df8547b3f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>As you can see, the sigmoid function approaches <code class="docutils literal notranslate"><span class="pre">0</span></code> as <code class="docutils literal notranslate"><span class="pre">z</span></code> goes to large negative values and approaches <code class="docutils literal notranslate"><span class="pre">1</span></code> as <code class="docutils literal notranslate"><span class="pre">z</span></code> goes to large positive values.</p>
</section>
<section id="Logistic-Regression">
<h5>Logistic Regression<a class="headerlink" href="#Logistic-Regression" title="Permalink to this heading"></a></h5>
<p><img alt="596815c54cc341e09eba907fc9192e5e" src="../../_images/C1_W3_LogisticRegression_right.png" /> A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x}^{(i)} + b ) \tag{2}\]</div>
<p>where</p>
<p><span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\tag{3}\)</span></p>
<div class="line-block">
<div class="line">Let’s apply logistic regression to the categorical data example of tumor classification.</div>
<div class="line">First, load the examples and initial values for the parameters.</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b_in</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
<p>Try the following steps: - Click on ‘Run Logistic Regression’ to find the best logistic regression model for the given training data - Note the resulting model fits the data quite well. - Note, the orange line is ‘<span class="math notranslate nohighlight">\(z\)</span>’ or <span class="math notranslate nohighlight">\(\mathbf{w} \cdot \mathbf{x}^{(i)} + b\)</span> above. It does not match the line in a linear regression model. Further improve these results by applying a <em>threshold</em>. - Tick the box on the ‘Toggle 0.5 threshold’ to show the predictions if a threshold is applied. - These
predictions look good. The predictions match the data - Now, add further data points in the large tumor size range (near 10), and re-run logistic regression. - unlike the linear regression model, this model continues to make correct predictions</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">addpt</span> <span class="o">=</span> <span class="n">plt_one_addpt_onclick</span><span class="p">(</span> <span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">logistic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "84227036e17f47eba77c8cdf8b3dd0e0", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>You have explored the use of the sigmoid function in logistic regression.</p>
</section>
</section>
<section id="Optional-Lab---3.3:-Logistic-Regression,-Decision-Boundary">
<h4>Optional Lab - 3.3: Logistic Regression, Decision Boundary<a class="headerlink" href="#Optional-Lab---3.3:-Logistic-Regression,-Decision-Boundary" title="Permalink to this heading"></a></h4>
<section id="id11">
<h5>Goals<a class="headerlink" href="#id11" title="Permalink to this heading"></a></h5>
<p>In this lab, you will: - Plot the decision boundary for a logistic regression model. This will give you a better sense of what the model is predicting.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">plot_data</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">draw_vthresh</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Dataset">
<h5>Dataset<a class="headerlink" href="#Dataset" title="Permalink to this heading"></a></h5>
<p>Let’s suppose you have following training dataset - The input variable <code class="docutils literal notranslate"><span class="pre">X</span></code> is a numpy array which has 6 training examples, each with two features - The output variable <code class="docutils literal notranslate"><span class="pre">y</span></code> is also a numpy array with 6 examples, and <code class="docutils literal notranslate"><span class="pre">y</span></code> is either <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">1</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Plot-data">
<h5>Plot data<a class="headerlink" href="#Plot-data" title="Permalink to this heading"></a></h5>
<p>Let’s use a helper function to plot this data. The data points with label <span class="math notranslate nohighlight">\(y=1\)</span> are shown as red crosses, while the data points with label <span class="math notranslate nohighlight">\(y=0\)</span> are shown as blue circles.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "90cfbf3f56164a6bbf90eb497eaf9206", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="Logistic-regression-model">
<h5>Logistic regression model<a class="headerlink" href="#Logistic-regression-model" title="Permalink to this heading"></a></h5>
<ul>
<li><p>Suppose you’d like to train a logistic regression model on this data which has the form</p>
<p><span class="math notranslate nohighlight">\(f(x) = g(w_0x_0+w_1x_1 + b)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\)</span>, which is the sigmoid function</p>
</li>
<li><p>Let’s say that you trained the model and get the parameters as <span class="math notranslate nohighlight">\(b = -3, w_0 = 1, w_1 = 1\)</span>. That is,</p>
<p><span class="math notranslate nohighlight">\(f(x) = g(x_0+x_1-3)\)</span></p>
<p>(You’ll learn how to fit these parameters to the data further in the course)</p>
</li>
</ul>
<p>Let’s try to understand what this trained model is predicting by plotting its decision boundary</p>
</section>
<section id="Refresher-on-logistic-regression-and-decision-boundary">
<h5>Refresher on logistic regression and decision boundary<a class="headerlink" href="#Refresher-on-logistic-regression-and-decision-boundary" title="Permalink to this heading"></a></h5>
<ul>
<li><p>Recall that for logistic regression, the model is represented as</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) \tag{1}\]</div>
<p>where <span class="math notranslate nohighlight">\(g(z)\)</span> is known as the sigmoid function and it maps all input values to values between 0 and 1:</p>
<p><span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\tag{2}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w} \cdot \mathbf{x}\)</span> is the vector dot product:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} \cdot \mathbf{x} = w_0 x_0 + w_1 x_1\]</div>
</li>
<li><p>We interpret the output of the model (<span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x)\)</span>) as the probability that <span class="math notranslate nohighlight">\(y=1\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and parameterized by <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>Therefore, to get a final prediction (<span class="math notranslate nohighlight">\(y=0\)</span> or <span class="math notranslate nohighlight">\(y=1\)</span>) from the logistic regression model, we can use the following heuristic -</p>
<p>if <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x) &gt;= 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x) &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y=0\)</span></p>
</li>
<li><p>Let’s plot the sigmoid function to see where <span class="math notranslate nohighlight">\(g(z) &gt;= 0.5\)</span></p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot sigmoid(z) over a range of values from -10 to 10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="c1"># Plot z vs sigmoid(z)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sigmoid function&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;sigmoid(z)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="n">draw_vthresh</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ae38febdc99d4cb5b53ec4817b2a1308", "version_major": 2, "version_minor": 0}</script></div>
</div>
<ul>
<li><p>As you can see, <span class="math notranslate nohighlight">\(g(z) &gt;= 0.5\)</span> for <span class="math notranslate nohighlight">\(z &gt;=0\)</span></p></li>
<li><p>For a logistic regression model, <span class="math notranslate nohighlight">\(z = \mathbf{w} \cdot \mathbf{x} + b\)</span>. Therefore,</p>
<p>if <span class="math notranslate nohighlight">\(\mathbf{w} \cdot \mathbf{x} + b &gt;= 0\)</span>, the model predicts <span class="math notranslate nohighlight">\(y=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(\mathbf{w} \cdot \mathbf{x} + b &lt; 0\)</span>, the model predicts <span class="math notranslate nohighlight">\(y=0\)</span></p>
</li>
</ul>
</section>
<section id="Plotting-decision-boundary">
<h5>Plotting decision boundary<a class="headerlink" href="#Plotting-decision-boundary" title="Permalink to this heading"></a></h5>
<p>Now, let’s go back to our example to understand how the logistic regression model is making predictions.</p>
<ul>
<li><p>Our logistic regression model has the form</p>
<p><span class="math notranslate nohighlight">\(f(\mathbf{x}) = g(-3 + x_0+x_1)\)</span></p>
</li>
<li><p>From what you’ve learnt above, you can see that this model predicts <span class="math notranslate nohighlight">\(y=1\)</span> if <span class="math notranslate nohighlight">\(-3 + x_0+x_1 &gt;= 0\)</span></p></li>
</ul>
<p>Let’s see what this looks like graphically. We’ll start by plotting <span class="math notranslate nohighlight">\(-3 + x_0+x_1 = 0\)</span>, which is equivalent to <span class="math notranslate nohighlight">\(x_1 = 3 - x_0\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Choose values between 0 and 6</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="n">x0</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1"># Plot the decision boundary</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>

<span class="c1"># Fill the region below the line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Plot the original data</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3125e99faed345b69736fa80661c880b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<ul class="simple">
<li><p>In the plot above, the blue line represents the line <span class="math notranslate nohighlight">\(x_0 + x_1 - 3 = 0\)</span> and it should intersect the x1 axis at 3 (if we set <span class="math notranslate nohighlight">\(x_1\)</span> = 3, <span class="math notranslate nohighlight">\(x_0\)</span> = 0) and the x0 axis at 3 (if we set <span class="math notranslate nohighlight">\(x_1\)</span> = 0, <span class="math notranslate nohighlight">\(x_0\)</span> = 3).</p></li>
<li><p>The shaded region represents <span class="math notranslate nohighlight">\(-3 + x_0+x_1 &lt; 0\)</span>. The region above the line is <span class="math notranslate nohighlight">\(-3 + x_0+x_1 &gt; 0\)</span>.</p></li>
<li><p>Any point in the shaded region (under the line) is classified as <span class="math notranslate nohighlight">\(y=0\)</span>. Any point on or above the line is classified as <span class="math notranslate nohighlight">\(y=1\)</span>. This line is known as the “decision boundary”.</p></li>
</ul>
<p>As we’ve seen in the lectures, by using higher order polynomial terms (eg: <span class="math notranslate nohighlight">\(f(x) = g( x_0^2 + x_1 -1)\)</span>, we can come up with more complex non-linear boundaries.</p>
<p>You have explored the decision boundary in the context of logistic regression.</p>
</section>
</section>
<section id="Optional-Lab---3.4:-Logistic-Regression,-Logistic-Loss">
<h4>Optional Lab - 3.4: Logistic Regression, Logistic Loss<a class="headerlink" href="#Optional-Lab---3.4:-Logistic-Regression,-Logistic-Loss" title="Permalink to this heading"></a></h4>
<p>In this ungraded lab, you will: - explore the reason the squared error loss is not appropriate for logistic regression - explore the logistic loss function</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">plt_logistic_loss</span> <span class="kn">import</span>  <span class="n">plt_logistic_cost</span><span class="p">,</span> <span class="n">plt_two_logistic_loss_curves</span><span class="p">,</span> <span class="n">plt_simple_example</span>
<span class="kn">from</span> <span class="nn">plt_logistic_loss</span> <span class="kn">import</span> <span class="n">soup_bowl</span><span class="p">,</span> <span class="n">plt_logistic_squared_error</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Squared-error-for-logistic-regression?">
<h5>Squared error for logistic regression?<a class="headerlink" href="#Squared-error-for-logistic-regression?" title="Permalink to this heading"></a></h5>
<p><img alt="0f2940278bcc465b9442a337d5f76b7b" src="../../_images/C1_W3_SqErrorVsLogistic.png" /> Recall for <strong>Linear</strong> Regression we have used the <strong>squared error cost function</strong>: The equation for the squared error cost with one variable is:</p>
<div class="math notranslate nohighlight">
\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}\]</div>
<p>Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soup_bowl</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "922a3db9c8af46d98eecf7964c29f548", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, <span class="math notranslate nohighlight">\(f_{wb}(x)\)</span> now has a non-linear component, the sigmoid function: <span class="math notranslate nohighlight">\(f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )\)</span>. Let’s try a squared error cost on the example from an earlier lab, now including the sigmoid.</p>
<p>Here is our training data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>
<span class="n">plt_simple_example</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6c93f5ec05b548be9b62d03284e82aea", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now, let’s get a surface plot of the cost using a <em>squared error cost</em>:</p>
<div class="math notranslate nohighlight">
\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">plt_logistic_squared_error</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4307231f4fea435d97d834de77a98d6a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>While this produces a pretty interesting plot, the surface above not nearly as smooth as the ‘soup bowl’ from linear regression!</p>
<p>Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below.</p>
</section>
<section id="Logistic-Loss-Function">
<h5>Logistic Loss Function<a class="headerlink" href="#Logistic-Loss-Function" title="Permalink to this heading"></a></h5>
<p><img alt="f64a756523a4467fb0697a6a91c9f345" src="../../_images/C1_W3_LogisticLoss_a.png" /> <img alt="ad1c4b1e795341a7b240951cac3eac5c" src="../../_images/C1_W3_LogisticLoss_b.png" /> <img alt="04c9e1c904f644f180276a3d43298a34" src="../../_images/C1_W3_LogisticLoss_c.png" /></p>
<p>Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number.</p>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Definition Note:</strong> In this course, these definitions are used:</div>
<div class="line"><strong>Loss</strong> is a measure of the difference of a single example to its target value while the</div>
<div class="line"><strong>Cost</strong> is a measure of the losses over the training set</div>
</div>
</div></blockquote>
<p>This is defined: * <span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is:</p>
<dl>
<dt>:nbsphinx-math:<a href="#id13"><span class="problematic" id="id14">`</span></a>begin{equation}</dt><dd><dl class="simple">
<dt>loss(f_{mathbf{w},b}(mathbf{x}^{(i)}), y^{(i)}) = begin{cases}</dt><dd><ul class="simple">
<li><p>logleft(f_{mathbf{w},b}left( mathbf{x}^{(i)} right) right) &amp; text{if $y^{(i)}=1$}\</p></li>
<li><p>log left( 1 - f_{mathbf{w},b}left( mathbf{x}^{(i)} right) right) &amp; text{if $y^{(i)}=0$}</p></li>
</ul>
</dd>
</dl>
<p>end{cases}</p>
</dd>
</dl>
<p>end{equation}`</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the target value.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p></li>
</ul>
<p>The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or (<span class="math notranslate nohighlight">\(y=0\)</span>) and another for when the target is one (<span class="math notranslate nohighlight">\(y=1\)</span>). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_two_logistic_loss_curves</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e91a13c1e88e448cba79ffea37acbec7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}\)</span> which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1.</p>
<p>The loss function above can be rewritten to be easier to implement.</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\]</div>
<div class="line-block">
<div class="line">This is a rather formidable-looking equation. It is less daunting when you consider <span class="math notranslate nohighlight">\(y^{(i)}\)</span> can have only two values, 0 and 1. One can then consider the equation in two pieces:</div>
<div class="line">when $ y^{(i)} = 0$, the left-hand term is eliminated:</div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 0) &amp;= (-(0) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 0\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \\
&amp;= -\log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}\end{split}\]</div>
<p>and when $ y^{(i)} = 1$, the right-hand term is eliminated:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 1) &amp;=  (-(1) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 1\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\\
  &amp;=  -\log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}\end{split}\]</div>
</div></blockquote>
<p>OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let’s take a look at the cost vs parameters curve for the simple example we considered above:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">cst</span> <span class="o">=</span> <span class="n">plt_logistic_cost</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0eb55315dcbd47fb826e8b23f1223d47", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline. Reminder: you can rotate the above plots using your mouse.</p>
<p>You have: - determined a squared error loss function is not suitable for classification tasks - developed and examined the logistic loss function which <strong>is</strong> suitable for classification tasks.</p>
</section>
</section>
<section id="Optional-Lab---3.5:-Cost-Function-for-Logistic-Regression">
<h4>Optional Lab - 3.5: Cost Function for Logistic Regression<a class="headerlink" href="#Optional-Lab---3.5:-Cost-Function-for-Logistic-Regression" title="Permalink to this heading"></a></h4>
<section id="id15">
<h5>Goals<a class="headerlink" href="#id15" title="Permalink to this heading"></a></h5>
<p>In this lab, you will: - examine the implementation and utilize the cost function for logistic regression.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span>  <span class="n">plot_data</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">dlc</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Dataset</p>
<p>Let’s start with the same dataset as was used in the decision boundary lab.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>  <span class="c1">#(m,n)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>                                           <span class="c1">#(m,)</span>
</pre></div>
</div>
</div>
<p>We will use a helper function to plot this data. The data points with label <span class="math notranslate nohighlight">\(y=1\)</span> are shown as red crosses, while the data points with label <span class="math notranslate nohighlight">\(y=0\)</span> are shown as blue circles.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>

<span class="c1"># Set both axes to be from 0-4</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "20fbf79f304b48f2a6ef2443e0a90472", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="id16">
<h5>Cost function<a class="headerlink" href="#id16" title="Permalink to this heading"></a></h5>
<p>In a previous lab, you developed the <em>logistic loss</em> function. Recall, loss is defined to apply to one example. Here you combine the losses to form the <strong>cost</strong>, which includes all the examples.</p>
<p>Recall that for logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m} \sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]</div>
<p>where * <span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}$$
</pre></div>
</div>
<ul>
<li><p>where m is the number of training examples in the data set and:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  f_{\mathbf{w},b}(\mathbf{x^{(i)}}) &amp;= g(z^{(i)})\tag{3} \\
  z^{(i)} &amp;= \mathbf{w} \cdot \mathbf{x}^{(i)}+ b\tag{4} \\
  g(z^{(i)}) &amp;= \frac{1}{1+e^{-z^{(i)}}}\tag{5}
\end{align}\end{split}\]</div>
</li>
</ul>
</section>
<section id="Code-Description">
<h5>Code Description<a class="headerlink" href="#Code-Description" title="Permalink to this heading"></a></h5>
<p>The algorithm for <code class="docutils literal notranslate"><span class="pre">compute_cost_logistic</span></code> loops over all the examples calculating the loss for each example and accumulating the total.</p>
<p>Note that the variables X and y are not scalar values but matrices of shape (<span class="math notranslate nohighlight">\(m, n\)</span>) and (<span class="math notranslate nohighlight">\(𝑚\)</span>,) respectively, where <span class="math notranslate nohighlight">\(𝑛\)</span> is the number of features and <span class="math notranslate nohighlight">\(𝑚\)</span> is the number of training examples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cost</span>

<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n)): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)) : target values</span>
<span class="sd">      w (ndarray (n,)) : model parameters</span>
<span class="sd">      b (scalar)       : model parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">      cost (scalar): cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_i</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">+=</span>  <span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f_wb_i</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f_wb_i</span><span class="p">)</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">cost</span>
<br/></pre></div>
</div>
</div>
<p>Check the implementation of the cost function using the cell below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost_logistic</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.36686678640551745
</pre></div></div>
</div>
<p><strong>Expected output</strong>: 0.3668667864055175</p>
</section>
<section id="Example">
<h5>Example<a class="headerlink" href="#Example" title="Permalink to this heading"></a></h5>
<p>Now, let’s see what the cost function output is for a different value of <span class="math notranslate nohighlight">\(w\)</span>.</p>
<ul class="simple">
<li><p>In a previous lab, you plotted the decision boundary for <span class="math notranslate nohighlight">\(b = -3, w_0 = 1, w_1 = 1\)</span>. That is, you had <code class="docutils literal notranslate"><span class="pre">b</span> <span class="pre">=</span> <span class="pre">-3,</span> <span class="pre">w</span> <span class="pre">=</span> <span class="pre">np.array([1,1])</span></code>.</p></li>
<li><p>Let’s say you want to see if <span class="math notranslate nohighlight">\(b = -4, w_0 = 1, w_1 = 1\)</span>, or <code class="docutils literal notranslate"><span class="pre">b</span> <span class="pre">=</span> <span class="pre">-4,</span> <span class="pre">w</span> <span class="pre">=</span> <span class="pre">np.array([1,1])</span></code> provides a better model.</p></li>
</ul>
<p>Let’s first plot the decision boundary for these two different <span class="math notranslate nohighlight">\(b\)</span> values to see which one fits the data better.</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(b = -3, w_0 = 1, w_1 = 1\)</span>, we’ll plot <span class="math notranslate nohighlight">\(-3 + x_0+x_1 = 0\)</span> (shown in blue)</p></li>
<li><p>For <span class="math notranslate nohighlight">\(b = -4, w_0 = 1, w_1 = 1\)</span>, we’ll plot <span class="math notranslate nohighlight">\(-4 + x_0+x_1 = 0\)</span> (shown in magenta)</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Choose values between 0 and 6</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Plot the two decision boundaries</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="n">x0</span>
<span class="n">x1_other</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">-</span> <span class="n">x0</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1"># Plot the decision boundary</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$b$=-3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1_other</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlmagenta&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$b$=-4&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Plot the original data</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Boundary&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "37eefc7624bd496c8209f47662e1e4bc", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>You can see from this plot that <code class="docutils literal notranslate"><span class="pre">b</span> <span class="pre">=</span> <span class="pre">-4,</span> <span class="pre">w</span> <span class="pre">=</span> <span class="pre">np.array([1,1])</span></code> is a worse model for the training data. Let’s see if the cost function implementation reflects this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b_1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span>
<span class="n">w_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b_2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost for b = -3 : &quot;</span><span class="p">,</span> <span class="n">compute_cost_logistic</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_array1</span><span class="p">,</span> <span class="n">b_1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost for b = -4 : &quot;</span><span class="p">,</span> <span class="n">compute_cost_logistic</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_array2</span><span class="p">,</span> <span class="n">b_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost for b = -3 :  0.36686678640551745
Cost for b = -4 :  0.5036808636748461
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<p>Cost for b = -3 : 0.3668667864055175</p>
<p>Cost for b = -4 : 0.5036808636748461</p>
<p>You can see the cost function behaves as expected and the cost for <code class="docutils literal notranslate"><span class="pre">b</span> <span class="pre">=</span> <span class="pre">-4,</span> <span class="pre">w</span> <span class="pre">=</span> <span class="pre">np.array([1,1])</span></code> is indeed higher than the cost for <code class="docutils literal notranslate"><span class="pre">b</span> <span class="pre">=</span> <span class="pre">-3,</span> <span class="pre">w</span> <span class="pre">=</span> <span class="pre">np.array([1,1])</span></code></p>
<p>In this lab you examined and utilized the cost function for logistic regression.</p>
</section>
</section>
<section id="Optional-Lab---3.6:-Gradient-Descent-for-Logistic-Regression">
<h4>Optional Lab - 3.6: Gradient Descent for Logistic Regression<a class="headerlink" href="#Optional-Lab---3.6:-Gradient-Descent-for-Logistic-Regression" title="Permalink to this heading"></a></h4>
<section id="id17">
<h5>Goals<a class="headerlink" href="#id17" title="Permalink to this heading"></a></h5>
<p>In this lab, you will: - update gradient descent for logistic regression. - explore gradient descent on a familiar data set</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[67]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span><span class="o">,</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span>  <span class="n">dlc</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">,</span> <span class="n">plt_tumor_data</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">compute_cost_logistic</span>
<span class="kn">from</span> <span class="nn">plt_quad_logistic</span> <span class="kn">import</span> <span class="n">plt_quad_logistic</span><span class="p">,</span> <span class="n">plt_prob</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Data set</p>
<p>Let’s start with the same two feature data set used in the decision boundary lab.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>As before, we’ll use a helper function to plot this data. The data points with label <span class="math notranslate nohighlight">\(y=1\)</span> are shown as red crosses, while the data points with label <span class="math notranslate nohighlight">\(y=0\)</span> are shown as blue circles.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9936d6eae641455f90636f5b551af09c", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="Logistic-Gradient-Descent">
<h5>Logistic Gradient Descent<a class="headerlink" href="#Logistic-Gradient-Descent" title="Permalink to this heading"></a></h5>
<p><img alt="19c3955cf05b4e4c9673bd5b8e4cf42b" src="../../_images/C1_W3_Logistic_gradient_descent.png" /></p>
<p>Recall the gradient descent algorithm utilizes the gradient calculation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\text{repeat until convergence:} \; \lbrace \\
&amp;  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1} \\
&amp;  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
&amp;\rbrace
\end{align*}\end{split}\]</div>
<p>Where each iteration performs simultaneous updates on <span class="math notranslate nohighlight">\(w_j\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>, where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{2} \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{3}
\end{align*}\end{split}\]</div>
<ul class="simple">
<li><p>m is the number of training examples in the data set</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the target</p></li>
<li><p>For a logistic regression model
<span class="math notranslate nohighlight">\(z = \mathbf{w} \cdot \mathbf{x} + b\)</span>
<span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x) = g(z)\)</span>
where <span class="math notranslate nohighlight">\(g(z)\)</span> is the sigmoid function:
<span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\)</span></p></li>
</ul>
</section>
<section id="Gradient-Descent-Implementation">
<h5>Gradient Descent Implementation<a class="headerlink" href="#Gradient-Descent-Implementation" title="Permalink to this heading"></a></h5>
<p>The gradient descent algorithm implementation has two components: - The loop implementing equation (1) above. This is <code class="docutils literal notranslate"><span class="pre">gradient_descent</span></code> below and is generally provided to you in optional and practice labs. - The calculation of the current gradient, equations (2,3) above. This is <code class="docutils literal notranslate"><span class="pre">compute_gradient_logistic</span></code> below. You will be asked to implement this week’s practice lab.</p>
<p>Calculating the Gradient, Code Description</p>
<div class="line-block">
<div class="line">Implements equation (2),(3) above for all <span class="math notranslate nohighlight">\(w_j\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. There are many ways to implement this. Outlined below is this: - initialize variables to accumulate <code class="docutils literal notranslate"><span class="pre">dj_dw</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_db</span></code> - for each example - calculate the error for that example <span class="math notranslate nohighlight">\(g(\mathbf{w} \cdot \mathbf{x}^{(i)} + b) - \mathbf{y}^{(i)}\)</span> - for each input value <span class="math notranslate nohighlight">\(x_{j}^{(i)}\)</span> in this example,</div>
<div class="line">- multiply the error by the input <span class="math notranslate nohighlight">\(x_{j}^{(i)}\)</span>, and add to the corresponding element of <code class="docutils literal notranslate"><span class="pre">dj_dw</span></code>. (equation 2 above) - add the error to <code class="docutils literal notranslate"><span class="pre">dj_db</span></code> (equation 3 above)</div>
</div>
<ul class="simple">
<li><p>divide <code class="docutils literal notranslate"><span class="pre">dj_db</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_dw</span></code> by total number of examples (m)</p></li>
<li><p>note that <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> in numpy <code class="docutils literal notranslate"><span class="pre">X[i,:]</span></code> or <code class="docutils literal notranslate"><span class="pre">X[i]</span></code> and <span class="math notranslate nohighlight">\(x_{j}^{(i)}\)</span> is <code class="docutils literal notranslate"><span class="pre">X[i,j]</span></code></p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_gradient_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)): target values</span>
<span class="sd">      w (ndarray (n,)): model parameters</span>
<span class="sd">      b (scalar)      : model parameter</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>                           <span class="c1">#(n,)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>          <span class="c1">#(n,)(n,)=scalar</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="n">f_wb_i</span>  <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>                       <span class="c1">#scalar</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err_i</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>      <span class="c1">#scalar</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err_i</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="o">/</span><span class="n">m</span>                                   <span class="c1">#(n,)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span><span class="o">/</span><span class="n">m</span>                                   <span class="c1">#scalar</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<p>Check the implementation of the gradient function using the cell below.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">dj_db_tmp</span><span class="p">,</span> <span class="n">dj_dw_tmp</span> <span class="o">=</span> <span class="n">compute_gradient_logistic</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db_tmp</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_dw: </span><span class="si">{</span><span class="n">dj_dw_tmp</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db: 0.49861806546328574
dj_dw: [0.498333393278696, 0.49883942983996693]
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db: 0.49861806546328574
dj_dw: [0.498333393278696, 0.49883942983996693]
</pre></div>
</div>
</section>
<section id="Gradient-Descent-Code">
<h5>Gradient Descent Code<a class="headerlink" href="#Gradient-Descent-Code" title="Permalink to this heading"></a></h5>
<p>The code implementing equation (1) above is implemented below. Take a moment to locate and compare the functions in the routine to the equations above.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent</span>

<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n)   : Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,))   : target values</span>
<span class="sd">      w_in (ndarray (n,)): Initial values of model parameters</span>
<span class="sd">      b_in (scalar)      : Initial values of model parameter</span>
<span class="sd">      alpha (float)      : Learning rate</span>
<span class="sd">      num_iters (scalar) : number of iterations to run gradient descent</span>

<span class="sd">    Returns:</span>
<span class="sd">      w (ndarray (n,))   : Updated values of parameters</span>
<span class="sd">      b (scalar)         : Updated value of parameter</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>  <span class="c1">#avoid modifying global w within function</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b_in</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">compute_cost_logistic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">J_history</span>         <span class="c1">#return final w,b and J history for graphing</span>
<br/></pre></div>
</div>
</div>
<p>Let’s run gradient descent on our data set.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_tmp</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">b_tmp</span>  <span class="o">=</span> <span class="mf">0.</span>
<span class="n">alph</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">w_out</span><span class="p">,</span> <span class="n">b_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">iters</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">updated parameters: w:</span><span class="si">{</span><span class="n">w_out</span><span class="si">}</span><span class="s2">, b:</span><span class="si">{</span><span class="n">b_out</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost 0.684610468560574
Iteration 1000: Cost 0.1590977666870457
Iteration 2000: Cost 0.08460064176930078
Iteration 3000: Cost 0.05705327279402531
Iteration 4000: Cost 0.04290759421682
Iteration 5000: Cost 0.03433847729884557
Iteration 6000: Cost 0.02860379802212006
Iteration 7000: Cost 0.02450156960879306
Iteration 8000: Cost 0.02142370332569295
Iteration 9000: Cost 0.019030137124109114

updated parameters: w:[5.281 5.078], b:-14.222409982019837
</pre></div></div>
</div>
</section>
<section id="Let's-plot-the-results-of-gradient-descent:">
<h5>Let’s plot the results of gradient descent:<a class="headerlink" href="#Let's-plot-the-results-of-gradient-descent:" title="Permalink to this heading"></a></h5>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[41]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1"># plot the probability</span>
<span class="n">plt_prob</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">w_out</span><span class="p">,</span> <span class="n">b_out</span><span class="p">)</span>

<span class="c1"># Plot the original data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary</span>
<span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="n">b_out</span><span class="o">/</span><span class="n">w_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x1</span> <span class="o">=</span> <span class="o">-</span><span class="n">b_out</span><span class="o">/</span><span class="n">w_out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="n">x0</span><span class="p">],[</span><span class="n">x1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">dlc</span><span class="p">[</span><span class="s2">&quot;dlblue&quot;</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1a80107a4b844eec838f46c4e3b1fd06", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>In the plot above: - the shading reflects the probability y=1 (result prior to decision boundary) - the decision boundary is the line at which the probability = 0.5</p>
<p>Another Data set</p>
<p>Let’s return to a one-variable data set. With just two parameters, <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, it is possible to plot the cost function using a contour plot to get a better idea of what gradient descent is up to.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>As before, we’ll use a helper function to plot this data. The data points with label <span class="math notranslate nohighlight">\(y=1\)</span> are shown as red crosses, while the data points with label <span class="math notranslate nohighlight">\(y=0\)</span> are shown as blue circles.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt_tumor_data</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3af2159e7476459db03fd403a34b0e15", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>In the plot below, try: - changing <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> by clicking within the contour plot on the upper right. - changes may take a second or two - note the changing value of cost on the upper left plot. - note the cost is accumulated by a loss on each example (vertical dotted lines) - run gradient descent by clicking the orange button. - note the steadily decreasing cost (contour and cost plot are in log(cost) - clicking in the contour plot will reset the model for a new run - to reset the
plot, rerun the cell</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">b_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">14</span><span class="p">])</span>
<span class="n">quad</span> <span class="o">=</span> <span class="n">plt_quad_logistic</span><span class="p">(</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">w_range</span><span class="p">,</span> <span class="n">b_range</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d70229c6782649948a7eab6ee4a3b53c", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>You have: - examined the formulas and implementation of calculating the gradient for logistic regression - utilized those routines in - exploring a single variable data set - exploring a two-variable data set</p>
</section>
</section>
<section id="Optional-Lab---3.7:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn">
<h4>Optional Lab - 3.7: Ungraded Lab: Logistic Regression using Scikit-Learn<a class="headerlink" href="#Optional-Lab---3.7:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn" title="Permalink to this heading"></a></h4>
<section id="id18">
<h5>Goals<a class="headerlink" href="#id18" title="Permalink to this heading"></a></h5>
<p>In this lab you will: - Train a logistic regression model using scikit-learn.</p>
<p>Dataset</p>
<p>Let’s start with the same dataset as before.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</section>
<section id="Fit-the-model">
<h5>Fit the model<a class="headerlink" href="#Fit-the-model" title="Permalink to this heading"></a></h5>
<p>The code below imports the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">logistic regression model</a> from scikit-learn. You can fit this model on the training data by calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression()</pre></div></div></div></div></div></div>
</div>
</section>
<section id="Make-Predictions">
<h5>Make Predictions<a class="headerlink" href="#Make-Predictions" title="Permalink to this heading"></a></h5>
<p>You can see the predictions made by this model by calling the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction on training set:&quot;</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Prediction on training set: [0 0 0 1 1 1]
</pre></div></div>
</div>
</section>
<section id="Calculate-accuracy">
<h5>Calculate accuracy<a class="headerlink" href="#Calculate-accuracy" title="Permalink to this heading"></a></h5>
<p>You can calculate this accuracy of this model by calling the <code class="docutils literal notranslate"><span class="pre">score</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set:&quot;</span><span class="p">,</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Accuracy on training set: 1.0
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Optional-Lab---3.8:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn">
<h4>Optional Lab - 3.8: Ungraded Lab: Logistic Regression using Scikit-Learn<a class="headerlink" href="#Optional-Lab---3.8:-Ungraded-Lab:-Logistic-Regression-using-Scikit-Learn" title="Permalink to this heading"></a></h4>
<section id="Ungraded-Lab:-Overfitting">
<h5>Ungraded Lab: Overfitting<a class="headerlink" href="#Ungraded-Lab:-Overfitting" title="Permalink to this heading"></a></h5>
<p><img alt="9bd3b491919a4a4abe20259989562876" src="../../_images/C1_W3_Overfitting_a.png" /> <img alt="23c91fa1c5f842d89d609b1729858fcf" src="../../_images/C1_W3_Overfitting_b.png" /> <img alt="f35a57e731fa4dacaa31b994dece5db9" src="../../_images/C1_W3_Overfitting_c.png" /></p>
<p>Goals</p>
<p>In this lab, you will explore: - the situations where overfitting can occur - some of the solutions</p>
<p>%matplotlib widget import matplotlib.pyplot as plt from ipywidgets import Output from plt_overfit import overfit_example, output plt.style.use(‘week3/OptionalLabs/deeplearning.mplstyle’)</p>
</section>
<section id="Overfitting">
<h5>Overfitting<a class="headerlink" href="#Overfitting" title="Permalink to this heading"></a></h5>
<p>The week’s lecture described situations where overfitting can arise. Run the cell below to generate a plot that will allow you to explore overfitting. There are further instructions below the cell.</p>
<p>plt.close(“all”) display(output) ofit = overfit_example(False)</p>
<p>In the plot above you can: - switch between Regression and Categorization examples - add data - select the degree of the model - fit the model to the data</p>
<p>Here are some things you should try: - Fit the data with degree = 1; Note ‘underfitting’. - Fit the data with degree = 6; Note ‘overfitting’ - tune degree to get the ‘best fit’ - add data: - extreme examples can increase overfitting (assuming they are outliers). - nominal examples can reduce overfitting - switch between <code class="docutils literal notranslate"><span class="pre">Regression</span></code> and <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> to try both examples.</p>
<p>To reset the plot, re-run the cell. Click slowly to allow the plot to update before receiving the next click.</p>
<p>Notes on implementations: - the ‘ideal’ curves represent the generator model to which noise was added to achieve the data set - ‘fit’ does not use pure gradient descent to improve speed. These methods can be used on smaller data sets.</p>
<p>You have developed some intuition about the causes and solutions to overfitting. In the next lab, you will explore a commonly used solution, Regularization.</p>
</section>
</section>
<section id="Optional-Lab---3.9---Regularized-Cost-and-Gradient">
<h4>Optional Lab - 3.9 - Regularized Cost and Gradient<a class="headerlink" href="#Optional-Lab---3.9---Regularized-Cost-and-Gradient" title="Permalink to this heading"></a></h4>
<p>Goals</p>
<p>In this lab, you will: - extend the previous linear and logistic cost functions with a regularization term. - rerun the previous example of over-fitting with a regularization term added.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">from</span> <span class="nn">lab_utils_common</span> <span class="kn">import</span> <span class="n">sigmoid</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Adding-regularization">
<h5>Adding regularization<a class="headerlink" href="#Adding-regularization" title="Permalink to this heading"></a></h5>
<p><img alt="684b6abc99254b0a84c4b2fe354b4b79" src="../../_images/C1_W3_LinearGradientRegularized.png" /> <img alt="78e712ff3cec48b489bc8803399dac98" src="../../_images/C1_W3_LogisticGradientRegularized.png" /></p>
<p>The slides above show the cost and gradient functions for both linear and logistic regression. Note: - Cost - The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same. - Gradient - The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of <span class="math notranslate nohighlight">\(f_{wb}\)</span>.</p>
</section>
<section id="Cost-functions-with-regularization">
<h5>Cost functions with regularization<a class="headerlink" href="#Cost-functions-with-regularization" title="Permalink to this heading"></a></h5>
<p>Cost function for regularized linear regression</p>
<p>The equation for the cost function regularized linear regression is:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{1}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{2}\]</div>
<p>Compare this to the cost function without regularization (which you implemented in a previous lab), which is of the form:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2\]</div>
<p>The difference is the regularization term, <span class="math notranslate nohighlight">\(\frac{\lambda}{2m} \sum_{j=0}^{n-1} w_j^2\)</span></p>
<p>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter <span class="math notranslate nohighlight">\(b\)</span> is not regularized. This is standard practice.</p>
<p>Below is an implementation of equations (1) and (2). Note that this uses a <em>standard pattern for this course</em>, a <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">loop</span></code> over all <code class="docutils literal notranslate"><span class="pre">m</span></code> examples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost_linear_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)): target values</span>
<span class="sd">      w (ndarray (n,)): model parameters</span>
<span class="sd">      b (scalar)      : model parameter</span>
<span class="sd">      lambda_ (scalar): Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost (scalar):  cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span>  <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n</span>  <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>                                   <span class="c1">#(n,)(n,)=scalar, see np.dot</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="p">(</span><span class="n">f_wb_i</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>                               <span class="c1">#scalar</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>                                              <span class="c1">#scalar</span>

    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">reg_cost</span> <span class="o">+=</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>                                          <span class="c1">#scalar</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>                              <span class="c1">#scalar</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">reg_cost</span>                                       <span class="c1">#scalar</span>
    <span class="k">return</span> <span class="n">total_cost</span>                                                  <span class="c1">#scalar</span>
</pre></div>
</div>
</div>
<p>Run the cell below to see it in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[51]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_tmp</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">cost_tmp</span> <span class="o">=</span> <span class="n">compute_cost_linear_reg</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">,</span> <span class="n">lambda_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost:&quot;</span><span class="p">,</span> <span class="n">cost_tmp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Regularized cost: 0.07917239320214277
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost: 0.07917239320214275</p>
</td></tr></table></section>
<section id="Cost-function-for-regularized-logistic-regression">
<h5>Cost function for regularized logistic regression<a class="headerlink" href="#Cost-function-for-regularized-logistic-regression" title="Permalink to this heading"></a></h5>
<p>For regularized <strong>logistic</strong> regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{3}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = sigmoid(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)  \tag{4}\]</div>
<p>Compare this to the cost function without regularization (which you implemented in a previous lab):</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]</div>
<p>As was the case in linear regression above, the difference is the regularization term, which is <span class="math notranslate nohighlight">\(\frac{\lambda}{2m} \sum_{j=0}^{n-1} w_j^2\)</span></p>
<p>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter <span class="math notranslate nohighlight">\(b\)</span> is not regularized. This is standard practice.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[52]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost_logistic_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)): target values</span>
<span class="sd">      w (ndarray (n,)): model parameters</span>
<span class="sd">      b (scalar)      : model parameter</span>
<span class="sd">      lambda_ (scalar): Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost (scalar):  cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span><span class="n">n</span>  <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>                                      <span class="c1">#(n,)(n,)=scalar, see np.dot</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_i</span><span class="p">)</span>                                          <span class="c1">#scalar</span>
        <span class="n">cost</span> <span class="o">+=</span>  <span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f_wb_i</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f_wb_i</span><span class="p">)</span>      <span class="c1">#scalar</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">/</span><span class="n">m</span>                                                      <span class="c1">#scalar</span>

    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">reg_cost</span> <span class="o">+=</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>                                          <span class="c1">#scalar</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>                              <span class="c1">#scalar</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">reg_cost</span>                                       <span class="c1">#scalar</span>
    <span class="k">return</span> <span class="n">total_cost</span>                                                  <span class="c1">#scalar</span>
</pre></div>
</div>
</div>
<p>Run the cell below to see it in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[53]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_tmp</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">cost_tmp</span> <span class="o">=</span> <span class="n">compute_cost_logistic_reg</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">,</span> <span class="n">lambda_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost:&quot;</span><span class="p">,</span> <span class="n">cost_tmp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Regularized cost: 0.6850849138741673
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost: 0.6850849138741673</p>
</td></tr></table></section>
<section id="Gradient-descent-with-regularization">
<h5>Gradient descent with regularization<a class="headerlink" href="#Gradient-descent-with-regularization" title="Permalink to this heading"></a></h5>
<p>The basic algorithm for running gradient descent does not change with regularization, it is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\text{repeat until convergence:} \; \lbrace \\
&amp;  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1} \\
&amp;  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\
&amp;\rbrace
\end{align*}\end{split}\]</div>
<p>Where each iteration performs simultaneous updates on <span class="math notranslate nohighlight">\(w_j\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>What changes with regularization is computing the gradients.</p>
</section>
<section id="Computing-the-Gradient-with-regularization-(both-linear/logistic)">
<h5>Computing the Gradient with regularization (both linear/logistic)<a class="headerlink" href="#Computing-the-Gradient-with-regularization-(both-linear/logistic)" title="Permalink to this heading"></a></h5>
<p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of <span class="math notranslate nohighlight">\(f_{\mathbf{w}b}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \frac{\lambda}{m} w_j \tag{2} \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &amp;= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{3}
\end{align*}\end{split}\]</div>
<ul>
<li><div class="line-block">
<div class="line">m is the number of training examples in the data set</div>
</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the target</p></li>
<li><div class="line-block">
<div class="line">For a linear regression model</div>
<div class="line"><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x) = \mathbf{w} \cdot \mathbf{x} + b\)</span></div>
</div>
</li>
<li><div class="line-block">
<div class="line">For a logistic regression model</div>
<div class="line"><span class="math notranslate nohighlight">\(z = \mathbf{w} \cdot \mathbf{x} + b\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x) = g(z)\)</span></div>
<div class="line">where <span class="math notranslate nohighlight">\(g(z)\)</span> is the sigmoid function:</div>
<div class="line"><span class="math notranslate nohighlight">\(g(z) = \frac{1}{1+e^{-z}}\)</span></div>
</div>
</li>
</ul>
<p>The term which adds regularization is the $:nbsphinx-math:<cite>frac{lambda}{m}</cite> w_j $.</p>
</section>
<section id="Gradient-function-for-regularized-linear-regression">
<h5>Gradient function for regularized linear regression<a class="headerlink" href="#Gradient-function-for-regularized-linear-regression" title="Permalink to this heading"></a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[54]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_gradient_linear_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>
<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)): target values</span>
<span class="sd">      w (ndarray (n,)): model parameters</span>
<span class="sd">      b (scalar)      : model parameter</span>
<span class="sd">      lambda_ (scalar): Controls amount of regularization</span>

<span class="sd">    Returns:</span>
<span class="sd">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>           <span class="c1">#(number of examples, number of features)</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span> <span class="o">/</span> <span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">/</span> <span class="n">m</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<p>Run the cell below to see it in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_tmp</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">dj_db_tmp</span><span class="p">,</span> <span class="n">dj_dw_tmp</span> <span class="o">=</span>  <span class="n">compute_gradient_linear_reg</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">,</span> <span class="n">lambda_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db_tmp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw_tmp</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db: 0.6648774569425726
Regularized dj_dw:
 [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]
</pre></div></div>
</div>
<p><strong>Expected Output</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db: 0.6648774569425726
Regularized dj_dw:
 [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]
</pre></div>
</div>
</section>
<section id="Gradient-function-for-regularized-logistic-regression">
<h5>Gradient function for regularized logistic regression<a class="headerlink" href="#Gradient-function-for-regularized-logistic-regression" title="Permalink to this heading"></a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_gradient_logistic_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X (ndarray (m,n): Data, m examples with n features</span>
<span class="sd">      y (ndarray (m,)): target values</span>
<span class="sd">      w (ndarray (n,)): model parameters</span>
<span class="sd">      b (scalar)      : model parameter</span>
<span class="sd">      lambda_ (scalar): Controls amount of regularization</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>                            <span class="c1">#(n,)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.0</span>                                       <span class="c1">#scalar</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>          <span class="c1">#(n,)(n,)=scalar</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="n">f_wb_i</span>  <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>                       <span class="c1">#scalar</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err_i</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>      <span class="c1">#scalar</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err_i</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="o">/</span><span class="n">m</span>                                   <span class="c1">#(n,)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span><span class="o">/</span><span class="n">m</span>                                   <span class="c1">#scalar</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
<br/></pre></div>
</div>
</div>
<p>Run the cell below to see it in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[57]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">w_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_tmp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b_tmp</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_tmp</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">dj_db_tmp</span><span class="p">,</span> <span class="n">dj_dw_tmp</span> <span class="o">=</span>  <span class="n">compute_gradient_logistic_reg</span><span class="p">(</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y_tmp</span><span class="p">,</span> <span class="n">w_tmp</span><span class="p">,</span> <span class="n">b_tmp</span><span class="p">,</span> <span class="n">lambda_tmp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db_tmp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw_tmp</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db: 0.341798994972791
Regularized dj_dw:
 [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]
</pre></div></div>
</div>
<p><strong>Expected Output</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db: 0.341798994972791
Regularized dj_dw:
 [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]
</pre></div>
</div>
</section>
<section id="Rerun-over-fitting-example">
<h5>Rerun over-fitting example<a class="headerlink" href="#Rerun-over-fitting-example" title="Permalink to this heading"></a></h5>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">ofit</span> <span class="o">=</span> <span class="n">overfit_example</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8571d4e25dd94aa2b4aaf825400340a3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2e19460fa1dd439394e3400171ca0992", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>In the plot above, try out regularization on the previous example. In particular: - Categorical (logistic regression) - set degree to 6, lambda to 0 (no regularization), fit the data - now set lambda to 1 (increase regularization), fit the data, notice the difference. - Regression (linear regression) - try the same procedure.</p>
<p>You have: - examples of cost and gradient routines with regularization added for both linear and logistic regression - developed some intuition on how regularization can reduce over-fitting</p>
</section>
</section>
</section>
<section id="id19">
<h3>Practice Quiz<a class="headerlink" href="#id19" title="Permalink to this heading"></a></h3>
<section id="id20">
<h4>Quiz-1<a class="headerlink" href="#id20" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/gradient_decent_for_logistic.png" style="width: 800px;" /></p>
<center/><figure/></section>
<section id="id21">
<h4>Quiz-2<a class="headerlink" href="#id21" title="Permalink to this heading"></a></h4>
<figure><center><p><img alt="missing" class="no-scaled-link" src="../../_images/costfnlogistic.png" style="width: 800px;" /></p>
<center/><figure/></section>
</section>
<section id="Assignment-W3:">
<h3>Assignment W3:<a class="headerlink" href="#Assignment-W3:" title="Permalink to this heading"></a></h3>
<p>In this exercise, you will implement logistic regression and apply it to two different datasets.</p>
<p>Outline</p>
<ul class="simple">
<li><p>1 - Packages</p></li>
<li><p>2 - Logistic Regression</p>
<ul>
<li><p>2.1 Problem Statement</p></li>
<li><p>2.2 Loading and visualizing the data</p></li>
<li><p>2.3 Sigmoid function</p></li>
<li><p>2.4 Cost function for logistic regression</p></li>
<li><p>2.5 Gradient for logistic regression</p></li>
<li><p>2.6 Learning parameters using gradient descent</p></li>
<li><p>2.7 Plotting the decision boundary</p></li>
<li><p>2.8 Evaluating logistic regression</p></li>
</ul>
</li>
<li><p>3 - Regularized Logistic Regression</p>
<ul>
<li><p>3.1 Problem Statement</p></li>
<li><p>3.2 Loading and visualizing the data</p></li>
<li><p>3.3 Feature mapping</p></li>
<li><p>3.4 Cost function for regularized logistic regression</p></li>
<li><p>3.5 Gradient for regularized logistic regression</p></li>
<li><p>3.6 Learning parameters using gradient descent</p></li>
<li><p>3.7 Plotting the decision boundary</p></li>
<li><p>3.8 Evaluating regularized logistic regression model</p></li>
</ul>
</li>
</ul>
<section id="id22">
<h4>1 - Packages<a class="headerlink" href="#id22" title="Permalink to this heading"></a></h4>
<p>First, let’s run the cell below to import all the packages that you will need during this assignment. - <a class="reference external" href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python. - <a class="reference external" href="http://matplotlib.org">matplotlib</a> is a famous library to plot graphs in Python. - <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> contains helper functions for this assignment. You do not need to modify code in this file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[64]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</section>
<section id="2---Logistic-Regression">
<h4>2 - Logistic Regression<a class="headerlink" href="#2---Logistic-Regression" title="Permalink to this heading"></a></h4>
<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<section id="2.1-Problem-Statement">
<h5>2.1 Problem Statement<a class="headerlink" href="#2.1-Problem-Statement" title="Permalink to this heading"></a></h5>
<p>Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. * You have historical data from previous applicants that you can use as a training set for logistic regression. * For each training example, you have the applicant’s scores on two exams and the admissions decision. * Your task is to build a classification model that estimates an applicant’s probability of admission based on the
scores from those two exams.</p>
</section>
<section id="2.2-Loading-and-visualizing-the-data">
<h5>2.2 Loading and visualizing the data<a class="headerlink" href="#2.2-Loading-and-visualizing-the-data" title="Permalink to this heading"></a></h5>
<p>You will start by loading the dataset for this task. - The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> - <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains exam scores on two exams for a student - <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is the admission decision - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the student was admitted - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the student was not admitted - Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data1.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[66], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># load dataset</span>
<span class="ansi-green-fg">----&gt; 2</span> X_train, y_train <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">load_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">week3/C1W3A1/data/ex2data1.txt</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-red-fg">TypeError</span>: load_data() takes 0 positional arguments but 1 was given
</pre></div></div>
</div>
<p>View the variables</p>
<div class="line-block">
<div class="line">Let’s get more familiar with your dataset.</div>
<div class="line">- A good place to start is to just print out each variable and see what it contains.</div>
</div>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and the type of the variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[73]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in X_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in X_train are:
 [[0.5 1.5]
 [1.  1. ]
 [1.5 0.5]
 [3.  0.5]
 [2.  2. ]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Now print the first five values of <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[74]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in y_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in y_train are:
 [0 0 0 1 1]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[75]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of X_train is: (6, 2)
The shape of y_train is: (6,)
We have m = 6 training examples
</pre></div></div>
</div>
<p>Visualize your data</p>
<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible. - The code below displays the data on a 2D plot (as shown below), where the axes are the two exam scores, and the positive and negative examples are shown with different markers. - We use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to generate this plot.</p>
<p><img alt="573805b200194dccb77aea407d15b209" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure1.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[76]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Admitted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Not admitted&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[76], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Plot examples</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">plot_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">X_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y_train</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">:</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">pos_label</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">Admitted</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">neg_label</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">Not admitted</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(95,135,135)"># Set the y-axis label</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plt<span style="color: rgb(98,98,98)">.</span>ylabel(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Exam 2 score</span><span style="color: rgb(175,0,0)">&#39;</span>)

<span class="ansi-red-fg">TypeError</span>: plot_data() missing 1 required positional argument: &#39;ax&#39;
</pre></div></div>
</div>
<p>Your goal is to build a logistic regression model to fit this data. - With this model, you can then predict if a new student will be admitted based on their scores on the two exams.</p>
</section>
<section id="2.3-Sigmoid-function">
<h5>2.3 Sigmoid function<a class="headerlink" href="#2.3-Sigmoid-function" title="Permalink to this heading"></a></h5>
<p>Recall that for logistic regression, the model is represented as</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x) = g(\mathbf{w}\cdot \mathbf{x} + b)\]</div>
<p>where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Let’s implement the sigmoid function first, so it can be used by the rest of this assignment.</p>
<p>Exercise 1</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function to calculate</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Note that - <code class="docutils literal notranslate"><span class="pre">z</span></code> is not always a single number, but can also be an array of numbers. - If the input is an array of numbers, we’d like to apply the sigmoid function to each value in the input array.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Args:</span>
<span class="sd">        z (ndarray): A scalar, numpy array of any size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        g (ndarray): sigmoid(z), with the same shape as z</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    <span class="c1">### END SOLUTION ###</span>

    <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<p><code class="docutils literal notranslate"><span class="pre">numpy</span></code> has a function called <code class="docutils literal notranslate"><span class="pre">`np.exp()</span></code> &lt;<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">https://numpy.org/doc/stable/reference/generated/numpy.exp.html</a>&gt;`__, which offers a convinient way to calculate the exponential ( <span class="math notranslate nohighlight">\(e^{z}\)</span>) of all elements in the input array (<code class="docutils literal notranslate"><span class="pre">z</span></code>).</p>
<details><p>Click for more hints</p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">np.exp(-z)</span></code></p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(1/e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">1/np.exp(-z)</span></code></p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">g</span></code></p>
<details><p>Hint to calculate g g = 1 / (1 + np.exp(-z))</p>
</details></li>
</ul>
</li>
</ul>
</details><p>When you are finished, try testing a few values by calling <code class="docutils literal notranslate"><span class="pre">sigmoid(x)</span></code> in the cell below. - For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. - Evaluating <code class="docutils literal notranslate"><span class="pre">sigmoid(0)</span></code> should give you exactly 0.5.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[80]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid(0) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid(0) = 0.5
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid(0)</p>
</td><td><p>0.5</p>
</td></tr></table><ul class="simple">
<li><p>As mentioned before, your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[81]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid([ -1, 0, 1, 2]) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))))</span>

<span class="c1"># UNIT TESTS</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[81], line 5</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(95,135,135)"># UNIT TESTS</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">public_tests</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span style="color: rgb(98,98,98)">*</span>
<span class="ansi-green-fg">----&gt; 5</span> <span class="ansi-yellow-bg">sigmoid_test</span>(sigmoid)

<span class="ansi-red-fg">NameError</span>: name &#39;sigmoid_test&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid([-1, 0, 1, 2])</p>
</td><td><p>[0.26894142 0.5 0.73105858 0.88079708]</p>
</td></tr></table></section>
<section id="2.4-Cost-function-for-logistic-regression">
<h5>2.4 Cost function for logistic regression<a class="headerlink" href="#2.4-Cost-function-for-logistic-regression" title="Permalink to this heading"></a></h5>
<p>In this section, you will implement the cost function for logistic regression.</p>
<p>Exercise 2</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function using the equations below.</p>
<p>Recall that for logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]</div>
<p>where * m is the number of training examples in the dataset</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is -</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, which is the actual label</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x^{(i)}} + b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p>
<ul class="simple">
<li><p>It might be helpful to first calculate an intermediate variable <span class="math notranslate nohighlight">\(z_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is the number of features, before calculating <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span></p></li>
</ul>
</li>
</ul>
<p>Note: * As you are doing this, remember that the variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are not scalar values but matrices of shape (<span class="math notranslate nohighlight">\(m, n\)</span>) and (<span class="math notranslate nohighlight">\(𝑚\)</span>,1) respectively, where <span class="math notranslate nohighlight">\(𝑛\)</span> is the number of features and <span class="math notranslate nohighlight">\(𝑚\)</span> is the number of training examples. * You can use the sigmoid function that you implemented above for this part.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[82]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED FUNCTION: compute_cost</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : scalar Values of bias parameter of the model</span>
<span class="sd">      lambda_: unused placeholder</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">+=</span> <span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f_wb</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f_wb</span><span class="p">)</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows: <code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">0</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(m):</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">+</span> <span class="pre">2*i</span></code></p>
<ul class="simple">
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">X</span></code> using a for loop and add the <code class="docutils literal notranslate"><span class="pre">loss</span></code> from each iteration to a variable (<code class="docutils literal notranslate"><span class="pre">loss_sum</span></code>) initialized outside the loop.</p></li>
<li><p>Then, you can return the <code class="docutils literal notranslate"><span class="pre">total_cost</span></code> as <code class="docutils literal notranslate"><span class="pre">loss_sum</span></code> divided by <code class="docutils literal notranslate"><span class="pre">m</span></code>.</p></li>
</ul>
<details><p>Click for more hints</p>
<ul class="simple">
<li><p>Here’s how you can structure the overall implementation for this function</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Loop over each training example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>

        <span class="c1"># First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb_ij</span> <span class="o">=</span> <span class="c1"># Your code here to calculate w[j] * X[i][j]</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">z_wb_ij</span> <span class="c1"># equivalent to z_wb = z_wb + z_wb_ij</span>
        <span class="c1"># Add the bias term to z_wb</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span> <span class="c1"># equivalent to z_wb = z_wb + b</span>

        <span class="n">f_wb</span> <span class="o">=</span> <span class="c1"># Your code here to calculate prediction f_wb for a training example</span>
        <span class="n">loss</span> <span class="o">=</span>  <span class="c1"># Your code here to calculate loss for a training example</span>

        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># equivalent to loss_sum = loss_sum + loss</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_sum</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">z_wb_ij</span></code>, <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
<details><p>Hint to calculate z_wb_ij     z_wb_ij = w[j]*X[i][j]</p>
</details><details><p>Hint to calculate f_wb     <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span> where <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. You can simply call the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function implemented above.</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = sigmoid(z_wb)</p>
</details></details><details><p>Hint to calculate loss     You can use the np.log function to calculate the log</p>
<details><p>    More hints to calculate loss     You can compute loss as loss = -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</p>
</details></details></details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[83]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Compute and display cost with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at initial w (zeros): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at initial w (zeros): 0.693
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at initial w (zeros)</p>
</td><td><p>0.693</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[84]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">24.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at test w,b: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>


<span class="c1"># UNIT TESTS</span>
<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at test w,b: 11.633
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[84], line 10</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Cost at test w,b: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{:.3f}</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(98,98,98)">.</span>format(cost))
<span class="ansi-green-intense-fg ansi-bold">      9</span> <span style="color: rgb(95,135,135)"># UNIT TESTS</span>
<span class="ansi-green-fg">---&gt; 10</span> <span class="ansi-yellow-bg">compute_cost_test</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">compute_cost</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/C1W2A1/public_tests.py:10</span>, in <span class="ansi-cyan-fg">compute_cost_test</span><span class="ansi-blue-fg">(target)</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> initial_w <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">2</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">3.0</span>
<span class="ansi-green-fg">---&gt; 10</span> cost <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">initial_w</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">initial_b</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     11</span> <span class="ansi-bold" style="color: rgb(0,135,0)">assert</span> cost <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(98,98,98)">0</span>, <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Case 1: Cost must be 0 for a perfect prediction but got </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>cost<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span> <span style="color: rgb(95,135,135)"># Case 2</span>

Cell <span class="ansi-green-fg">In[82], line 16</span>, in <span class="ansi-cyan-fg">compute_cost</span><span class="ansi-blue-fg">(X, y, w, b, lambda_)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">compute_cost</span>(X, y, w, b, lambda_<span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1</span>):
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(188,188,188)">    </span><span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span style="color: rgb(175,0,0)">    Computes the cost over all examples</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(175,0,0)">    Args:</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span> <span style="color: rgb(175,0,0)">      total_cost: (scalar)         cost </span>
<span class="ansi-green-intense-fg ansi-bold">     14</span> <span style="color: rgb(175,0,0)">    &#34;&#34;&#34;</span>
<span class="ansi-green-fg">---&gt; 16</span>     m, n <span style="color: rgb(98,98,98)">=</span> X<span style="color: rgb(98,98,98)">.</span>shape
<span class="ansi-green-intense-fg ansi-bold">     18</span>     <span style="color: rgb(95,135,135)">### START CODE HERE ###</span>
<span class="ansi-green-intense-fg ansi-bold">     19</span>     cost <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0</span>

<span class="ansi-red-fg">ValueError</span>: not enough values to unpack (expected 2, got 1)
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at test w,b</p>
</td><td><p>0.218</p>
</td></tr></table></section>
<section id="2.5-Gradient-for-logistic-regression">
<h5>2.5 Gradient for logistic regression<a class="headerlink" href="#2.5-Gradient-for-logistic-regression" title="Permalink to this heading"></a></h5>
<p>In this section, you will implement the gradient for logistic regression.</p>
<p>Recall that the gradient descent algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; b := b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \newline       \; &amp; w_j := w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1}\newline &amp; \rbrace\end{align*}\]</div>
<p>where, parameters <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(w_j\)</span> are all updated simultaniously</p>
<p>Exercise 3</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function to compute <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> from equations (2) and (3) below.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>* m is the number of training examples in the dataset</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the actual label</p></li>
<li><p><strong>Note</strong>: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x)\)</span>.</p></li>
</ul>
<p>As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[85]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C3</span>
<span class="c1"># GRADED FUNCTION: compute_gradient</span>
<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for logistic regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) variable such as house size</span>
<span class="sd">      y : (array_like Shape (m,1)) actual value</span>
<span class="sd">      w : (array_like Shape (n,1)) values of parameters of the model</span>
<span class="sd">      b : (scalar)                 value of parameter of the model</span>
<span class="sd">      lambda_: unused placeholder.</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="n">f_wb_i</span>  <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err_i</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err_i</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="o">/</span><span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>


    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient(X, y, w, b, lambda_=None): m, n = X.shape dj_dw = np.zeros(w.shape) dj_db = 0.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
for i in range(m):
    # Calculate f_wb (exactly as you did in the compute_cost function above)
    f_wb =

    # Calculate the  gradient for b from this example
    dj_db_i = # Your code here to calculate the error

    # add that to dj_db
    dj_db += dj_db_i

    # get dj_dw for each attribute
    for j in range(n):
        # You code here to calculate the gradient from the i-th example for j-th attribute
        dj_dw_ij =
        dj_dw[j] += dj_dw_ij

# divide dj_db and dj_dw by total number of examples
dj_dw = dj_dw / m
dj_db = dj_db / m
### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code>, <code class="docutils literal notranslate"><span class="pre">dj_db_i</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_dw_ij</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate dj_db_i     You can calculate dj_db_i as dj_db_i = f_wb - y[i]</p>
</details><details><p>Hint to calculate dj_dw_ij     You can calculate dj_dw_ij as dj_dw_ij = (f_wb - y[i])* X[i][j]</p>
</details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[86]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display gradient with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_db at initial w (zeros):</span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_dw at initial w (zeros):</span><span class="si">{</span><span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at initial w (zeros):0.0
dj_dw at initial w (zeros):[-0.25, -0.16666666666666666]
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.1</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-12.00921658929115, -11.262842205513591]</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[87]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost and gradient with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">24</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>  <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_db at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_db</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_dw at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_test</span><span class="p">(</span><span class="n">compute_gradient</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at test_w: -0.49999999997085626
dj_dw at test_w: [-0.9999999999478811, -0.8333333333035631]
Using X with shape (4, 1)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[87], line 10</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">dj_dw at test_w:</span><span style="color: rgb(175,0,0)">&#39;</span>, dj_dw<span style="color: rgb(98,98,98)">.</span>tolist())
<span class="ansi-green-intense-fg ansi-bold">      9</span> <span style="color: rgb(95,135,135)"># UNIT TESTS    </span>
<span class="ansi-green-fg">---&gt; 10</span> <span class="ansi-yellow-bg">compute_gradient_test</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">compute_gradient</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification/week2/C1W2A1/public_tests.py:50</span>, in <span class="ansi-cyan-fg">compute_gradient_test</span><span class="ansi-blue-fg">(target)</span>
<span class="ansi-green-intense-fg ansi-bold">     48</span> initial_w <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">2.</span>
<span class="ansi-green-intense-fg ansi-bold">     49</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-fg">---&gt; 50</span> dj_dw, dj_db <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">initial_w</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">initial_b</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     51</span> <span style="color: rgb(95,135,135)">#assert dj_dw.shape == initial_w.shape, f&#34;Wrong shape for dj_dw. {dj_dw} != {initial_w.shape}&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">     52</span> <span class="ansi-bold" style="color: rgb(0,135,0)">assert</span> dj_db <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(98,98,98)">0.0</span>, <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Case 1: dj_db is wrong: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>dj_db<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)"> != 0.0</span><span style="color: rgb(175,0,0)">&#34;</span>

Cell <span class="ansi-green-fg">In[85], line 17</span>, in <span class="ansi-cyan-fg">compute_gradient</span><span class="ansi-blue-fg">(X, y, w, b, lambda_)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">compute_gradient</span>(X, y, w, b, lambda_<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">None</span>):
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(188,188,188)">    </span><span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span style="color: rgb(175,0,0)">    Computes the gradient for logistic regression </span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(175,0,0)"> </span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">     15</span> <span style="color: rgb(175,0,0)">      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. </span>
<span class="ansi-green-intense-fg ansi-bold">     16</span> <span style="color: rgb(175,0,0)">    &#34;&#34;&#34;</span>
<span class="ansi-green-fg">---&gt; 17</span>     m, n <span style="color: rgb(98,98,98)">=</span> X<span style="color: rgb(98,98,98)">.</span>shape
<span class="ansi-green-intense-fg ansi-bold">     18</span>     dj_dw <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>zeros(w<span style="color: rgb(98,98,98)">.</span>shape)
<span class="ansi-green-intense-fg ansi-bold">     19</span>     dj_db <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.</span>

<span class="ansi-red-fg">ValueError</span>: not enough values to unpack (expected 2, got 1)
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.5999999999991071</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-44.8313536178737957, -44.37384124953978]</p>
</td></tr></table></section>
<section id="2.6-Learning-parameters-using-gradient-descent">
<h5>2.6 Learning parameters using gradient descent<a class="headerlink" href="#2.6-Learning-parameters-using-gradient-descent" title="Permalink to this heading"></a></h5>
<p>Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. - You don’t need to implement anything for this part. Simply run the cells below.</p>
<ul class="simple">
<li><p>A good way to verify that gradient descent is working correctly is to look at the value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> and check that it is decreasing with each step.</p></li>
<li><p>Assuming you have implemented the gradient and computed the cost correctly, your value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> should never increase, and should converge to a steady value by the end of the algorithm.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[88]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      X :    (array_like Shape (m, n)</span>
<span class="sd">      y :    (array_like Shape (m,))</span>
<span class="sd">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span>
<span class="sd">      b_in : (scalar)                 Initial value of parameter of the model</span>
<span class="sd">      cost_function:                  function to compute cost</span>
<span class="sd">      alpha : (float)                 Learning rate</span>
<span class="sd">      num_iters : (int)               number of iterations to run gradient descent</span>
<span class="sd">      lambda_ (scalar, float)         regularization constant</span>

<span class="sd">    Returns:</span>
<span class="sd">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">      b : (scalar)                Updated value of parameter of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w_in</span> <span class="o">=</span> <span class="n">w_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">b_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">cost</span> <span class="o">=</span>  <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_iters</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span> <span class="n">w_history</span> <span class="c1">#return w and J,w history for graphing</span>
</pre></div>
</div>
</div>
<p>Now let’s run the gradient descent algorithm above to learn the parameters for our dataset.</p>
<p><strong>Note</strong></p>
<p>The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[89]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">intial_w</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>


<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span> <span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                   <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost     4.00
Iteration 1000: Cost     2.07
Iteration 2000: Cost     0.59
Iteration 3000: Cost     0.22
Iteration 4000: Cost     0.16
Iteration 5000: Cost     0.14
Iteration 6000: Cost     0.13
Iteration 7000: Cost     0.13
Iteration 8000: Cost     0.13
Iteration 9000: Cost     0.13
Iteration 9999: Cost     0.13
</pre></div></div>
</div>
<details><p>Expected Output: Cost 0.30, (Click to see details):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># With the following settings
np.random.seed(1)
intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)
initial_b = -8
iterations = 10000
alpha = 0.001
#
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div>
</div>
</section>
<section id="2.7-Plotting-the-decision-boundary">
<h5>2.7 Plotting the decision boundary<a class="headerlink" href="#2.7-Plotting-the-decision-boundary" title="Permalink to this heading"></a></h5>
<div class="line-block">
<div class="line">We will now use the final parameters from gradient descent to plot the linear fit. If you implemented the previous parts correctly, you should see the following plot:</div>
<div class="line"><img alt="741366caa0b9454b891b6c249745f44d" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure2.png" style="width: 450px; height: 450px;" /></div>
</div>
<p>We will use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to create this plot.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[90]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[90], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">plot_decision_boundary</span>(w, b, X_train, y_train)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_decision_boundary&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="2.8-Evaluating-logistic-regression">
<h5>2.8 Evaluating logistic regression<a class="headerlink" href="#2.8-Evaluating-logistic-regression" title="Permalink to this heading"></a></h5>
<p>We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set.</p>
<p>You will implement the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function below to do this.</p>
</section>
<section id="Exercise-4">
<h5>Exercise 4<a class="headerlink" href="#Exercise-4" title="Permalink to this heading"></a></h5>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function to produce <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code> predictions given a dataset and a learned parameter vector <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. - First you need to compute the prediction from the model <span class="math notranslate nohighlight">\(f(x^{(i)}) = g(w \cdot x^{(i)})\)</span> for every example - You’ve implemented this before in the parts above - We interpret the output of the model (<span class="math notranslate nohighlight">\(f(x^{(i)})\)</span>) as the probability that <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span> given <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and parameterized by <span class="math notranslate nohighlight">\(w\)</span>. - Therefore, to get a
final prediction (<span class="math notranslate nohighlight">\(y^{(i)}=0\)</span> or <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>) from the logistic regression model, you can use the following heuristic -</p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &gt;= 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span></p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[91]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C4</span>
<span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict whether the label is 0 or 1 using learned logistic</span>
<span class="sd">    regression parameters w</span>

<span class="sd">    Args:</span>
<span class="sd">    X : (ndarray Shape (m, n))</span>
<span class="sd">    w : (array_like Shape (n,))      Parameters of the model</span>
<span class="sd">    b : (scalar, float)              Parameter of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    p: (ndarray (m,1))</span>
<span class="sd">        The predictions for X using a threshold at 0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of training examples</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Loop over each example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="mi">0</span>

        <span class="c1"># Add bias term</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span>

        <span class="c1"># Calculate the prediction for this example</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_wb</span><span class="p">)</span>

        <span class="c1"># Apply the threshold</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">f_wb</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><div class="line-block">
<div class="line">Here’s how you can structure the overall implementation for this function ```python def predict(X, w, b): # number of training examples m, n = X.shape</div>
<div class="line">p = np.zeros(m)</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
# Loop over each example
for i in range(m):

    # Calculate f_wb (exactly how you did it in the compute_cost function above)
    # using a couple of lines of code
    f_wb =

    # Calculate the prediction for that training example
    p[i] = # Your code here to calculate the prediction based on f_wb

### END CODE HERE ###
return p
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">p[i]</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate p[i]     As an example, if you’d like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as x = y &lt; 3 . Now do the same for p[i] = 1 if f_wb &gt;= 0.5 and 0 otherwise.</p>
<details><p>    More hints to calculate p[i]     You can compute p[i] as p[i] = f_wb &gt;= 0.5</p>
</details></details></li>
</ul>
</details><p>Once you have completed the function <code class="docutils literal notranslate"><span class="pre">predict</span></code>, let’s run the code below to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test your predict code</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tmp_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tmp_b</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">tmp_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="n">tmp_p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">tmp_X</span><span class="p">,</span> <span class="n">tmp_w</span><span class="p">,</span> <span class="n">tmp_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output of predict: shape </span><span class="si">{</span><span class="n">tmp_p</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, value </span><span class="si">{</span><span class="n">tmp_p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Output of predict: shape (4,), value [0. 1. 1. 1.]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[92], line 11</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Output of predict: shape </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>tmp_p<span style="color: rgb(98,98,98)">.</span>shape<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">, value </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>tmp_p<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">&#39;</span>)
<span class="ansi-green-intense-fg ansi-bold">     10</span> <span style="color: rgb(95,135,135)"># UNIT TESTS        </span>
<span class="ansi-green-fg">---&gt; 11</span> <span class="ansi-yellow-bg">predict_test</span>(predict)

<span class="ansi-red-fg">NameError</span>: name &#39;predict_test&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<table><tr><td><p>Output of predict: shape (4,),value [0. 1. 1. 1.]</p>
</td></tr></table><p>Now let’s use this to compute the accuracy on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[93]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on our training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 100.000000
</pre></div></div>
</div>
<table><tr><td><p>Train Accuracy (approx):</p>
</td><td><p>92.00</p>
</td></tr></table></section>
</section>
<section id="3---Regularized-Logistic-Regression">
<h4>3 - Regularized Logistic Regression<a class="headerlink" href="#3---Regularized-Logistic-Regression" title="Permalink to this heading"></a></h4>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<section id="3.1-Problem-Statement">
<h5>3.1 Problem Statement<a class="headerlink" href="#3.1-Problem-Statement" title="Permalink to this heading"></a></h5>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. - From these two tests, you would like to determine whether the microchips should be accepted or rejected. - To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
</section>
<section id="3.2-Loading-and-visualizing-the-data">
<h5>3.2 Loading and visualizing the data<a class="headerlink" href="#3.2-Loading-and-visualizing-the-data" title="Permalink to this heading"></a></h5>
<p>Similar to previous parts of this exercise, let’s start by loading the dataset for this task and visualizing it.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains the test results for the microchips from two tests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span></code> contains the results of the QA</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the microchip was accepted</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the microchip was rejected</p></li>
</ul>
</li>
<li><p>Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p></li>
</ul>
</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[94]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;data/ex2data2.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[94], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># load dataset</span>
<span class="ansi-green-fg">----&gt; 2</span> X_train, y_train <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">load_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">data/ex2data2.txt</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-red-fg">TypeError</span>: load_data() takes 0 positional arguments but 1 was given
</pre></div></div>
</div>
<p>View the variables</p>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and the type of the variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print X_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># print y_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train: [[0.5 1.5]
 [1.  1. ]
 [1.5 0.5]
 [3.  0.5]
 [2.  2. ]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
y_train: [0 0 0 1 1]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[96]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of X_train is: (6, 2)
The shape of y_train is: (6,)
We have m = 6 training examples
</pre></div></div>
</div>
<p>Visualize your data</p>
<p>The helper function <code class="docutils literal notranslate"><span class="pre">plot_data</span></code> (from <code class="docutils literal notranslate"><span class="pre">utils.py</span></code>) is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>
<p><img alt="91878602d34a4140b651012d9a4660b6" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/images/figure3.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[97]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Accepted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Rejected&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 2&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[97], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Plot examples</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">plot_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">X_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">y_train</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">:</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">pos_label</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">Accepted</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">neg_label</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">Rejected</span><span class="ansi-yellow-bg" style="color: rgb(175,0,0)">&#34;</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(95,135,135)"># Set the y-axis label</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plt<span style="color: rgb(98,98,98)">.</span>ylabel(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Microchip Test 2</span><span style="color: rgb(175,0,0)">&#39;</span>)

<span class="ansi-red-fg">TypeError</span>: plot_data() missing 1 required positional argument: &#39;ax&#39;
</pre></div></div>
</div>
<p>Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
</section>
<section id="3.3-Feature-mapping">
<h5>3.3 Feature mapping<a class="headerlink" href="#3.3-Feature-mapping" title="Permalink to this heading"></a></h5>
<p>One way to fit the data better is to create more features from each data point. In the provided function <code class="docutils literal notranslate"><span class="pre">map_feature</span></code>, we will map the features into all polynomial terms of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> up to the sixth power.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{map\_feature}(x) =
\left[\begin{array}{c}
x_1\\
x_2\\
x_1^2\\
x_1 x_2\\
x_2^2\\
x_1^3\\
\vdots\\
x_1 x_2^5\\
x_2^6\end{array}\right]\end{split}\]</div>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.</p>
<ul class="simple">
<li><p>A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.</p></li>
<li><p>We have provided the <code class="docutils literal notranslate"><span class="pre">map_feature</span></code> function for you in utils.py.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[98]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape of data:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mapped_X</span> <span class="o">=</span>  <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape after feature mapping:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Original shape of data: (6, 2)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[98], line 3</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Original shape of data:</span><span style="color: rgb(175,0,0)">&#34;</span>, X_train<span style="color: rgb(98,98,98)">.</span>shape)
<span class="ansi-green-fg">----&gt; 3</span> mapped_X <span style="color: rgb(98,98,98)">=</span>  <span class="ansi-yellow-bg">map_feature</span>(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Shape after feature mapping:</span><span style="color: rgb(175,0,0)">&#34;</span>, mapped_X<span style="color: rgb(98,98,98)">.</span>shape)

<span class="ansi-red-fg">NameError</span>: name &#39;map_feature&#39; is not defined
</pre></div></div>
</div>
<p>Let’s also print the first elements of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">mapped_X</span></code> to see the tranformation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[99]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train[0]:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mapped X_train[0]:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train[0]: [0.5 1.5]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[99], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">X_train[0]:</span><span style="color: rgb(175,0,0)">&#34;</span>, X_train[<span style="color: rgb(98,98,98)">0</span>])
<span class="ansi-green-fg">----&gt; 2</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">mapped X_train[0]:</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">mapped_X</span>[<span style="color: rgb(98,98,98)">0</span>])

<span class="ansi-red-fg">NameError</span>: name &#39;mapped_X&#39; is not defined
</pre></div></div>
</div>
<p>While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
</section>
<section id="3.4-Cost-function-for-regularized-logistic-regression">
<h5>3.4 Cost function for regularized logistic regression<a class="headerlink" href="#3.4-Cost-function-for-regularized-logistic-regression" title="Permalink to this heading"></a></h5>
<p>In this part, you will implement the cost function for regularized logistic regression.</p>
<p>Recall that for regularized logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Compare this to the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}.b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]</div>
<p>The difference is the regularization term, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(b\)</span> parameter is not regularized.</p>
<p>Exercise 5</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function below to calculate the following term for each element in <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>The starter code then adds this to the cost without regularization (which you computed above in <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code>) to calculate the cost with regulatization.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[100]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C5</span>
<span class="k">def</span> <span class="nf">compute_cost_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (array_like Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : (array_like Shape (n,)) Values of bias parameter of the model</span>
<span class="sd">      lambda_ : (scalar, float)    Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calls the compute_cost function that you implemented above</span>
    <span class="n">cost_without_reg</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># You need to calculate this value</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Add the regularization cost to get the total cost</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost_without_reg</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_cost_reg(X, y, w, b, lambda_ = 1):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   m, n = X.shape

    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b)

    # You need to calculate this value
    reg_cost = 0.

    ### START CODE HERE ###
    for j in range(n):
        reg_cost_j = # Your code here to calculate the cost from w[j]
        reg_cost = reg_cost + reg_cost_j

    ### END CODE HERE ###

    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

return total_cost
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">reg_cost_j</span></code></p>
<details><p>Hint to calculate reg_cost_j     You can use calculate reg_cost_j as reg_cost_j = w[j]**2</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[101]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost :&quot;</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="c1"># UNIT TEST</span>
<span class="n">compute_cost_reg_test</span><span class="p">(</span><span class="n">compute_cost_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[101], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> X_mapped <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">map_feature</span>(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(X_mapped<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>]) <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(98,98,98)">0.5</span>

<span class="ansi-red-fg">NameError</span>: name &#39;map_feature&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost :</p>
</td><td><p>0.6618252552483948</p>
</td></tr></table></section>
<section id="3.5-Gradient-for-regularized-logistic-regression">
<h5>3.5 Gradient for regularized logistic regression<a class="headerlink" href="#3.5-Gradient-for-regularized-logistic-regression" title="Permalink to this heading"></a></h5>
<p>In this section, you will implement the gradient for regularized logistic regression.</p>
<p>The gradient of the regularized cost function has two components. The first, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is a scalar, the other is a vector with the same shape as the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, where the <span class="math notranslate nohighlight">\(j^\mathrm{th}\)</span> element is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \left( \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>As you can see,<span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is the same, the difference is the following term in <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Exercise 6</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function below to modify the code below to calculate the following term</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>The starter code will add this term to the <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span> returned from <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> above to get the gradient for the regularized cost function.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[102]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C6</span>
<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n))   variable such as house size</span>
<span class="sd">      y : (ndarray Shape (m,))    actual value</span>
<span class="sd">      w : (ndarray Shape (n,))    values of parameters of the model</span>
<span class="sd">      b : (scalar)                value of parameter of the model</span>
<span class="sd">      lambda_ : (scalar,float)    regularization constant</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient_reg(X, y, w, b, lambda_ = 1): m, n = X.shape</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db, dj_dw = compute_gradient(X, y, w, b)

### START CODE HERE ###
# Loop over the elements of w
for j in range(n):

    dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]

    # Add the regularization term  to the correspoding element of dj_dw
    dj_dw[j] = dj_dw[j] + dj_dw_j_reg

### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">dj_dw_j_reg</span></code></p>
<details><p>Hint to calculate dj_dw_j_reg     You can use calculate dj_dw_j_reg as dj_dw_j_reg = (lambda_ / m) * w[j]</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[103]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First few elements of regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_reg_test</span><span class="p">(</span><span class="n">compute_gradient_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[103], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> X_mapped <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">map_feature</span>(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_w  <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(X_mapped<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>]) <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(98,98,98)">0.5</span>

<span class="ansi-red-fg">NameError</span>: name &#39;map_feature&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db:0.07138288792343656</p>
</td></tr><tr><td><p>First few elements of regularized dj_dw:</p>
</td></tr><tr><td><p>[[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]]</p>
</td></tr></table></section>
<section id="3.6-Learning-parameters-using-gradient-descent">
<h5>3.6 Learning parameters using gradient descent<a class="headerlink" href="#3.6-Learning-parameters-using-gradient-descent" title="Permalink to this heading"></a></h5>
<p>Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>. - If you have completed the cost and gradient for regularized logistic regression correctly, you should be able to step through the next cell to learn the parameters <span class="math notranslate nohighlight">\(w\)</span>. - After training our parameters, we will use it to plot the decision boundary.</p>
<p><strong>Note</strong></p>
<p>The code block below takes quite a while to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you hae time, run for 100,000 iterations to see better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[104]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize fitting parameters</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                    <span class="n">compute_cost_reg</span><span class="p">,</span> <span class="n">compute_gradient_reg</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[104], line 3</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Initialize fitting parameters</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-fg">----&gt; 3</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(<span class="ansi-yellow-bg">X_mapped</span><span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>])<span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1.</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(95,135,135)"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;X_mapped&#39; is not defined
</pre></div></div>
</div>
<details><p>Expected Output: Cost &lt; 0.5 (Click for details)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Using the following settings
#np.random.seed(1)
#initial_w = np.random.rand(X_mapped.shape[1])-0.5
#initial_b = 1.
#lambda_ = 0.01;
#iterations = 10000
#alpha = 0.01
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div>
</div>
</section>
<section id="3.7-Plotting-the-decision-boundary">
<h5>3.7 Plotting the decision boundary<a class="headerlink" href="#3.7-Plotting-the-decision-boundary" title="Permalink to this heading"></a></h5>
<p>To help you visualize the model learned by this classifier, we will use our <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary</span></code> function which plots the (non-linear) decision boundary that separates the positive and negative examples.</p>
<ul class="simple">
<li><p>In the function, we plotted the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then drew a contour plot of where the predictions change from y = 0 to y = 1.</p></li>
<li><p>After learning the parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>, the next step is to plot a decision boundary similar to Figure 4.</p></li>
</ul>
<p><img alt="aff632f755774d62ba7bec9dcfe34bd9" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure4.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[105]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[105], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">plot_decision_boundary</span>(w, b, X_mapped, y_train)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_decision_boundary&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="3.8-Evaluating-regularized-logistic-regression-model">
<h5>3.8 Evaluating regularized logistic regression model<a class="headerlink" href="#3.8-Evaluating-regularized-logistic-regression-model" title="Permalink to this heading"></a></h5>
<p>You will use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[106]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on the training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[106], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)">#Compute accuracy on the training set</span>
<span class="ansi-green-fg">----&gt; 2</span> p <span style="color: rgb(98,98,98)">=</span> predict(<span class="ansi-yellow-bg">X_mapped</span>, w, b)
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Train Accuracy: </span><span class="ansi-bold" style="color: rgb(175,95,135)">%f</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(98,98,98)">%</span>(np<span style="color: rgb(98,98,98)">.</span>mean(p <span style="color: rgb(98,98,98)">==</span> y_train) <span style="color: rgb(98,98,98)">*</span> <span style="color: rgb(98,98,98)">100</span>))

<span class="ansi-red-fg">NameError</span>: name &#39;X_mapped&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Train Accuracy:~ 80%</p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../index.html" class="btn btn-neutral float-left" title="Welcome to Machine-Learning-Andrew-Ng’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>