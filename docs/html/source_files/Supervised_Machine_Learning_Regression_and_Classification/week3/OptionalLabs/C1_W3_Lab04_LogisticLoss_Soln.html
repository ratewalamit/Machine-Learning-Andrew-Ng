<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optional Lab: Logistic Regression, Logistic Loss &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link href="../../../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Optional Lab: Logistic Regression, Logistic Loss</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/OptionalLabs/C1_W3_Lab04_LogisticLoss_Soln.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Optional-Lab:-Logistic-Regression,-Logistic-Loss">
<h1>Optional Lab: Logistic Regression, Logistic Loss<a class="headerlink" href="#Optional-Lab:-Logistic-Regression,-Logistic-Loss" title="Permalink to this headline"></a></h1>
<p>In this ungraded lab, you will: - explore the reason the squared error loss is not appropriate for logistic regression - explore the logistic loss function</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">plt_logistic_loss</span> <span class="kn">import</span>  <span class="n">plt_logistic_cost</span><span class="p">,</span> <span class="n">plt_two_logistic_loss_curves</span><span class="p">,</span> <span class="n">plt_simple_example</span>
<span class="kn">from</span> <span class="nn">plt_logistic_loss</span> <span class="kn">import</span> <span class="n">soup_bowl</span><span class="p">,</span> <span class="n">plt_logistic_squared_error</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;./deeplearning.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Squared-error-for-logistic-regression?">
<h2>Squared error for logistic regression?<a class="headerlink" href="#Squared-error-for-logistic-regression?" title="Permalink to this headline"></a></h2>
<p><img alt="5892645e7c99401ea907edbebaa6a28d" src="../../../../_images/C1_W3_SqErrorVsLogistic.png" /> Recall for <strong>Linear</strong> Regression we have used the <strong>squared error cost function</strong>: The equation for the squared error cost with one variable is:</p>
<div class="math notranslate nohighlight">
\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}\]</div>
<p>Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">soup_bowl</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ceb32c967c0a432a8ebee570b1de6fa8", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, <span class="math notranslate nohighlight">\(f_{wb}(x)\)</span> now has a non-linear component, the sigmoid function: <span class="math notranslate nohighlight">\(f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )\)</span>. Let’s try a squared error cost on the example from an earlier lab, now including the sigmoid.</p>
<p>Here is our training data:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>
<span class="n">plt_simple_example</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4578660aa6a943dca5305be4cc0beae2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Now, let’s get a surface plot of the cost using a <em>squared error cost</em>:</p>
<div class="math notranslate nohighlight">
\[J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )\]</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">plt_logistic_squared_error</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9b53d2621a7e4781894109f7e6b65468", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>While this produces a pretty interesting plot, the surface above not nearly as smooth as the ‘soup bowl’ from linear regression!</p>
<p>Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below.</p>
</section>
<section id="Logistic-Loss-Function">
<h2>Logistic Loss Function<a class="headerlink" href="#Logistic-Loss-Function" title="Permalink to this headline"></a></h2>
<p><img alt="bff6f2072a7b4bc48f47f6f07b4de662" src="../../../../_images/C1_W3_LogisticLoss_a.png" /> <img alt="ee3d412918484a33afdcee30b097b3ba" src="../../../../_images/C1_W3_LogisticLoss_b.png" /> <img alt="9bdc94f115e64c0c8fcb10d9b4d8c883" src="../../../../_images/C1_W3_LogisticLoss_c.png" /></p>
<p>Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number.</p>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Definition Note:</strong> In this course, these definitions are used:</div>
<div class="line"><strong>Loss</strong> is a measure of the difference of a single example to its target value while the</div>
<div class="line"><strong>Cost</strong> is a measure of the losses over the training set</div>
</div>
</div></blockquote>
<p>This is defined: * <span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is:</p>
<dl>
<dt>:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{equation}</dt><dd><dl class="simple">
<dt>loss(f_{mathbf{w},b}(mathbf{x}^{(i)}), y^{(i)}) = begin{cases}</dt><dd><ul class="simple">
<li><p>logleft(f_{mathbf{w},b}left( mathbf{x}^{(i)} right) right) &amp; text{if $y^{(i)}=1$}\</p></li>
<li><p>log left( 1 - f_{mathbf{w},b}left( mathbf{x}^{(i)} right) right) &amp; text{if $y^{(i)}=0$}</p></li>
</ul>
</dd>
</dl>
<p>end{cases}</p>
</dd>
</dl>
<p>end{equation}`</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the target value.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p></li>
</ul>
<p>The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or (<span class="math notranslate nohighlight">\(y=0\)</span>) and another for when the target is one (<span class="math notranslate nohighlight">\(y=1\)</span>). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_two_logistic_loss_curves</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "89fa8263d4574b58bb48a78098e2c94a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}\)</span> which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1.</p>
<p>The loss function above can be rewritten to be easier to implement.</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\]</div>
<div class="line-block">
<div class="line">This is a rather formidable-looking equation. It is less daunting when you consider <span class="math notranslate nohighlight">\(y^{(i)}\)</span> can have only two values, 0 and 1. One can then consider the equation in two pieces:</div>
<div class="line">when $ y^{(i)} = 0$, the left-hand term is eliminated:</div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 0) &amp;= (-(0) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 0\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \\
&amp;= -\log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}\end{split}\]</div>
<p>and when $ y^{(i)} = 1$, the right-hand term is eliminated:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 1) &amp;=  (-(1) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 1\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\\
  &amp;=  -\log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}\end{split}\]</div>
</div></blockquote>
<p>OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let’s take a look at the cost vs parameters curve for the simple example we considered above:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">cst</span> <span class="o">=</span> <span class="n">plt_logistic_cost</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "69578fe1a0534bfc81d02c4ee11c4e8d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline. Reminder: you can rotate the above plots using your mouse.</p>
</section>
<section id="Congratulation!">
<h2>Congratulation!<a class="headerlink" href="#Congratulation!" title="Permalink to this headline"></a></h2>
<p>You have: - determined a squared error loss function is not suitable for classification tasks - developed and examined the logistic loss function which <strong>is</strong> suitable for classification tasks.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>