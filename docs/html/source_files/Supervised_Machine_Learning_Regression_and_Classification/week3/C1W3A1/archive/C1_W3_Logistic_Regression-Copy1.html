<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Logistic Regression &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link href="../../../../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Logistic Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/archive/C1_W3_Logistic_Regression-Copy1.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Logistic-Regression">
<h1>Logistic Regression<a class="headerlink" href="#Logistic-Regression" title="Permalink to this heading"></a></h1>
<p>In this exercise, you will implement logistic regression and apply it to two different datasets.</p>
</section>
<section id="Outline">
<h1>Outline<a class="headerlink" href="#Outline" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p>1 - Packages</p></li>
<li><p>2 - Logistic Regression</p>
<ul>
<li><p>2.1 Problem Statement</p></li>
<li><p>2.2 Loading and visualizing the data</p></li>
<li><p>2.3 Sigmoid function</p></li>
<li><p>2.4 Cost function for logistic regression</p></li>
<li><p>2.5 Gradient for logistic regression</p></li>
<li><p>2.6 Learning parameters using gradient descent</p></li>
<li><p>2.7 Plotting the decision boundary</p></li>
<li><p>2.8 Evaluating logistic regression</p></li>
</ul>
</li>
<li><p>3 - Regularized Logistic Regression</p>
<ul>
<li><p>3.1 Problem Statement</p></li>
<li><p>3.2 Loading and visualizing the data</p></li>
<li><p>3.3 Feature mapping</p></li>
<li><p>3.4 Cost function for regularized logistic regression</p></li>
<li><p>3.5 Gradient for regularized logistic regression</p></li>
<li><p>3.6 Learning parameters using gradient descent</p></li>
<li><p>3.7 Plotting the decision boundary</p></li>
<li><p>3.8 Evaluating regularized logistic regression model</p></li>
</ul>
</li>
</ul>
<p>## 1 - Packages</p>
<p>First, let’s run the cell below to import all the packages that you will need during this assignment. - <a class="reference external" href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python. - <a class="reference external" href="http://matplotlib.org">matplotlib</a> is a famous library to plot graphs in Python. - <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> contains helper functions for this assignment. You do not need to modify code in this file.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[1], line 3</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span class="ansi-bold" style="color: rgb(0,0,255)">numpy</span> <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> <span class="ansi-bold" style="color: rgb(0,0,255)">np</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span class="ansi-bold" style="color: rgb(0,0,255)">matplotlib</span><span class="ansi-bold" style="color: rgb(0,0,255)">.</span><span class="ansi-bold" style="color: rgb(0,0,255)">pyplot</span> <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> <span class="ansi-bold" style="color: rgb(0,0,255)">plt</span>
<span class="ansi-green-fg">----&gt; 3</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">utils</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span style="color: rgb(98,98,98)">*</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span class="ansi-bold" style="color: rgb(0,0,255)">copy</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span class="ansi-bold" style="color: rgb(0,0,255)">math</span>

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;utils&#39;
</pre></div></div>
</div>
<p>## 2 - Logistic Regression</p>
<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<p>### 2.1 Problem Statement</p>
<p>Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. * You have historical data from previous applicants that you can use as a training set for logistic regression. * For each training example, you have the applicant’s scores on two exams and the admissions decision. * Your task is to build a classification model that estimates an applicant’s probability of admission based on the
scores from those two exams.</p>
<p>### 2.2 Loading and visualizing the data</p>
<p>You will start by loading the dataset for this task. - The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> - <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains exam scores on two exams for a student - <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is the admission decision - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the student was admitted - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the student was not admitted - Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;data/ex2data1.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># load dataset</span>
<span class="ansi-green-fg">----&gt; 2</span> X_train, y_train <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">load_data</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">data/ex2data1.txt</span><span style="color: rgb(175,0,0)">&#34;</span>)

<span class="ansi-red-fg">NameError</span>: name &#39;load_data&#39; is not defined
</pre></div></div>
</div>
<section id="View-the-variables">
<h2>View the variables<a class="headerlink" href="#View-the-variables" title="Permalink to this heading"></a></h2>
<div class="line-block">
<div class="line">Let’s get more familiar with your dataset.</div>
<div class="line">- A good place to start is to just print out each variable and see what it contains.</div>
</div>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and the type of the variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in X_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[3], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">First five elements in X_train are:</span><span class="ansi-bold" style="color: rgb(175,95,0)">\n</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">X_train</span>[:<span style="color: rgb(98,98,98)">5</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Type of X_train:</span><span style="color: rgb(175,0,0)">&#34;</span>,<span style="color: rgb(0,135,0)">type</span>(X_train))

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p>Now print the first five values of <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in y_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[4], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">First five elements in y_train are:</span><span class="ansi-bold" style="color: rgb(175,95,0)">\n</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">y_train</span>[:<span style="color: rgb(98,98,98)">5</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Type of y_train:</span><span style="color: rgb(175,0,0)">&#34;</span>,<span style="color: rgb(0,135,0)">type</span>(y_train))

<span class="ansi-red-fg">NameError</span>: name &#39;y_train&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="Check-the-dimensions-of-your-variables">
<h2>Check the dimensions of your variables<a class="headerlink" href="#Check-the-dimensions-of-your-variables" title="Permalink to this heading"></a></h2>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">The shape of X_train is: </span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(<span class="ansi-yellow-bg">X_train</span><span style="color: rgb(98,98,98)">.</span>shape))
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">The shape of y_train is: </span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(y_train<span style="color: rgb(98,98,98)">.</span>shape))
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">We have m = </span><span class="ansi-bold" style="color: rgb(175,95,135)">%d</span><span style="color: rgb(175,0,0)"> training examples</span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">%</span> (<span style="color: rgb(0,135,0)">len</span>(y_train)))

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="Visualize-your-data">
<h2>Visualize your data<a class="headerlink" href="#Visualize-your-data" title="Permalink to this heading"></a></h2>
<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible. - The code below displays the data on a 2D plot (as shown below), where the axes are the two exam scores, and the positive and negative examples are shown with different markers. - We use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to generate this plot.</p>
<p><img alt="a660582bdf3b4cc1bc3ff4314932bd54" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/archive/images/figure1.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Admitted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Not admitted&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[6], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Plot examples</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">plot_data</span>(X_train, y_train[:], pos_label<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Admitted</span><span style="color: rgb(175,0,0)">&#34;</span>, neg_label<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Not admitted</span><span style="color: rgb(175,0,0)">&#34;</span>)
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(95,135,135)"># Set the y-axis label</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plt<span style="color: rgb(98,98,98)">.</span>ylabel(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Exam 2 score</span><span style="color: rgb(175,0,0)">&#39;</span>)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_data&#39; is not defined
</pre></div></div>
</div>
<p>Your goal is to build a logistic regression model to fit this data. - With this model, you can then predict if a new student will be admitted based on their scores on the two exams.</p>
<p>### 2.3 Sigmoid function</p>
<p>Recall that for logistic regression, the model is represented as</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x) = g(\mathbf{w}\cdot \mathbf{x} + b)\]</div>
<p>where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Let’s implement the sigmoid function first, so it can be used by the rest of this assignment.</p>
<p>### Exercise 1 Please complete the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function to calculate</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Note that - <code class="docutils literal notranslate"><span class="pre">z</span></code> is not always a single number, but can also be an array of numbers. - If the input is an array of numbers, we’d like to apply the sigmoid function to each value in the input array.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Args:</span>
<span class="sd">        z (ndarray): A scalar, numpy array of any size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        g (ndarray): sigmoid(z), with the same shape as z</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">### START CODE HERE ###</span>

    <span class="c1">### END SOLUTION ###</span>

    <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<p><code class="docutils literal notranslate"><span class="pre">numpy</span></code> has a function called <code class="docutils literal notranslate"><span class="pre">`np.exp()</span></code> &lt;<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">https://numpy.org/doc/stable/reference/generated/numpy.exp.html</a>&gt;`__, which offers a convinient way to calculate the exponential ( <span class="math notranslate nohighlight">\(e^{z}\)</span>) of all elements in the input array (<code class="docutils literal notranslate"><span class="pre">z</span></code>).</p>
<details><p>Click for more hints</p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">np.exp(-z)</span></code></p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(1/e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">1/np.exp(-z)</span></code></p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">g</span></code></p>
<details><p>Hint to calculate g g = 1 / (1 + np.exp(-z))</p>
</details></li>
</ul>
</li>
</ul>
</details><p>When you are finished, try testing a few values by calling <code class="docutils literal notranslate"><span class="pre">sigmoid(x)</span></code> in the cell below. - For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. - Evaluating <code class="docutils literal notranslate"><span class="pre">sigmoid(0)</span></code> should give you exactly 0.5.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid(0) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[8], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">sigmoid(0) = </span><span style="color: rgb(175,0,0)">&#34;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(<span class="ansi-yellow-bg">sigmoid</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">0</span><span class="ansi-yellow-bg">)</span>))

Cell <span class="ansi-green-fg">In[7], line 20</span>, in <span class="ansi-cyan-fg">sigmoid</span><span class="ansi-blue-fg">(z)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(175,0,0)">Compute the sigmoid of z</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span> <span style="color: rgb(175,0,0)">     </span>
<span class="ansi-green-intense-fg ansi-bold">     14</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">     16</span> <span style="color: rgb(95,135,135)">### START CODE HERE ### </span>
<span class="ansi-green-intense-fg ansi-bold">     17</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> <span style="color: rgb(95,135,135)">### END SOLUTION ###  </span>
<span class="ansi-green-fg">---&gt; 20</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">g</span>

<span class="ansi-red-fg">NameError</span>: name &#39;g&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid(0)</p>
</td><td><p>0.5</p>
</td></tr></table><ul class="simple">
<li><p>As mentioned before, your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid([ -1, 0, 1, 2]) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))))</span>

<span class="c1"># UNIT TESTS</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">sigmoid([ -1, 0, 1, 2]) = </span><span style="color: rgb(175,0,0)">&#34;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(<span class="ansi-yellow-bg">sigmoid</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">np</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">.</span><span class="ansi-yellow-bg">array</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">-</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">0</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">2</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>))
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(95,135,135)"># UNIT TESTS</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">public_tests</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> <span style="color: rgb(98,98,98)">*</span>

Cell <span class="ansi-green-fg">In[7], line 20</span>, in <span class="ansi-cyan-fg">sigmoid</span><span class="ansi-blue-fg">(z)</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(175,0,0)">Compute the sigmoid of z</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">     13</span> <span style="color: rgb(175,0,0)">     </span>
<span class="ansi-green-intense-fg ansi-bold">     14</span> <span style="color: rgb(175,0,0)">&#34;&#34;&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">     16</span> <span style="color: rgb(95,135,135)">### START CODE HERE ### </span>
<span class="ansi-green-intense-fg ansi-bold">     17</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> <span style="color: rgb(95,135,135)">### END SOLUTION ###  </span>
<span class="ansi-green-fg">---&gt; 20</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">g</span>

<span class="ansi-red-fg">NameError</span>: name &#39;g&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid([-1, 0, 1, 2])</p>
</td><td><p>[0.26894142 0.5 0.73105858 0.88079708]</p>
</td></tr></table><p>### 2.4 Cost function for logistic regression</p>
<p>In this section, you will implement the cost function for logistic regression.</p>
<p>### Exercise 2</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function using the equations below.</p>
<p>Recall that for logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]</div>
<p>where * m is the number of training examples in the dataset</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is -</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, which is the actual label</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x^{(i)}} + b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p>
<ul class="simple">
<li><p>It might be helpful to first calculate an intermediate variable <span class="math notranslate nohighlight">\(z_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is the number of features, before calculating <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span></p></li>
</ul>
</li>
</ul>
<p>Note: * As you are doing this, remember that the variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are not scalar values but matrices of shape (<span class="math notranslate nohighlight">\(m, n\)</span>) and (<span class="math notranslate nohighlight">\(𝑚\)</span>,1) respectively, where <span class="math notranslate nohighlight">\(𝑛\)</span> is the number of features and <span class="math notranslate nohighlight">\(𝑚\)</span> is the number of training examples. * You can use the sigmoid function that you implemented above for this part.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED FUNCTION: compute_cost</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : scalar Values of bias parameter of the model</span>
<span class="sd">      lambda_: unused placeholder</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows: <code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">0</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(m):</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">+</span> <span class="pre">2*i</span></code></p>
<ul class="simple">
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">X</span></code> using a for loop and add the <code class="docutils literal notranslate"><span class="pre">loss</span></code> from each iteration to a variable (<code class="docutils literal notranslate"><span class="pre">loss_sum</span></code>) initialized outside the loop.</p></li>
<li><p>Then, you can return the <code class="docutils literal notranslate"><span class="pre">total_cost</span></code> as <code class="docutils literal notranslate"><span class="pre">loss_sum</span></code> divided by <code class="docutils literal notranslate"><span class="pre">m</span></code>.</p></li>
</ul>
<details><p>Click for more hints</p>
<ul class="simple">
<li><p>Here’s how you can structure the overall implementation for this function</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Loop over each training example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>

        <span class="c1"># First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb_ij</span> <span class="o">=</span> <span class="c1"># Your code here to calculate w[j] * X[i][j]</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">z_wb_ij</span> <span class="c1"># equivalent to z_wb = z_wb + z_wb_ij</span>
        <span class="c1"># Add the bias term to z_wb</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span> <span class="c1"># equivalent to z_wb = z_wb + b</span>

        <span class="n">f_wb</span> <span class="o">=</span> <span class="c1"># Your code here to calculate prediction f_wb for a training example</span>
        <span class="n">loss</span> <span class="o">=</span>  <span class="c1"># Your code here to calculate loss for a training example</span>

        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># equivalent to loss_sum = loss_sum + loss</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_sum</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">z_wb_ij</span></code>, <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
<details><p>Hint to calculate z_wb_ij     z_wb_ij = w[j]*X[i][j]</p>
</details><details><p>Hint to calculate f_wb     <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span> where <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. You can simply call the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function implemented above.</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = sigmoid(z_wb)</p>
</details></details><details><p>Hint to calculate loss     You can use the np.log function to calculate the log</p>
<details><p>    More hints to calculate loss     You can compute loss as loss = -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</p>
</details></details></details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Compute and display cost with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at initial w (zeros): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> m, n <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">X_train</span><span style="color: rgb(98,98,98)">.</span>shape
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(95,135,135)"># Compute and display cost with w initialized to zeroes</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>zeros(n)

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at initial w (zeros)</p>
</td><td><p>0.693</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">24.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at test w,b: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>


<span class="c1"># UNIT TESTS</span>
<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[12], line 4</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> test_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>array([<span style="color: rgb(98,98,98)">0.2</span>, <span style="color: rgb(98,98,98)">0.2</span>])
<span class="ansi-green-intense-fg ansi-bold">      3</span> test_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">24.</span>
<span class="ansi-green-fg">----&gt; 4</span> cost <span style="color: rgb(98,98,98)">=</span> compute_cost(<span class="ansi-yellow-bg">X_train</span>, y_train, test_w, test_b)
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Cost at test w,b: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{:.3f}</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(98,98,98)">.</span>format(cost))
<span class="ansi-green-intense-fg ansi-bold">      9</span> <span style="color: rgb(95,135,135)"># UNIT TESTS</span>

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at test w,b</p>
</td><td><p>0.218</p>
</td></tr></table><p>### 2.5 Gradient for logistic regression</p>
<p>In this section, you will implement the gradient for logistic regression.</p>
<p>Recall that the gradient descent algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; b := b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \newline       \; &amp; w_j := w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1}\newline &amp; \rbrace\end{align*}\]</div>
<p>where, parameters <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(w_j\)</span> are all updated simultaniously</p>
<p>### Exercise 3</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function to compute <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> from equations (2) and (3) below.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>* m is the number of training examples in the dataset</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the actual label</p></li>
<li><p><strong>Note</strong>: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x)\)</span>.</p></li>
</ul>
<p>As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C3</span>
<span class="c1"># GRADED FUNCTION: compute_gradient</span>
<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for logistic regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) variable such as house size</span>
<span class="sd">      y : (array_like Shape (m,1)) actual value</span>
<span class="sd">      w : (array_like Shape (n,1)) values of parameters of the model</span>
<span class="sd">      b : (scalar)                 value of parameter of the model</span>
<span class="sd">      lambda_: unused placeholder.</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">err</span>  <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="kc">None</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">dj_db_i</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">dj_db</span> <span class="o">+=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">dj_dw</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1">### END CODE HERE ###</span>


    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient(X, y, w, b, lambda_=None): m, n = X.shape dj_dw = np.zeros(w.shape) dj_db = 0.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
err  = 0.
for i in range(m):
    # Calculate f_wb (exactly as you did in the compute_cost function above)
    f_wb =

    # Calculate the  gradient for b from this example
    dj_db_i = # Your code here to calculate the error

    # add that to dj_db
    dj_db += dj_db_i

    # get dj_dw for each attribute
    for j in range(n):
        # You code here to calculate the gradient from the i-th example for j-th attribute
        dj_dw_ij =
        dj_dw[j] += dj_dw_ij

# divide dj_db and dj_dw by total number of examples
dj_dw = dj_dw / m
dj_db = dj_db / m
### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code>, <code class="docutils literal notranslate"><span class="pre">dj_db_i</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_dw_ij</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate dj_db_i     You can calculate dj_db_i as dj_db_i = f_wb - y[i]</p>
</details><details><p>Hint to calculate dj_dw_ij     You can calculate dj_dw_ij as dj_dw_ij = (f_wb - y[i])* X[i][j]</p>
</details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display gradient with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_db at initial w (zeros):</span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_dw at initial w (zeros):</span><span class="si">{</span><span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[14], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Compute and display gradient with w initialized to zeroes</span>
<span class="ansi-green-fg">----&gt; 2</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>zeros(<span class="ansi-yellow-bg">n</span>)
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> dj_db, dj_dw <span style="color: rgb(98,98,98)">=</span> compute_gradient(X_train, y_train, initial_w, initial_b)

<span class="ansi-red-fg">NameError</span>: name &#39;n&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.1</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-12.00921658929115, -11.262842205513591]</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost and gradient with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">24</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>  <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_db at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_db</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_dw at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_test</span><span class="p">(</span><span class="n">compute_gradient</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[15], line 4</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> test_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>array([ <span style="color: rgb(98,98,98)">0.2</span>, <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">0.5</span>])
<span class="ansi-green-intense-fg ansi-bold">      3</span> test_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">24</span>
<span class="ansi-green-fg">----&gt; 4</span> dj_db, dj_dw  <span style="color: rgb(98,98,98)">=</span> compute_gradient(<span class="ansi-yellow-bg">X_train</span>, y_train, test_w, test_b)
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">dj_db at test_w:</span><span style="color: rgb(175,0,0)">&#39;</span>, dj_db)
<span class="ansi-green-intense-fg ansi-bold">      7</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">dj_dw at test_w:</span><span style="color: rgb(175,0,0)">&#39;</span>, dj_dw<span style="color: rgb(98,98,98)">.</span>tolist())

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.5999999999991071</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-44.8313536178737957, -44.37384124953978]</p>
</td></tr></table><p>### 2.6 Learning parameters using gradient descent</p>
<p>Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. - You don’t need to implement anything for this part. Simply run the cells below.</p>
<ul class="simple">
<li><p>A good way to verify that gradient descent is working correctly is to look at the value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> and check that it is decreasing with each step.</p></li>
<li><p>Assuming you have implemented the gradient and computed the cost correctly, your value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> should never increase, and should converge to a steady value by the end of the algorithm.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      X :    (array_like Shape (m, n)</span>
<span class="sd">      y :    (array_like Shape (m,))</span>
<span class="sd">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span>
<span class="sd">      b_in : (scalar)                 Initial value of parameter of the model</span>
<span class="sd">      cost_function:                  function to compute cost</span>
<span class="sd">      alpha : (float)                 Learning rate</span>
<span class="sd">      num_iters : (int)               number of iterations to run gradient descent</span>
<span class="sd">      lambda_ (scalar, float)         regularization constant</span>

<span class="sd">    Returns:</span>
<span class="sd">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">      b : (scalar)                Updated value of parameter of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w_in</span> <span class="o">=</span> <span class="n">w_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">b_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">cost</span> <span class="o">=</span>  <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_iters</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span> <span class="n">w_history</span> <span class="c1">#return w and J,w history for graphing</span>
</pre></div>
</div>
</div>
<p>Now let’s run the gradient descent algorithm above to learn the parameters for our dataset.</p>
<p><strong>Note</strong></p>
<p>The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">intial_w</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>


<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span> <span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                   <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[17], line 10</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span> iterations <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">10000</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> alpha <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.001</span>
<span class="ansi-green-fg">---&gt; 10</span> w,b, J_history,_ <span style="color: rgb(98,98,98)">=</span> gradient_descent(<span class="ansi-yellow-bg">X_train</span> ,y_train, initial_w, initial_b,
<span class="ansi-green-intense-fg ansi-bold">     11</span>                                    compute_cost, compute_gradient, alpha, iterations, <span style="color: rgb(98,98,98)">0</span>)

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<details><p>Expected Output: Cost 0.30, (Click to see details):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># With the following settings
np.random.seed(1)
intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)
initial_b = -8
iterations = 10000
alpha = 0.001
#
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div>
</div>
<p>### 2.7 Plotting the decision boundary</p>
<div class="line-block">
<div class="line">We will now use the final parameters from gradient descent to plot the linear fit. If you implemented the previous parts correctly, you should see the following plot:</div>
<div class="line"><img alt="f93efb54a99149259c90e55010862650" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/archive/images/figure2.png" style="width: 450px; height: 450px;" /></div>
</div>
<p>We will use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to create this plot.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[18], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">plot_decision_boundary</span>(w, b, X_train, y_train)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_decision_boundary&#39; is not defined
</pre></div></div>
</div>
<p>### 2.8 Evaluating logistic regression</p>
<p>We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set.</p>
<p>You will implement the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function below to do this.</p>
<p>### Exercise 4</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function to produce <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code> predictions given a dataset and a learned parameter vector <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. - First you need to compute the prediction from the model <span class="math notranslate nohighlight">\(f(x^{(i)}) = g(w \cdot x^{(i)})\)</span> for every example - You’ve implemented this before in the parts above - We interpret the output of the model (<span class="math notranslate nohighlight">\(f(x^{(i)})\)</span>) as the probability that <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span> given <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and parameterized by <span class="math notranslate nohighlight">\(w\)</span>. - Therefore, to get a
final prediction (<span class="math notranslate nohighlight">\(y^{(i)}=0\)</span> or <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>) from the logistic regression model, you can use the following heuristic -</p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &gt;= 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span></p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C4</span>
<span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict whether the label is 0 or 1 using learned logistic</span>
<span class="sd">    regression parameters w</span>

<span class="sd">    Args:</span>
<span class="sd">    X : (ndarray Shape (m, n))</span>
<span class="sd">    w : (array_like Shape (n,))      Parameters of the model</span>
<span class="sd">    b : (scalar, float)              Parameter of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    p: (ndarray (m,1))</span>
<span class="sd">        The predictions for X using a threshold at 0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of training examples</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Loop over each example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="kc">None</span>

        <span class="c1"># Add bias term</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="kc">None</span>

        <span class="c1"># Calculate the prediction for this example</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Apply the threshold</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><div class="line-block">
<div class="line">Here’s how you can structure the overall implementation for this function ```python def predict(X, w, b): # number of training examples m, n = X.shape</div>
<div class="line">p = np.zeros(m)</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
# Loop over each example
for i in range(m):

    # Calculate f_wb (exactly how you did it in the compute_cost function above)
    # using a couple of lines of code
    f_wb =

    # Calculate the prediction for that training example
    p[i] = # Your code here to calculate the prediction based on f_wb

### END CODE HERE ###
return p
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">p[i]</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate p[i]     As an example, if you’d like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as x = y &lt; 3 . Now do the same for p[i] = 1 if f_wb &gt;= 0.5 and 0 otherwise.</p>
<details><p>    More hints to calculate p[i]     You can compute p[i] as p[i] = f_wb &gt;= 0.5</p>
</details></details></li>
</ul>
</details><p>Once you have completed the function <code class="docutils literal notranslate"><span class="pre">predict</span></code>, let’s run the code below to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test your predict code</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tmp_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tmp_b</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">tmp_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="n">tmp_p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">tmp_X</span><span class="p">,</span> <span class="n">tmp_w</span><span class="p">,</span> <span class="n">tmp_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output of predict: shape </span><span class="si">{</span><span class="n">tmp_p</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, value </span><span class="si">{</span><span class="n">tmp_p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[20], line 7</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> tmp_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.3</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> tmp_X <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>randn(<span style="color: rgb(98,98,98)">4</span>, <span style="color: rgb(98,98,98)">2</span>) <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-fg">----&gt; 7</span> tmp_p <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">predict</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">tmp_X</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">tmp_w</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">tmp_b</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Output of predict: shape </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>tmp_p<span style="color: rgb(98,98,98)">.</span>shape<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">, value </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>tmp_p<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">&#39;</span>)
<span class="ansi-green-intense-fg ansi-bold">     10</span> <span style="color: rgb(95,135,135)"># UNIT TESTS        </span>

Cell <span class="ansi-green-fg">In[19], line 29</span>, in <span class="ansi-cyan-fg">predict</span><span class="ansi-blue-fg">(X, w, b)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span> <span style="color: rgb(95,135,135)"># Loop over each feature</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> j <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> <span style="color: rgb(0,135,0)">range</span>(n):
<span class="ansi-green-intense-fg ansi-bold">     28</span>     <span style="color: rgb(95,135,135)"># Add the corresponding term to z_wb</span>
<span class="ansi-green-fg">---&gt; 29</span>     z_wb <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span> <span style="color: rgb(95,135,135)"># Add bias term </span>
<span class="ansi-green-intense-fg ansi-bold">     32</span> z_wb <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>

<span class="ansi-red-fg">TypeError</span>: unsupported operand type(s) for +=: &#39;NoneType&#39; and &#39;NoneType&#39;
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<table><tr><td><p>Output of predict: shape (4,),value [0. 1. 1. 1.]</p>
</td></tr></table><p>Now let’s use this to compute the accuracy on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on our training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)">#Compute accuracy on our training set</span>
<span class="ansi-green-fg">----&gt; 2</span> p <span style="color: rgb(98,98,98)">=</span> predict(<span class="ansi-yellow-bg">X_train</span>, w,b)
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Train Accuracy: </span><span class="ansi-bold" style="color: rgb(175,95,135)">%f</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(98,98,98)">%</span>(np<span style="color: rgb(98,98,98)">.</span>mean(p <span style="color: rgb(98,98,98)">==</span> y_train) <span style="color: rgb(98,98,98)">*</span> <span style="color: rgb(98,98,98)">100</span>))

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<table><tr><td><p>Train Accuracy (approx):</p>
</td><td><p>92.00</p>
</td></tr></table><p>## 3 - Regularized Logistic Regression</p>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<p>### 3.1 Problem Statement</p>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. - From these two tests, you would like to determine whether the microchips should be accepted or rejected. - To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
<p>### 3.2 Loading and visualizing the data</p>
<p>Similar to previous parts of this exercise, let’s start by loading the dataset for this task and visualizing it.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains the test results for the microchips from two tests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span></code> contains the results of the QA</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the microchip was accepted</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the microchip was rejected</p></li>
</ul>
</li>
<li><p>Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p></li>
</ul>
</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;data/ex2data2.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[22], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># load dataset</span>
<span class="ansi-green-fg">----&gt; 2</span> X_train, y_train <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">load_data</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">data/ex2data2.txt</span><span style="color: rgb(175,0,0)">&#34;</span>)

<span class="ansi-red-fg">NameError</span>: name &#39;load_data&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="id1">
<h2>View the variables<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and the type of the variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print X_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># print y_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># print X_train</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">X_train:</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">X_train</span>[:<span style="color: rgb(98,98,98)">5</span>])
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Type of X_train:</span><span style="color: rgb(175,0,0)">&#34;</span>,<span style="color: rgb(0,135,0)">type</span>(X_train))
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span style="color: rgb(95,135,135)"># print y_train</span>

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="id2">
<h2>Check the dimensions of your variables<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[24], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">The shape of X_train is: </span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(<span class="ansi-yellow-bg">X_train</span><span style="color: rgb(98,98,98)">.</span>shape))
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">The shape of y_train is: </span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">str</span>(y_train<span style="color: rgb(98,98,98)">.</span>shape))
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span style="color: rgb(0,135,0)">print</span> (<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">We have m = </span><span class="ansi-bold" style="color: rgb(175,95,135)">%d</span><span style="color: rgb(175,0,0)"> training examples</span><span style="color: rgb(175,0,0)">&#39;</span> <span style="color: rgb(98,98,98)">%</span> (<span style="color: rgb(0,135,0)">len</span>(y_train)))

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
</section>
<section id="id3">
<h2>Visualize your data<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h2>
<p>The helper function <code class="docutils literal notranslate"><span class="pre">plot_data</span></code> (from <code class="docutils literal notranslate"><span class="pre">utils.py</span></code>) is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>
<p><img alt="b80a128676b649b9825c383c97259e2f" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/archive/images/figure3.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Accepted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Rejected&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 2&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[25], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Plot examples</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">plot_data</span>(X_train, y_train[:], pos_label<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Accepted</span><span style="color: rgb(175,0,0)">&#34;</span>, neg_label<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Rejected</span><span style="color: rgb(175,0,0)">&#34;</span>)
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(95,135,135)"># Set the y-axis label</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plt<span style="color: rgb(98,98,98)">.</span>ylabel(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Microchip Test 2</span><span style="color: rgb(175,0,0)">&#39;</span>)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_data&#39; is not defined
</pre></div></div>
</div>
<p>Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
<p>### 3.3 Feature mapping</p>
<p>One way to fit the data better is to create more features from each data point. In the provided function <code class="docutils literal notranslate"><span class="pre">map_feature</span></code>, we will map the features into all polynomial terms of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> up to the sixth power.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{map\_feature}(x) =
\left[\begin{array}{c}
x_1\\
x_2\\
x_1^2\\
x_1 x_2\\
x_2^2\\
x_1^3\\
\vdots\\
x_1 x_2^5\\
x_2^6\end{array}\right]\end{split}\]</div>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.</p>
<ul class="simple">
<li><p>A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.</p></li>
<li><p>We have provided the <code class="docutils literal notranslate"><span class="pre">map_feature</span></code> function for you in utils.py.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape of data:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mapped_X</span> <span class="o">=</span>  <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape after feature mapping:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[26], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Original shape of data:</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">X_train</span><span style="color: rgb(98,98,98)">.</span>shape)
<span class="ansi-green-intense-fg ansi-bold">      3</span> mapped_X <span style="color: rgb(98,98,98)">=</span>  map_feature(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Shape after feature mapping:</span><span style="color: rgb(175,0,0)">&#34;</span>, mapped_X<span style="color: rgb(98,98,98)">.</span>shape)

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p>Let’s also print the first elements of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">mapped_X</span></code> to see the tranformation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train[0]:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mapped X_train[0]:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[27], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">X_train[0]:</span><span style="color: rgb(175,0,0)">&#34;</span>, <span class="ansi-yellow-bg">X_train</span>[<span style="color: rgb(98,98,98)">0</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">mapped X_train[0]:</span><span style="color: rgb(175,0,0)">&#34;</span>, mapped_X[<span style="color: rgb(98,98,98)">0</span>])

<span class="ansi-red-fg">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div></div>
</div>
<p>While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
<p>### 3.4 Cost function for regularized logistic regression</p>
<p>In this part, you will implement the cost function for regularized logistic regression.</p>
<p>Recall that for regularized logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Compare this to the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}.b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]</div>
<p>The difference is the regularization term, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(b\)</span> parameter is not regularized.</p>
<p>### Exercise 5</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function below to calculate the following term for each element in <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>The starter code then adds this to the cost without regularization (which you computed above in <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code>) to calculate the cost with regulatization.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C5</span>
<span class="k">def</span> <span class="nf">compute_cost_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (array_like Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : (array_like Shape (n,)) Values of bias parameter of the model</span>
<span class="sd">      lambda_ : (scalar, float)    Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calls the compute_cost function that you implemented above</span>
    <span class="n">cost_without_reg</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># You need to calculate this value</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>


    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Add the regularization cost to get the total cost</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost_without_reg</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_cost_reg(X, y, w, b, lambda_ = 1):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   m, n = X.shape

    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b)

    # You need to calculate this value
    reg_cost = 0.

    ### START CODE HERE ###
    for j in range(n):
        reg_cost_j = # Your code here to calculate the cost from w[j]
        reg_cost = reg_cost + reg_cost_j

    ### END CODE HERE ###

    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

return total_cost
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">reg_cost_j</span></code></p>
<details><p>Hint to calculate reg_cost_j     You can use calculate reg_cost_j as reg_cost_j = w[j]**2</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost :&quot;</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="c1"># UNIT TEST</span>
<span class="n">compute_cost_reg_test</span><span class="p">(</span><span class="n">compute_cost_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[29], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> X_mapped <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">map_feature</span>(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(X_mapped<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>]) <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(98,98,98)">0.5</span>

<span class="ansi-red-fg">NameError</span>: name &#39;map_feature&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost :</p>
</td><td><p>0.6618252552483948</p>
</td></tr></table><p>### 3.5 Gradient for regularized logistic regression</p>
<p>In this section, you will implement the gradient for regularized logistic regression.</p>
<p>The gradient of the regularized cost function has two components. The first, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is a scalar, the other is a vector with the same shape as the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, where the <span class="math notranslate nohighlight">\(j^\mathrm{th}\)</span> element is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \left( \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>As you can see,<span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is the same, the difference is the following term in <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>### Exercise 6</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function below to modify the code below to calculate the following term</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>The starter code will add this term to the <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span> returned from <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> above to get the gradient for the regularized cost function.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C6</span>
<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n))   variable such as house size</span>
<span class="sd">      y : (ndarray Shape (m,))    actual value</span>
<span class="sd">      w : (ndarray Shape (n,))    values of parameters of the model</span>
<span class="sd">      b : (scalar)                value of parameter of the model</span>
<span class="sd">      lambda_ : (scalar,float)    regularization constant</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>


    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient_reg(X, y, w, b, lambda_ = 1): m, n = X.shape</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db, dj_dw = compute_gradient(X, y, w, b)

### START CODE HERE ###
# Loop over the elements of w
for j in range(n):

    dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]

    # Add the regularization term  to the correspoding element of dj_dw
    dj_dw[j] = dj_dw[j] + dj_dw_j_reg

### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">dj_dw_j_reg</span></code></p>
<details><p>Hint to calculate dj_dw_j_reg     You can use calculate dj_dw_j_reg as dj_dw_j_reg = (lambda_ / m) * w[j]</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First few elements of regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_reg_test</span><span class="p">(</span><span class="n">compute_gradient_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[31], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> X_mapped <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">map_feature</span>(X_train[:, <span style="color: rgb(98,98,98)">0</span>], X_train[:, <span style="color: rgb(98,98,98)">1</span>])
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_w  <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(X_mapped<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>]) <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(98,98,98)">0.5</span>

<span class="ansi-red-fg">NameError</span>: name &#39;map_feature&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db:0.07138288792343656</p>
</td></tr><tr><td><p>First few elements of regularized dj_dw:</p>
</td></tr><tr><td><p>[[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]]</p>
</td></tr></table><p>### 3.6 Learning parameters using gradient descent</p>
<p>Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>. - If you have completed the cost and gradient for regularized logistic regression correctly, you should be able to step through the next cell to learn the parameters <span class="math notranslate nohighlight">\(w\)</span>. - After training our parameters, we will use it to plot the decision boundary.</p>
<p><strong>Note</strong></p>
<p>The code block below takes quite a while to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you hae time, run for 100,000 iterations to see better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize fitting parameters</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                    <span class="n">compute_cost_reg</span><span class="p">,</span> <span class="n">compute_gradient_reg</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[32], line 3</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)"># Initialize fitting parameters</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>seed(<span style="color: rgb(98,98,98)">1</span>)
<span class="ansi-green-fg">----&gt; 3</span> initial_w <span style="color: rgb(98,98,98)">=</span> np<span style="color: rgb(98,98,98)">.</span>random<span style="color: rgb(98,98,98)">.</span>rand(<span class="ansi-yellow-bg">X_mapped</span><span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">1</span>])<span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1.</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> <span style="color: rgb(95,135,135)"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;X_mapped&#39; is not defined
</pre></div></div>
</div>
<details><p>Expected Output: Cost &lt; 0.5 (Click for details)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Using the following settings
#np.random.seed(1)
#initial_w = np.random.rand(X_mapped.shape[1])-0.5
#initial_b = 1.
#lambda_ = 0.01;
#iterations = 10000
#alpha = 0.01
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div>
</div>
<p>### 3.7 Plotting the decision boundary To help you visualize the model learned by this classifier, we will use our <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary</span></code> function which plots the (non-linear) decision boundary that separates the positive and negative examples.</p>
<ul class="simple">
<li><p>In the function, we plotted the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then drew a contour plot of where the predictions change from y = 0 to y = 1.</p></li>
<li><p>After learning the parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>, the next step is to plot a decision boundary similar to Figure 4.</p></li>
</ul>
<p><img alt="95766c83e38c413887ae9ea0df256d4a" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/archive/images/figure4.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[33], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">plot_decision_boundary</span>(w, b, X_mapped, y_train)

<span class="ansi-red-fg">NameError</span>: name &#39;plot_decision_boundary&#39; is not defined
</pre></div></div>
</div>
<p>### 3.8 Evaluating regularized logistic regression model</p>
<p>You will use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on the training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[34], line 2</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span style="color: rgb(95,135,135)">#Compute accuracy on the training set</span>
<span class="ansi-green-fg">----&gt; 2</span> p <span style="color: rgb(98,98,98)">=</span> predict(<span class="ansi-yellow-bg">X_mapped</span>, w, b)
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(175,0,0)">Train Accuracy: </span><span class="ansi-bold" style="color: rgb(175,95,135)">%f</span><span style="color: rgb(175,0,0)">&#39;</span><span style="color: rgb(98,98,98)">%</span>(np<span style="color: rgb(98,98,98)">.</span>mean(p <span style="color: rgb(98,98,98)">==</span> y_train) <span style="color: rgb(98,98,98)">*</span> <span style="color: rgb(98,98,98)">100</span>))

<span class="ansi-red-fg">NameError</span>: name &#39;X_mapped&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Train Accuracy:~ 80%</p>
</td></tr></table><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>