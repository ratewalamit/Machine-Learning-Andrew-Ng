<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2 - Logistic Regression &mdash; Machine Learning by Andrew Ng  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Machine Learning by Andrew Ng
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Supervised.html">Supervised_Machine_Learning</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Machine Learning by Andrew Ng</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">2 - Logistic Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/source_files/Supervised_Machine_Learning_Regression_and_Classification/Practice.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>

<span class="n">niter</span><span class="o">=</span><span class="mi">10000</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>



<span class="c1">#wx,by=np.meshgrid(np.linspace(-6,12,100),np.linspace(10,-20,100))</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">#constraining parameters</span>
        <span class="n">b</span><span class="o">=-</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="c1">#print(cost_i[i],alpha)</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>




<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">theta_f</span><span class="p">),</span><span class="n">y_train</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_4320/3281116716.py:84: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[-9.33e-05  1.40e-03  2.20e-03  4.21e-03  2.22e-03  2.82e-03  5.85e-03
  5.76e-03  1.76e-03  2.81e-03  6.51e-03  6.15e-03  6.38e-03  1.66e-03
  3.68e-03  6.78e-03  6.91e-03  6.66e-03  6.74e-03  1.83e-03  3.54e-03
  7.08e-03  6.94e-03  7.32e-03  6.77e-03  7.03e-03  1.64e-03]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-2.01e-03  4.82e-04  4.12e-04  2.61e-03  1.09e-03  1.44e-03  5.01e-03
  4.80e-03  4.66e-04  1.10e-03  5.78e-03  5.42e-03  5.54e-03  1.32e-04
  2.39e-03  6.16e-03  6.38e-03  6.07e-03  6.03e-03  3.60e-04  2.03e-03
  6.56e-03  6.42e-03  6.87e-03  6.22e-03  6.41e-03  1.51e-05]
[-2.17e-03  4.23e-04  2.28e-04  2.45e-03  9.85e-04  1.32e-03  4.92e-03
  4.70e-03  3.56e-04  9.28e-04  5.70e-03  5.34e-03  5.46e-03 -9.45e-06
  2.27e-03  6.10e-03  6.32e-03  6.01e-03  5.96e-03  2.28e-04  1.88e-03
  6.50e-03  6.36e-03  6.82e-03  6.16e-03  6.34e-03 -1.35e-04]
[-2.31e-03  3.71e-04  4.65e-05  2.29e-03  8.85e-04  1.21e-03  4.83e-03
  4.61e-03  2.51e-04  7.59e-04  5.63e-03  5.27e-03  5.37e-03 -1.47e-04
  2.15e-03  6.03e-03  6.27e-03  5.95e-03  5.89e-03  1.02e-04  1.73e-03
  6.45e-03  6.30e-03  6.78e-03  6.10e-03  6.28e-03 -2.80e-04]
[-2.45e-03  3.27e-04 -1.32e-04  2.14e-03  7.87e-04  1.10e-03  4.75e-03
  4.51e-03  1.52e-04  5.94e-04  5.56e-03  5.19e-03  5.29e-03 -2.79e-04
  2.04e-03  5.97e-03  6.21e-03  5.89e-03  5.81e-03 -1.91e-05  1.59e-03
  6.40e-03  6.25e-03  6.73e-03  6.04e-03  6.22e-03 -4.21e-04]
[-2.58e-03  2.88e-04 -3.08e-04  1.98e-03  6.92e-04  9.95e-04  4.67e-03
  4.42e-03  5.69e-05  4.32e-04  5.49e-03  5.12e-03  5.20e-03 -4.08e-04
  1.94e-03  5.91e-03  6.16e-03  5.83e-03  5.74e-03 -1.35e-04  1.45e-03
  6.35e-03  6.19e-03  6.68e-03  5.99e-03  6.15e-03 -5.57e-04]
[-2.69e-03  2.55e-04 -4.82e-04  1.84e-03  6.00e-04  8.97e-04  4.59e-03
  4.33e-03 -3.31e-05  2.73e-04  5.42e-03  5.04e-03  5.12e-03 -5.33e-04
  1.83e-03  5.84e-03  6.11e-03  5.77e-03  5.67e-03 -2.47e-04  1.31e-03
  6.30e-03  6.14e-03  6.64e-03  5.93e-03  6.09e-03 -6.88e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.    0.    0.01  0.
  0.01 -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-2.90e-03  2.07e-04 -8.22e-04  1.55e-03  4.22e-04  7.17e-04  4.43e-03
  4.16e-03 -2.00e-04 -3.44e-05  5.28e-03  4.90e-03  4.96e-03 -7.72e-04
  1.64e-03  5.72e-03  6.01e-03  5.66e-03  5.53e-03 -4.58e-04  1.05e-03
  6.20e-03  6.03e-03  6.55e-03  5.82e-03  5.97e-03 -9.38e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.22e-03  1.67e-04 -1.47e-03  1.00e-03  8.84e-05  4.13e-04  4.12e-03
  3.83e-03 -4.86e-04 -6.15e-04  5.03e-03  4.61e-03  4.66e-03 -1.21e-03
  1.30e-03  5.49e-03  5.82e-03  5.43e-03  5.26e-03 -8.32e-04  5.58e-04
  6.01e-03  5.82e-03  6.38e-03  5.60e-03  5.73e-03 -1.39e-03]
[-3.28e-03  1.68e-04 -1.63e-03  8.75e-04  9.12e-06  3.48e-04  4.05e-03
  3.75e-03 -5.49e-04 -7.53e-04  4.97e-03  4.54e-03  4.59e-03 -1.31e-03
  1.23e-03  5.43e-03  5.77e-03  5.38e-03  5.20e-03 -9.17e-04  4.42e-04
  5.96e-03  5.77e-03  6.34e-03  5.55e-03  5.68e-03 -1.50e-03]
[-3.33e-03  1.73e-04 -1.79e-03  7.49e-04 -6.88e-05  2.87e-04  3.98e-03
  3.68e-03 -6.09e-04 -8.89e-04  4.91e-03  4.47e-03  4.52e-03 -1.41e-03
  1.16e-03  5.37e-03  5.72e-03  5.33e-03  5.14e-03 -9.99e-04  3.28e-04
  5.92e-03  5.72e-03  6.30e-03  5.50e-03  5.62e-03 -1.60e-03]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.46e-03  2.06e-04 -2.24e-03  3.87e-04 -2.95e-04  1.25e-04  3.77e-03
  3.46e-03 -7.72e-04 -1.28e-03  4.74e-03  4.27e-03  4.32e-03 -1.70e-03
  9.53e-04  5.21e-03  5.59e-03  5.17e-03  4.95e-03 -1.23e-03  2.43e-06
  5.79e-03  5.57e-03  6.18e-03  5.34e-03  5.46e-03 -1.89e-03]
[-3.49e-03  2.24e-04 -2.39e-03  2.71e-04 -3.68e-04  7.81e-05  3.70e-03
  3.39e-03 -8.21e-04 -1.41e-03  4.68e-03  4.20e-03  4.25e-03 -1.79e-03
  8.91e-04  5.16e-03  5.55e-03  5.12e-03  4.89e-03 -1.30e-03 -1.02e-04
  5.74e-03  5.52e-03  6.14e-03  5.29e-03  5.41e-03 -1.99e-03]
[-3.52e-03  2.44e-04 -2.54e-03  1.58e-04 -4.40e-04  3.39e-05  3.63e-03
  3.32e-03 -8.68e-04 -1.53e-03  4.63e-03  4.14e-03  4.19e-03 -1.88e-03
  8.32e-04  5.10e-03  5.51e-03  5.07e-03  4.83e-03 -1.37e-03 -2.04e-04
  5.70e-03  5.47e-03  6.10e-03  5.24e-03  5.36e-03 -2.08e-03]
[-3.54e-03  2.67e-04 -2.68e-03  4.64e-05 -5.11e-04 -7.22e-06  3.57e-03
  3.25e-03 -9.12e-04 -1.65e-03  4.57e-03  4.07e-03  4.12e-03 -1.97e-03
  7.75e-04  5.05e-03  5.46e-03  5.02e-03  4.78e-03 -1.43e-03 -3.04e-04
  5.66e-03  5.42e-03  6.06e-03  5.19e-03  5.31e-03 -2.16e-03]
[-3.55e-03  2.92e-04 -2.82e-03 -6.24e-05 -5.82e-04 -4.56e-05  3.50e-03
  3.19e-03 -9.54e-04 -1.77e-03  4.52e-03  4.01e-03  4.06e-03 -2.05e-03
  7.20e-04  5.00e-03  5.42e-03  4.97e-03  4.72e-03 -1.50e-03 -4.01e-04
  5.62e-03  5.38e-03  6.03e-03  5.14e-03  5.26e-03 -2.25e-03]
[-3.57e-03  3.19e-04 -2.96e-03 -1.69e-04 -6.51e-04 -8.13e-05  3.44e-03
  3.12e-03 -9.94e-04 -1.89e-03  4.47e-03  3.94e-03  4.00e-03 -2.14e-03
  6.67e-04  4.95e-03  5.38e-03  4.92e-03  4.66e-03 -1.56e-03 -4.97e-04
  5.58e-03  5.33e-03  5.99e-03  5.10e-03  5.21e-03 -2.33e-03]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-3.36e-03  9.29e-04 -4.89e-03 -1.54e-03 -1.61e-03 -3.75e-04  2.58e-03
  2.28e-03 -1.41e-03 -3.45e-03  3.79e-03  3.04e-03  3.19e-03 -3.22e-03
  7.68e-05  4.24e-03  4.83e-03  4.24e-03  3.90e-03 -2.28e-03 -1.75e-03
  5.04e-03  4.65e-03  5.47e-03  4.41e-03  4.55e-03 -3.38e-03]
[-3.32e-03  9.78e-04 -5.01e-03 -1.62e-03 -1.67e-03 -3.83e-04  2.53e-03
  2.23e-03 -1.43e-03 -3.54e-03  3.75e-03  2.98e-03  3.15e-03 -3.28e-03
  4.77e-05  4.20e-03  4.80e-03  4.19e-03  3.85e-03 -2.32e-03 -1.82e-03
  5.01e-03  4.61e-03  5.44e-03  4.37e-03  4.51e-03 -3.43e-03]
[-3.29e-03  1.03e-03 -5.13e-03 -1.70e-03 -1.73e-03 -3.89e-04  2.48e-03
  2.18e-03 -1.44e-03 -3.64e-03  3.71e-03  2.93e-03  3.10e-03 -3.34e-03
  1.96e-05  4.16e-03  4.76e-03  4.15e-03  3.81e-03 -2.35e-03 -1.89e-03
  4.98e-03  4.57e-03  5.41e-03  4.33e-03  4.47e-03 -3.49e-03]
[-3.25e-03  1.08e-03 -5.24e-03 -1.78e-03 -1.79e-03 -3.95e-04  2.43e-03
  2.14e-03 -1.46e-03 -3.73e-03  3.67e-03  2.87e-03  3.05e-03 -3.40e-03
 -7.66e-06  4.12e-03  4.73e-03  4.11e-03  3.76e-03 -2.39e-03 -1.96e-03
  4.95e-03  4.53e-03  5.38e-03  4.28e-03  4.43e-03 -3.55e-03]
[-3.22e-03  1.13e-03 -5.35e-03 -1.85e-03 -1.85e-03 -3.99e-04  2.38e-03
  2.09e-03 -1.47e-03 -3.81e-03  3.63e-03  2.82e-03  3.01e-03 -3.47e-03
 -3.40e-05  4.07e-03  4.70e-03  4.07e-03  3.72e-03 -2.42e-03 -2.03e-03
  4.92e-03  4.49e-03  5.35e-03  4.24e-03  4.39e-03 -3.60e-03]
[-3.18e-03  1.18e-03 -5.47e-03 -1.93e-03 -1.90e-03 -4.03e-04  2.33e-03
  2.04e-03 -1.49e-03 -3.90e-03  3.60e-03  2.76e-03  2.96e-03 -3.52e-03
 -5.95e-05  4.03e-03  4.67e-03  4.03e-03  3.68e-03 -2.46e-03 -2.10e-03
  4.89e-03  4.45e-03  5.32e-03  4.20e-03  4.35e-03 -3.66e-03]
[-3.14e-03  1.24e-03 -5.58e-03 -2.00e-03 -1.96e-03 -4.06e-04  2.28e-03
  2.00e-03 -1.50e-03 -3.99e-03  3.56e-03  2.71e-03  2.92e-03 -3.58e-03
 -8.42e-05  3.99e-03  4.63e-03  3.99e-03  3.63e-03 -2.49e-03 -2.17e-03
  4.85e-03  4.41e-03  5.29e-03  4.16e-03  4.32e-03 -3.71e-03]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.34  0.18 -0.73 -0.28 -0.27 -0.05  0.24  0.2  -0.19 -0.53  0.4   0.28
  0.31 -0.46 -0.03  0.44  0.53  0.44  0.4  -0.32 -0.3   0.55  0.49  0.6
  0.46  0.48 -0.47] 0.3499985218344349 260.61872511912435
[[ 5.13e-02  7.00e-01  2.63e-03 ...  6.29e-04  8.59e-03  1.17e-01]
 [-9.27e-02  6.85e-01  8.60e-03 ...  1.89e-03 -1.40e-02  1.03e-01]
 [-2.14e-01  6.92e-01  4.57e-02 ...  1.05e-02 -3.40e-02  1.10e-01]
 ...
 [-4.84e-01  9.99e-01  2.35e-01 ...  2.34e-01 -4.83e-01  9.96e-01]
 [-6.34e-03  9.99e-01  4.01e-05 ...  4.00e-05 -6.31e-03  9.96e-01]
 [ 6.33e-01 -3.06e-02  4.00e-01 ...  3.51e-07 -1.70e-08  8.23e-10]] [0.51 0.53 0.52 0.59 0.58 0.58 0.59 0.58 0.56 0.54 0.48 0.44 0.41 0.35
 0.5  0.53 0.55 0.44 0.52 0.54 0.53 0.56 0.56 0.56 0.56 0.51 0.46 0.36
 0.58 0.46 0.26 0.45 0.54 0.59 0.6  0.6  0.58 0.58 0.57 0.53 0.54 0.48
 0.41 0.5  0.45 0.48 0.46 0.55 0.51 0.51 0.55 0.59 0.57 0.6  0.6  0.6
 0.59 0.57 0.34 0.47 0.53 0.5  0.52 0.51 0.47 0.35 0.37 0.22 0.22 0.26
 0.22 0.24 0.24 0.27 0.37 0.44 0.48 0.49 0.46 0.49 0.52 0.56 0.54 0.56
 0.47 0.53 0.51 0.45 0.51 0.3  0.3  0.49 0.47 0.34 0.23 0.1  0.2  0.47
 0.54 0.54 0.52 0.13 0.54 0.52 0.57 0.55 0.57 0.59 0.55 0.51 0.5  0.5
 0.47 0.5  0.56 0.18 0.24 0.43] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "aace8e5ec62b4e82bf476b600795c20f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],[</span><span class="mf">2.7</span><span class="p">,</span><span class="mf">1.6</span><span class="p">],[</span><span class="mf">2.8</span><span class="p">,</span><span class="mf">1.2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.2</span><span class="p">],[</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">1.3</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">longdouble</span><span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mf">1.5</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mf">1.5</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<br/><br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_1_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_1_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>

<span class="n">niter</span><span class="o">=</span><span class="mi">10000</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span><span class="mf">3.</span><span class="p">,</span><span class="mf">5.</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>



<span class="c1">#wx,by=np.meshgrid(np.linspace(-6,12,100),np.linspace(10,-20,100))</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="c1">#if w.shape!=x[i].shape:</span>
        <span class="c1">#    print(&quot;Shape of W and X dosn&#39;t match&quot;, w.shape,x[i].shape)</span>
        <span class="c1">#    sys.exit()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">#constraining parameters</span>
        <span class="n">b</span><span class="o">=-</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="c1">#print(cost_i[i],alpha)</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>




<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">theta_f</span><span class="p">),</span><span class="n">y_train</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 95</span>
<span class="ansi-green-intense-fg ansi-bold">     93</span> alpha<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-intense-fg ansi-bold">     94</span> theta_in<span style="color: rgb(98,98,98)">=</span>Win,Bin
<span class="ansi-green-fg">---&gt; 95</span> grad_dec_result,theta_f<span style="color: rgb(98,98,98)">=</span><span class="ansi-yellow-bg">gradient_decent</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">y_train</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">theta_in</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">alpha</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">niter</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     97</span> wf,bf<span style="color: rgb(98,98,98)">=</span>theta_f
<span class="ansi-green-intense-fg ansi-bold">     98</span> <span style="color: rgb(0,135,0)">print</span>(wf,bf,grad_dec_result[<span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">1</span>])

Cell <span class="ansi-green-fg">In[23], line 77</span>, in <span class="ansi-cyan-fg">gradient_decent</span><span class="ansi-blue-fg">(x, y, theta, alpha, niter)</span>
<span class="ansi-green-intense-fg ansi-bold">     75</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> np<span style="color: rgb(98,98,98)">.</span>abs((cost_i[i]<span style="color: rgb(98,98,98)">-</span>cost_i[i<span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">1</span>])<span style="color: rgb(98,98,98)">/</span>cost_i[i])<span style="color: rgb(98,98,98)">&lt;</span><span style="color: rgb(98,98,98)">0.05</span>:
<span class="ansi-green-intense-fg ansi-bold">     76</span>         alpha<span style="color: rgb(98,98,98)">/</span><span style="color: rgb(98,98,98)">=</span><span style="color: rgb(98,98,98)">2</span>
<span class="ansi-green-fg">---&gt; 77</span> dcw,dcb<span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">compute_gradient</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">theta</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">y</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     79</span> w <span style="color: rgb(98,98,98)">=</span> w<span style="color: rgb(98,98,98)">-</span>alpha<span style="color: rgb(98,98,98)">*</span>dcw
<span class="ansi-green-intense-fg ansi-bold">     80</span> b <span style="color: rgb(98,98,98)">=</span> b<span style="color: rgb(98,98,98)">-</span>alpha<span style="color: rgb(98,98,98)">*</span>dcb

Cell <span class="ansi-green-fg">In[23], line 59</span>, in <span class="ansi-cyan-fg">compute_gradient</span><span class="ansi-blue-fg">(x, theta, y)</span>
<span class="ansi-green-intense-fg ansi-bold">     58</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">compute_gradient</span>(x,theta,y):
<span class="ansi-green-fg">---&gt; 59</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">dcost_w</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">theta</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">y</span><span class="ansi-yellow-bg">)</span>,dcost_b(x,theta,y)

Cell <span class="ansi-green-fg">In[23], line 52</span>, in <span class="ansi-cyan-fg">dcost_w</span><span class="ansi-blue-fg">(x, theta, y)</span>
<span class="ansi-green-intense-fg ansi-bold">     50</span> dcost_w_result<span style="color: rgb(98,98,98)">=</span>np<span style="color: rgb(98,98,98)">.</span>zeros(w<span style="color: rgb(98,98,98)">.</span>shape)
<span class="ansi-green-intense-fg ansi-bold">     51</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> wi <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> <span style="color: rgb(0,135,0)">range</span>(<span style="color: rgb(0,135,0)">len</span>(w)):
<span class="ansi-green-fg">---&gt; 52</span>     dcost_w_result[wi]<span style="color: rgb(98,98,98)">=</span>np<span style="color: rgb(98,98,98)">.</span>sum((<span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">theta</span><span class="ansi-yellow-bg">)</span><span style="color: rgb(98,98,98)">-</span>y)<span style="color: rgb(98,98,98)">*</span>dmodel_w(x,theta)[:,wi])<span style="color: rgb(98,98,98)">/</span><span style="color: rgb(0,135,0)">len</span>(x)
<span class="ansi-green-intense-fg ansi-bold">     53</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span>  dcost_w_result

Cell <span class="ansi-green-fg">In[23], line 28</span>, in <span class="ansi-cyan-fg">model</span><span class="ansi-blue-fg">(x, theta)</span>
<span class="ansi-green-intense-fg ansi-bold">     23</span> sigmoid<span style="color: rgb(98,98,98)">=</span>np<span style="color: rgb(98,98,98)">.</span>zeros(<span style="color: rgb(0,135,0)">len</span>(x))
<span class="ansi-green-intense-fg ansi-bold">     24</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> i <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> <span style="color: rgb(0,135,0)">range</span>(<span style="color: rgb(0,135,0)">len</span>(x)):
<span class="ansi-green-intense-fg ansi-bold">     25</span>     <span style="color: rgb(95,135,135)">#if w.shape!=x[i].shape:</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     <span style="color: rgb(95,135,135)">#    print(&#34;Shape of W and X dosn&#39;t match&#34;, w.shape,x[i].shape)</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     <span style="color: rgb(95,135,135)">#    sys.exit()            </span>
<span class="ansi-green-fg">---&gt; 28</span>     sigmoid[i]<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(98,98,98)">1</span><span style="color: rgb(98,98,98)">/</span>(<span style="color: rgb(98,98,98)">1</span><span style="color: rgb(98,98,98)">+</span>np<span style="color: rgb(98,98,98)">.</span>exp(<span style="color: rgb(98,98,98)">-</span>(<span class="ansi-yellow-bg">np</span><span class="ansi-yellow-bg" style="color: rgb(98,98,98)">.</span><span class="ansi-yellow-bg">dot</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">w</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">i</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span><span style="color: rgb(98,98,98)">+</span>b)))
<span class="ansi-green-intense-fg ansi-bold">     29</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> sigmoid

File <span class="ansi-green-fg">&lt;__array_function__ internals&gt;:200</span>, in <span class="ansi-cyan-fg">dot</span><span class="ansi-blue-fg">(*args, **kwargs)</span>

<span class="ansi-red-fg">ValueError</span>: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
<span class="n">x_train</span><span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">lam</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">w</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="c1">#if theta[1]&gt;0:  #constraining parameters</span>
    <span class="c1">#    b=-theta[1]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">dcw_reg</span><span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="c1">#print(dcw_reg)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">dcw</span><span class="o">+</span><span class="n">dcw_reg</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The cost is&quot;</span><span class="p">,</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">100</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">lam_in</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam_in</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">theta_f</span><span class="p">)</span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
<span class="n">x_train</span><span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">lam</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">w</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">#constraining parameters</span>
        <span class="n">b</span><span class="o">=-</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">dcw_reg</span><span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dcw_reg</span><span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">dcw</span><span class="o">+</span><span class="n">dcw_reg</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="c1">#print(&quot;The cost is&quot;,cost_i[i],theta)</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">100</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">lam_in</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam_in</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">theta_f</span><span class="p">),</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
<span class="n">x_train</span><span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">lam</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">w</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">#constraining parameters</span>
        <span class="n">b</span><span class="o">=-</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">dcw_reg</span><span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">dcw_reg</span><span class="p">)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">dcw</span><span class="o">+</span><span class="n">dcw_reg</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="c1">#print(&quot;The cost is&quot;,cost_i[i],theta)</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">100</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">lam_in</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam_in</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>


<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">bf</span><span class="o">/</span><span class="n">wf</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted model&quot;</span><span class="p">)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">theta_f</span><span class="p">),</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-cyan-fg">  Cell </span><span class="ansi-green-fg">In[13], line 145</span>
<span class="ansi-red-fg">    print(x_train, model(x_train,theta_f)# load dataset</span>
         ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> &#39;(&#39; was never closed

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">initial_w</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2.3752155572731053
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Regularized cost : 0.6618252552483951
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 2.0028840493199764
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_6369/878878066.py:82: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 0.9256284847946785
The cost is 0.7321371677849211
The cost is 0.6598208762641768
The cost is 0.6204053236990379
The cost is 0.5947327181536963
The cost is 0.5769735539510019
The cost is 0.5643870236028828
The cost is 0.555336262275471
The cost is 0.5487540462424862
The cost is 0.5439204525169686
The cost is 0.5403404754899873
The cost is 0.537668730905861
The cost is 0.5356611484735105
The cost is 0.5341432903438514
The cost is 0.5329892209020973
The cost is 0.5321071993517749
The cost is 0.5314298559357999
The cost is 0.53090735989295
The cost is 0.5305026126614626
The cost is 0.5301878305603884
The cost is 0.529942092517406
The cost is 0.5297495655369997
The cost is 0.529598210913902
The cost is 0.5294788345282262
The cost is 0.5293843854020753
The cost is 0.5293094346883392
The cost is 0.5292497866608107
The cost is 0.5292021868554273
The cost is 0.5291641021078087
The cost is 0.5291335540683393
The cost is 0.5291089926830363
The cost is 0.5290891996748446
The cost is 0.52907321463902
The cost is 0.5290602782525139
The cost is 0.5290497884840468
The cost is 0.5290412667161044
The cost is 0.5290343314504222
The cost is 0.5290286778351836
The cost is 0.5290240616761656
The cost is 0.5290202869125287
The cost is 0.5290171957780294
The cost is 0.5290146610500187
The cost is 0.5290125799264344
The cost is 0.529010869175946
The cost is 0.5290094612865763
The cost is 0.5290083013995638
The cost is 0.5290073448624335
The cost is 0.5290065552716438
The cost is 0.5290059029033056
[ 0.62  1.18 -2.02 -0.92 -1.43  0.13 -0.37 -0.36 -0.17 -1.46 -0.05 -0.61
 -0.27 -1.19 -0.24 -0.2  -0.04 -0.27 -0.29 -0.46 -1.04  0.03 -0.28  0.02
 -0.32 -0.14 -0.93] 1.2715618096527268 0.5290053880660263
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(array([ 0.62,  1.18, -2.02, -0.92, -1.43,  0.13, -0.37, -0.36, -0.17,
       -1.46, -0.05, -0.61, -0.27, -1.19, -0.24, -0.2 , -0.04, -0.27,
       -0.29, -0.46, -1.04,  0.03, -0.28,  0.02, -0.32, -0.14, -0.93]), 1.2715618096527268)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6c7cde3c85ff4845bc6c9df1ae1f7790", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">theta_f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(array([ 0.62,  1.18, -2.02, -0.92, -1.43,  0.13, -0.37, -0.36, -0.17,
       -1.46, -0.05, -0.61, -0.27, -1.19, -0.24, -0.2 , -0.04, -0.27,
       -0.29, -0.46, -1.04,  0.03, -0.28,  0.02, -0.32, -0.14, -0.93]), 1.2715618096527268)
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">meshd</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[array([[-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ],
        [-1.  , -0.89, -0.79, -0.68, -0.58, -0.47, -0.37, -0.26, -0.16,
         -0.05,  0.05,  0.16,  0.26,  0.37,  0.47,  0.58,  0.68,  0.79,
          0.89,  1.  ]]),
 array([[-1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,
         -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  , -1.  ,
         -1.  , -1.  ],
        [-0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89,
         -0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89, -0.89,
         -0.89, -0.89],
        [-0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79,
         -0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79, -0.79,
         -0.79, -0.79],
        [-0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68,
         -0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68, -0.68,
         -0.68, -0.68],
        [-0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58,
         -0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58, -0.58,
         -0.58, -0.58],
        [-0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47,
         -0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47, -0.47,
         -0.47, -0.47],
        [-0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37,
         -0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37, -0.37,
         -0.37, -0.37],
        [-0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26,
         -0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26, -0.26,
         -0.26, -0.26],
        [-0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16,
         -0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16, -0.16,
         -0.16, -0.16],
        [-0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05,
         -0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05, -0.05,
         -0.05, -0.05],
        [ 0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
          0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,  0.05,
          0.05,  0.05],
        [ 0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,
          0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,  0.16,
          0.16,  0.16],
        [ 0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,
          0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,  0.26,
          0.26,  0.26],
        [ 0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,
          0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,  0.37,
          0.37,  0.37],
        [ 0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,
          0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,  0.47,
          0.47,  0.47],
        [ 0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,
          0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,  0.58,
          0.58,  0.58],
        [ 0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,
          0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,  0.68,
          0.68,  0.68],
        [ 0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,
          0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,  0.79,
          0.79,  0.79],
        [ 0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,
          0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,  0.89,
          0.89,  0.89],
        [ 1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,
          1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,
          1.  ,  1.  ]])]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[54]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z_plot</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[54]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[1.41e-04, 4.91e-04, 1.25e-03, 2.55e-03, 4.44e-03, 6.92e-03,
        9.93e-03, 1.34e-02, 1.71e-02, 2.09e-02, 2.46e-02, 2.76e-02,
        2.95e-02, 2.97e-02, 2.77e-02, 2.33e-02, 1.67e-02, 9.68e-03,
        4.09e-03, 1.11e-03],
       [6.82e-04, 2.28e-03, 5.60e-03, 1.11e-02, 1.89e-02, 2.88e-02,
        4.05e-02, 5.37e-02, 6.78e-02, 8.21e-02, 9.57e-02, 1.07e-01,
        1.16e-01, 1.18e-01, 1.13e-01, 9.90e-02, 7.52e-02, 4.64e-02,
        2.10e-02, 6.11e-03],
       [2.20e-03, 7.06e-03, 1.68e-02, 3.23e-02, 5.32e-02, 7.87e-02,
        1.08e-01, 1.39e-01, 1.71e-01, 2.02e-01, 2.30e-01, 2.55e-01,
        2.72e-01, 2.80e-01, 2.74e-01, 2.48e-01, 2.00e-01, 1.33e-01,
        6.56e-02, 2.05e-02],
       [5.27e-03, 1.64e-02, 3.76e-02, 6.96e-02, 1.11e-01, 1.57e-01,
        2.07e-01, 2.57e-01, 3.05e-01, 3.50e-01, 3.89e-01, 4.21e-01,
        4.45e-01, 4.56e-01, 4.50e-01, 4.20e-01, 3.58e-01, 2.59e-01,
        1.40e-01, 4.74e-02],
       [1.02e-02, 3.07e-02, 6.80e-02, 1.21e-01, 1.84e-01, 2.51e-01,
        3.16e-01, 3.78e-01, 4.35e-01, 4.84e-01, 5.25e-01, 5.58e-01,
        5.81e-01, 5.92e-01, 5.88e-01, 5.60e-01, 4.96e-01, 3.84e-01,
        2.27e-01, 8.31e-02],
       [1.70e-02, 4.94e-02, 1.05e-01, 1.79e-01, 2.61e-01, 3.42e-01,
        4.16e-01, 4.82e-01, 5.38e-01, 5.86e-01, 6.24e-01, 6.54e-01,
        6.74e-01, 6.84e-01, 6.79e-01, 6.54e-01, 5.94e-01, 4.81e-01,
        3.04e-01, 1.19e-01],
       [2.52e-02, 7.09e-02, 1.46e-01, 2.38e-01, 3.33e-01, 4.21e-01,
        4.98e-01, 5.62e-01, 6.15e-01, 6.58e-01, 6.91e-01, 7.17e-01,
        7.34e-01, 7.42e-01, 7.37e-01, 7.13e-01, 6.57e-01, 5.47e-01,
        3.61e-01, 1.47e-01],
       [3.43e-02, 9.36e-02, 1.85e-01, 2.92e-01, 3.96e-01, 4.86e-01,
        5.60e-01, 6.21e-01, 6.69e-01, 7.07e-01, 7.37e-01, 7.58e-01,
        7.72e-01, 7.78e-01, 7.72e-01, 7.49e-01, 6.95e-01, 5.86e-01,
        3.95e-01, 1.64e-01],
       [4.35e-02, 1.16e-01, 2.22e-01, 3.39e-01, 4.47e-01, 5.36e-01,
        6.07e-01, 6.63e-01, 7.07e-01, 7.40e-01, 7.66e-01, 7.85e-01,
        7.96e-01, 8.00e-01, 7.92e-01, 7.68e-01, 7.14e-01, 6.04e-01,
        4.07e-01, 1.67e-01],
       [5.21e-02, 1.36e-01, 2.53e-01, 3.77e-01, 4.86e-01, 5.73e-01,
        6.40e-01, 6.92e-01, 7.32e-01, 7.62e-01, 7.85e-01, 8.01e-01,
        8.10e-01, 8.11e-01, 8.02e-01, 7.76e-01, 7.19e-01, 6.04e-01,
        4.01e-01, 1.59e-01],
       [5.93e-02, 1.52e-01, 2.78e-01, 4.06e-01, 5.14e-01, 5.98e-01,
        6.62e-01, 7.11e-01, 7.47e-01, 7.75e-01, 7.95e-01, 8.08e-01,
        8.15e-01, 8.15e-01, 8.03e-01, 7.74e-01, 7.12e-01, 5.89e-01,
        3.77e-01, 1.41e-01],
       [6.43e-02, 1.63e-01, 2.94e-01, 4.23e-01, 5.31e-01, 6.13e-01,
        6.74e-01, 7.20e-01, 7.54e-01, 7.79e-01, 7.97e-01, 8.09e-01,
        8.14e-01, 8.10e-01, 7.96e-01, 7.61e-01, 6.91e-01, 5.57e-01,
        3.37e-01, 1.16e-01],
       [6.61e-02, 1.67e-01, 2.99e-01, 4.29e-01, 5.35e-01, 6.16e-01,
        6.75e-01, 7.19e-01, 7.52e-01, 7.75e-01, 7.91e-01, 8.01e-01,
        8.04e-01, 7.97e-01, 7.78e-01, 7.37e-01, 6.55e-01, 5.06e-01,
        2.84e-01, 8.77e-02],
       [6.38e-02, 1.62e-01, 2.92e-01, 4.20e-01, 5.25e-01, 6.05e-01,
        6.64e-01, 7.07e-01, 7.38e-01, 7.61e-01, 7.75e-01, 7.83e-01,
        7.83e-01, 7.72e-01, 7.46e-01, 6.95e-01, 5.99e-01, 4.35e-01,
        2.20e-01, 6.00e-02],
       [5.68e-02, 1.46e-01, 2.69e-01, 3.93e-01, 4.96e-01, 5.76e-01,
        6.35e-01, 6.78e-01, 7.09e-01, 7.30e-01, 7.44e-01, 7.49e-01,
        7.44e-01, 7.28e-01, 6.93e-01, 6.28e-01, 5.15e-01, 3.42e-01,
        1.52e-01, 3.61e-02],
       [4.51e-02, 1.20e-01, 2.27e-01, 3.41e-01, 4.41e-01, 5.19e-01,
        5.79e-01, 6.23e-01, 6.54e-01, 6.75e-01, 6.86e-01, 6.87e-01,
        6.78e-01, 6.53e-01, 6.06e-01, 5.25e-01, 3.98e-01, 2.34e-01,
        8.89e-02, 1.84e-02],
       [3.03e-02, 8.38e-02, 1.66e-01, 2.61e-01, 3.50e-01, 4.25e-01,
        4.83e-01, 5.26e-01, 5.57e-01, 5.76e-01, 5.84e-01, 5.81e-01,
        5.64e-01, 5.28e-01, 4.68e-01, 3.77e-01, 2.55e-01, 1.29e-01,
        4.18e-02, 7.54e-03],
       [1.60e-02, 4.65e-02, 9.74e-02, 1.62e-01, 2.28e-01, 2.87e-01,
        3.37e-01, 3.75e-01, 4.01e-01, 4.16e-01, 4.19e-01, 4.10e-01,
        3.86e-01, 3.44e-01, 2.83e-01, 2.05e-01, 1.21e-01, 5.20e-02,
        1.45e-02, 2.29e-03],
       [6.02e-03, 1.82e-02, 4.03e-02, 7.05e-02, 1.05e-01, 1.38e-01,
        1.67e-01, 1.90e-01, 2.06e-01, 2.13e-01, 2.11e-01, 2.00e-01,
        1.80e-01, 1.50e-01, 1.12e-01, 7.18e-02, 3.69e-02, 1.38e-02,
        3.39e-03, 4.67e-04],
       [1.39e-03, 4.41e-03, 1.02e-02, 1.85e-02, 2.86e-02, 3.88e-02,
        4.81e-02, 5.53e-02, 5.98e-02, 6.12e-02, 5.92e-02, 5.40e-02,
        4.58e-02, 3.54e-02, 2.43e-02, 1.40e-02, 6.44e-03, 2.15e-03,
        4.63e-04, 5.54e-05]])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">8</span><span class="o">***************************</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span><span class="nn">os</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">subprocess</span><span class="o">,</span><span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="n">home_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">())</span>
<span class="n">proj_path</span><span class="o">=</span><span class="n">home_path</span><span class="o">+</span><span class="s2">&quot;/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">home_path</span><span class="si">}</span><span class="s2">/week3/C1W3A1/&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<section id="2---Logistic-Regression">
<h1>2 - Logistic Regression<a class="headerlink" href="#2---Logistic-Regression" title="Permalink to this headline"></a></h1>
<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<section id="2.1-Problem-Statement">
<h2>2.1 Problem Statement<a class="headerlink" href="#2.1-Problem-Statement" title="Permalink to this headline"></a></h2>
<p>Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. * You have historical data from previous applicants that you can use as a training set for logistic regression. * For each training example, you have the applicant’s scores on two exams and the admissions decision. * Your task is to build a classification model that estimates an applicant’s probability of admission based on the
scores from those two exams.</p>
</section>
<section id="2.2-Loading-and-visualizing-the-data">
<h2>2.2 Loading and visualizing the data<a class="headerlink" href="#2.2-Loading-and-visualizing-the-data" title="Permalink to this headline"></a></h2>
<p>You will start by loading the dataset for this task. - The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> - <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains exam scores on two exams for a student - <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is the admission decision - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the student was admitted - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the student was not admitted - Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data1.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>View the variables</p>
<div class="line-block">
<div class="line">Let’s get more familiar with your dataset.</div>
<div class="line">- A good place to start is to just print out each variable and see what it contains.</div>
</div>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and the type of the variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[57]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in X_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in X_train are:
 [[34.62 78.02]
 [30.29 43.89]
 [35.85 72.9 ]
 [60.18 86.31]
 [79.03 75.34]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Now print the first five values of <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in y_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in y_train are:
 [0. 0. 0. 1. 1.]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[59]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape</span>
<span class="n">of</span> <span class="n">y_train</span> <span class="ow">is</span><span class="p">:</span> <span class="s1">&#39; + str(y_train.shape))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-cyan-fg">  Cell </span><span class="ansi-green-fg">In[59], line 2</span>
<span class="ansi-red-fg">    print (&#39;The shape</span>
           ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> unterminated string literal (detected at line 2)

</pre></div></div>
</div>
<p>Visualize your data</p>
<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible. - The code below displays the data on a 2D plot (as shown below), where the axes are the two exam scores, and the positive and negative examples are shown with different markers. - We use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to generate this plot.</p>
<p><img alt="fd31b401246049ccbe86719a0b6f6cfb" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure1.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Admitted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Not admitted&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_23_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_23_0.png" />
</div>
</div>
<p>Your goal is to build a logistic regression model to fit this data. - With this model, you can then predict if a new student will be admitted based on their scores on the two exams.</p>
</section>
<section id="2.3-Sigmoid-function">
<h2>2.3 Sigmoid function<a class="headerlink" href="#2.3-Sigmoid-function" title="Permalink to this headline"></a></h2>
<p>Recall that for logistic regression, the model is represented as</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x) = g(\mathbf{w}\cdot \mathbf{x} + b)\]</div>
<p>where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Let’s implement the sigmoid function first, so it can be used by the rest of this assignment.</p>
<p>Exercise 1</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function to calculate</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Note that - <code class="docutils literal notranslate"><span class="pre">z</span></code> is not always a single number, but can also be an array of numbers. - If the input is an array of numbers, we’d like to apply the sigmoid function to each value in the input array.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[61]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Args:</span>
<span class="sd">        z (ndarray): A scalar, numpy array of any size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        g (ndarray): sigmoid(z), with the same shape as z</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    <span class="c1">### END SOLUTION ###</span>

    <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<p><code class="docutils literal notranslate"><span class="pre">numpy</span></code> has a function called <code class="docutils literal notranslate"><span class="pre">`np.exp()</span></code> &lt;<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">https://numpy.org/doc/stable/reference/generated/numpy.exp.html</a>&gt;`__, which offers a convinient way to calculate the exponential ( <span class="math notranslate nohighlight">\(e^{z}\)</span>) of all elements in the input array (<code class="docutils literal notranslate"><span class="pre">z</span></code>).</p>
<details><p>Click for more hints</p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">np.exp(-z)</span></code></p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(1/e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">1/np.exp(-z)</span></code></p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">g</span></code></p>
<details><p>Hint to calculate g g = 1 / (1 + np.exp(-z))</p>
</details></li>
</ul>
</li>
</ul>
</details><p>When you are finished, try testing a few values by calling <code class="docutils literal notranslate"><span class="pre">sigmoid(x)</span></code> in the cell below. - For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. - Evaluating <code class="docutils literal notranslate"><span class="pre">sigmoid(0)</span></code> should give you exactly 0.5.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[62]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid(0) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid(0) = 0.5
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid(0)</p>
</td><td><p>0.5</p>
</td></tr></table><ul class="simple">
<li><p>As mentioned before, your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[63]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid([ -1, 0, 1, 2]) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))))</span>

<span class="c1"># UNIT TESTS</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid([ -1, 0, 1, 2]) = [0.27 0.5  0.73 0.88]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid([-1, 0, 1, 2])</p>
</td><td><p>[0.26894142 0.5 0.73105858 0.88079708]</p>
</td></tr></table></section>
<section id="2.4-Cost-function-for-logistic-regression">
<h2>2.4 Cost function for logistic regression<a class="headerlink" href="#2.4-Cost-function-for-logistic-regression" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the cost function for logistic regression.</p>
<p>Exercise 2</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function using the equations below.</p>
<p>Recall that for logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]</div>
<p>where * m is the number of training examples in the dataset</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is -</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, which is the actual label</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x^{(i)}} + b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p>
<ul class="simple">
<li><p>It might be helpful to first calculate an intermediate variable <span class="math notranslate nohighlight">\(z_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is the number of features, before calculating <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span></p></li>
</ul>
</li>
</ul>
<p>Note: * As you are doing this, remember that the variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are not scalar values but matrices of shape (<span class="math notranslate nohighlight">\(m, n\)</span>) and (<span class="math notranslate nohighlight">\(𝑚\)</span>,1) respectively, where <span class="math notranslate nohighlight">\(𝑛\)</span> is the number of features and <span class="math notranslate nohighlight">\(𝑚\)</span> is the number of training examples. * You can use the sigmoid function that you implemented above for this part.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[64]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED FUNCTION: compute_cost</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : scalar Values of bias parameter of the model</span>
<span class="sd">      lambda_: unused placeholder</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">+=</span> <span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f_wb</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f_wb</span><span class="p">)</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows: <code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">0</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(m):</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">+</span> <span class="pre">2*i</span></code></p>
<ul class="simple">
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">X</span></code> using a for loop and add the <code class="docutils literal notranslate"><span class="pre">loss</span></code> from each iteration to a variable (<code class="docutils literal notranslate"><span class="pre">loss_sum</span></code>) initialized outside the loop.</p></li>
<li><p>Then, you can return the <code class="docutils literal notranslate"><span class="pre">total_cost</span></code> as <code class="docutils literal notranslate"><span class="pre">loss_sum</span></code> divided by <code class="docutils literal notranslate"><span class="pre">m</span></code>.</p></li>
</ul>
<details><p>Click for more hints</p>
<ul class="simple">
<li><p>Here’s how you can structure the overall implementation for this function</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Loop over each training example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>

        <span class="c1"># First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb_ij</span> <span class="o">=</span> <span class="c1"># Your code here to calculate w[j] * X[i][j]</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">z_wb_ij</span> <span class="c1"># equivalent to z_wb = z_wb + z_wb_ij</span>
        <span class="c1"># Add the bias term to z_wb</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span> <span class="c1"># equivalent to z_wb = z_wb + b</span>

        <span class="n">f_wb</span> <span class="o">=</span> <span class="c1"># Your code here to calculate prediction f_wb for a training example</span>
        <span class="n">loss</span> <span class="o">=</span>  <span class="c1"># Your code here to calculate loss for a training example</span>

        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># equivalent to loss_sum = loss_sum + loss</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_sum</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">z_wb_ij</span></code>, <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
<details><p>Hint to calculate z_wb_ij     z_wb_ij = w[j]*X[i][j]</p>
</details><details><p>Hint to calculate f_wb     <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span> where <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. You can simply call the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function implemented above.</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = sigmoid(z_wb)</p>
</details></details><details><p>Hint to calculate loss     You can use the np.log function to calculate the log</p>
<details><p>    More hints to calculate loss     You can compute loss as loss = -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</p>
</details></details></details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[65]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Compute and display cost with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at initial w (zeros): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at initial w (zeros): 0.693
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at initial w (zeros)</p>
</td><td><p>0.693</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">24.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at test w,b: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>


<span class="c1"># UNIT TESTS</span>
<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at test w,b: 0.218
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at test w,b</p>
</td><td><p>0.218</p>
</td></tr></table></section>
<section id="2.5-Gradient-for-logistic-regression">
<h2>2.5 Gradient for logistic regression<a class="headerlink" href="#2.5-Gradient-for-logistic-regression" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the gradient for logistic regression.</p>
<p>Recall that the gradient descent algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; b := b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \newline       \; &amp; w_j := w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1}\newline &amp; \rbrace\end{align*}\]</div>
<p>where, parameters <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(w_j\)</span> are all updated simultaniously</p>
<p>Exercise 3</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function to compute <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> from equations (2) and (3) below.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>* m is the number of training examples in the dataset</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the actual label</p></li>
<li><p><strong>Note</strong>: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x)\)</span>.</p></li>
</ul>
<p>As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[67]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C3</span>
<span class="c1"># GRADED FUNCTION: compute_gradient</span>
<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for logistic regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) variable such as house size</span>
<span class="sd">      y : (array_like Shape (m,1)) actual value</span>
<span class="sd">      w : (array_like Shape (n,1)) values of parameters of the model</span>
<span class="sd">      b : (scalar)                 value of parameter of the model</span>
<span class="sd">      lambda_: unused placeholder.</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="n">f_wb_i</span>  <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err_i</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err_i</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="o">/</span><span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>


    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient(X, y, w, b, lambda_=None): m, n = X.shape dj_dw = np.zeros(w.shape) dj_db = 0.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
for i in range(m):
    # Calculate f_wb (exactly as you did in the compute_cost function above)
    f_wb =

    # Calculate the  gradient for b from this example
    dj_db_i = # Your code here to calculate the error

    # add that to dj_db
    dj_db += dj_db_i

    # get dj_dw for each attribute
    for j in range(n):
        # You code here to calculate the gradient from the i-th example for j-th attribute
        dj_dw_ij =
        dj_dw[j] += dj_dw_ij

# divide dj_db and dj_dw by total number of examples
dj_dw = dj_dw / m
dj_db = dj_db / m
### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code>, <code class="docutils literal notranslate"><span class="pre">dj_db_i</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_dw_ij</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate dj_db_i     You can calculate dj_db_i as dj_db_i = f_wb - y[i]</p>
</details><details><p>Hint to calculate dj_dw_ij     You can calculate dj_dw_ij as dj_dw_ij = (f_wb - y[i])* X[i][j]</p>
</details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[68]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display gradient with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_db at initial w (zeros):</span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_dw at initial w (zeros):</span><span class="si">{</span><span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at initial w (zeros):-0.1
dj_dw at initial w (zeros):[-12.00921658929115, -11.262842205513591]
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.1</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-12.00921658929115, -11.262842205513591]</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[69]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost and gradient with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">24</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>  <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_db at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_db</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_dw at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_test</span><span class="p">(</span><span class="n">compute_gradient</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at test_w: -0.5999999999991071
dj_dw at test_w: [-44.831353617873795, -44.37384124953978]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.5999999999991071</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-44.8313536178737957, -44.37384124953978]</p>
</td></tr></table></section>
<section id="2.6-Learning-parameters-using-gradient-descent">
<h2>2.6 Learning parameters using gradient descent<a class="headerlink" href="#2.6-Learning-parameters-using-gradient-descent" title="Permalink to this headline"></a></h2>
<p>Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. - You don’t need to implement anything for this part. Simply run the cells below.</p>
<ul class="simple">
<li><p>A good way to verify that gradient descent is working correctly is to look at the value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> and check that it is decreasing with each step.</p></li>
<li><p>Assuming you have implemented the gradient and computed the cost correctly, your value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> should never increase, and should converge to a steady value by the end of the algorithm.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[70]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      X :    (array_like Shape (m, n)</span>
<span class="sd">      y :    (array_like Shape (m,))</span>
<span class="sd">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span>
<span class="sd">      b_in : (scalar)                 Initial value of parameter of the model</span>
<span class="sd">      cost_function:                  function to compute cost</span>
<span class="sd">      alpha : (float)                 Learning rate</span>
<span class="sd">      num_iters : (int)               number of iterations to run gradient descent</span>
<span class="sd">      lambda_ (scalar, float)         regularization constant</span>

<span class="sd">    Returns:</span>
<span class="sd">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">      b : (scalar)                Updated value of parameter of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w_in</span> <span class="o">=</span> <span class="n">w_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">b_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">cost</span> <span class="o">=</span>  <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_iters</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span> <span class="n">w_history</span> <span class="c1">#return w and J,w history for graphing</span>
</pre></div>
</div>
</div>
<p>Now let’s run the gradient descent algorithm above to learn the parameters for our dataset.</p>
<p><strong>Note</strong></p>
<p>The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[71]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">intial_w</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>


<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span> <span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                   <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div></div>
</div>
<details><p>Expected Output: Cost 0.30, (Click to see details):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># With the following settings
np.random.seed(1)
intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)
initial_b = -8
iterations = 10000
alpha = 0.001
#
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div>
</div>
</section>
<section id="2.7-Plotting-the-decision-boundary">
<h2>2.7 Plotting the decision boundary<a class="headerlink" href="#2.7-Plotting-the-decision-boundary" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line">We will now use the final parameters from gradient descent to plot the linear fit. If you implemented the previous parts correctly, you should see the following plot:</div>
<div class="line"><img alt="b9fb700fcde34c4784bc2fd82c5e3f1c" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure2.png" style="width: 450px; height: 450px;" /></div>
</div>
<p>We will use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to create this plot.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_56_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_56_0.png" />
</div>
</div>
</section>
<section id="2.8-Evaluating-logistic-regression">
<h2>2.8 Evaluating logistic regression<a class="headerlink" href="#2.8-Evaluating-logistic-regression" title="Permalink to this headline"></a></h2>
<p>We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set.</p>
<p>You will implement the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function below to do this.</p>
</section>
<section id="Exercise-4">
<h2>Exercise 4<a class="headerlink" href="#Exercise-4" title="Permalink to this headline"></a></h2>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function to produce <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code> predictions given a dataset and a learned parameter vector <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. - First you need to compute the prediction from the model <span class="math notranslate nohighlight">\(f(x^{(i)}) = g(w \cdot x^{(i)})\)</span> for every example - You’ve implemented this before in the parts above - We interpret the output of the model (<span class="math notranslate nohighlight">\(f(x^{(i)})\)</span>) as the probability that <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span> given <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and parameterized by <span class="math notranslate nohighlight">\(w\)</span>. - Therefore, to get a
final prediction (<span class="math notranslate nohighlight">\(y^{(i)}=0\)</span> or <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>) from the logistic regression model, you can use the following heuristic -</p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &gt;= 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span></p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[73]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C4</span>
<span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict whether the label is 0 or 1 using learned logistic</span>
<span class="sd">    regression parameters w</span>

<span class="sd">    Args:</span>
<span class="sd">    X : (ndarray Shape (m, n))</span>
<span class="sd">    w : (array_like Shape (n,))      Parameters of the model</span>
<span class="sd">    b : (scalar, float)              Parameter of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    p: (ndarray (m,1))</span>
<span class="sd">        The predictions for X using a threshold at 0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of training examples</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Loop over each example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="mi">0</span>

        <span class="c1"># Add bias term</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span>

        <span class="c1"># Calculate the prediction for this example</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_wb</span><span class="p">)</span>

        <span class="c1"># Apply the threshold</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">f_wb</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><div class="line-block">
<div class="line">Here’s how you can structure the overall implementation for this function ```python def predict(X, w, b): # number of training examples m, n = X.shape</div>
<div class="line">p = np.zeros(m)</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
# Loop over each example
for i in range(m):

    # Calculate f_wb (exactly how you did it in the compute_cost function above)
    # using a couple of lines of code
    f_wb =

    # Calculate the prediction for that training example
    p[i] = # Your code here to calculate the prediction based on f_wb

### END CODE HERE ###
return p
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">p[i]</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate p[i]     As an example, if you’d like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as x = y &lt; 3 . Now do the same for p[i] = 1 if f_wb &gt;= 0.5 and 0 otherwise.</p>
<details><p>    More hints to calculate p[i]     You can compute p[i] as p[i] = f_wb &gt;= 0.5</p>
</details></details></li>
</ul>
</details><p>Once you have completed the function <code class="docutils literal notranslate"><span class="pre">predict</span></code>, let’s run the code below to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[74]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test your predict code</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tmp_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tmp_b</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">tmp_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="n">tmp_p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">tmp_X</span><span class="p">,</span> <span class="n">tmp_w</span><span class="p">,</span> <span class="n">tmp_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output of predict: shape </span><span class="si">{</span><span class="n">tmp_p</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, value </span><span class="si">{</span><span class="n">tmp_p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Output of predict: shape (4,), value [0. 1. 1. 1.]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<table><tr><td><p>Output of predict: shape (4,),value [0. 1. 1. 1.]</p>
</td></tr></table><p>Now let’s use this to compute the accuracy on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[75]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on our training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 92.000000
</pre></div></div>
</div>
<table><tr><td><p>Train Accuracy (approx):</p>
</td><td><p>92.00</p>
</td></tr></table></section>
</section>
<section id="3---Regularized-Logistic-Regression">
<h1>3 - Regularized Logistic Regression<a class="headerlink" href="#3---Regularized-Logistic-Regression" title="Permalink to this headline"></a></h1>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<section id="3.1-Problem-Statement">
<h2>3.1 Problem Statement<a class="headerlink" href="#3.1-Problem-Statement" title="Permalink to this headline"></a></h2>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. - From these two tests, you would like to determine whether the microchips should be accepted or rejected. - To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
</section>
<section id="3.2-Loading-and-visualizing-the-data">
<h2>3.2 Loading and visualizing the data<a class="headerlink" href="#3.2-Loading-and-visualizing-the-data" title="Permalink to this headline"></a></h2>
<p>Similar to previous parts of this exercise, let’s start by loading the dataset for this task and visualizing it.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains the test results for the microchips from two tests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span></code> contains the results of the QA</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the microchip was accepted</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the microchip was rejected</p></li>
</ul>
</li>
<li><p>Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p></li>
</ul>
</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>View the variables</p>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and the type of the variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print X_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># print y_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train: [[ 0.05  0.7 ]
 [-0.09  0.68]
 [-0.21  0.69]
 [-0.38  0.5 ]
 [-0.51  0.47]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
y_train: [1. 1. 1. 1. 1.]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of X_train is: (118, 2)
The shape of y_train is: (118,)
We have m = 118 training examples
</pre></div></div>
</div>
<p>Visualize your data</p>
<p>The helper function <code class="docutils literal notranslate"><span class="pre">plot_data</span></code> (from <code class="docutils literal notranslate"><span class="pre">utils.py</span></code>) is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>
<p><img alt="f204ef188fd84ab08741b9c85d08c117" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure3.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[78]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Accepted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Rejected&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 2&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec69572a491941fc866b2028c415a2e7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
</section>
<section id="3.3-Feature-mapping">
<h2>3.3 Feature mapping<a class="headerlink" href="#3.3-Feature-mapping" title="Permalink to this headline"></a></h2>
<p>One way to fit the data better is to create more features from each data point. In the provided function <code class="docutils literal notranslate"><span class="pre">map_feature</span></code>, we will map the features into all polynomial terms of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> up to the sixth power.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{map\_feature}(x) =
\left[\begin{array}{c}
x_1\\
x_2\\
x_1^2\\
x_1 x_2\\
x_2^2\\
x_1^3\\
\vdots\\
x_1 x_2^5\\
x_2^6\end{array}\right]\end{split}\]</div>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.</p>
<ul class="simple">
<li><p>A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.</p></li>
<li><p>We have provided the <code class="docutils literal notranslate"><span class="pre">map_feature</span></code> function for you in utils.py.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[79]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape of data:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mapped_X</span> <span class="o">=</span>  <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape after feature mapping:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Original shape of data: (118, 2)
Shape after feature mapping: (118, 27)
</pre></div></div>
</div>
<p>Let’s also print the first elements of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">mapped_X</span></code> to see the tranformation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[80]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train[0]:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mapped X_train[0]:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train[0]: [0.05 0.7 ]
mapped X_train[0]: [5.13e-02 7.00e-01 2.63e-03 3.59e-02 4.89e-01 1.35e-04 1.84e-03 2.51e-02
 3.42e-01 6.91e-06 9.43e-05 1.29e-03 1.76e-02 2.39e-01 3.54e-07 4.83e-06
 6.59e-05 9.00e-04 1.23e-02 1.68e-01 1.82e-08 2.48e-07 3.38e-06 4.61e-05
 6.29e-04 8.59e-03 1.17e-01]
</pre></div></div>
</div>
<p>While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
</section>
<section id="3.4-Cost-function-for-regularized-logistic-regression">
<h2>3.4 Cost function for regularized logistic regression<a class="headerlink" href="#3.4-Cost-function-for-regularized-logistic-regression" title="Permalink to this headline"></a></h2>
<p>In this part, you will implement the cost function for regularized logistic regression.</p>
<p>Recall that for regularized logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Compare this to the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}.b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]</div>
<p>The difference is the regularization term, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(b\)</span> parameter is not regularized.</p>
<p>Exercise 5</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function below to calculate the following term for each element in <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>The starter code then adds this to the cost without regularization (which you computed above in <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code>) to calculate the cost with regulatization.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[81]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C5</span>
<span class="k">def</span> <span class="nf">compute_cost_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (array_like Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : (array_like Shape (n,)) Values of bias parameter of the model</span>
<span class="sd">      lambda_ : (scalar, float)    Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calls the compute_cost function that you implemented above</span>
    <span class="n">cost_without_reg</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># You need to calculate this value</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Add the regularization cost to get the total cost</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost_without_reg</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_cost_reg(X, y, w, b, lambda_ = 1):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   m, n = X.shape

    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b)

    # You need to calculate this value
    reg_cost = 0.

    ### START CODE HERE ###
    for j in range(n):
        reg_cost_j = # Your code here to calculate the cost from w[j]
        reg_cost = reg_cost + reg_cost_j

    ### END CODE HERE ###

    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

return total_cost
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">reg_cost_j</span></code></p>
<details><p>Hint to calculate reg_cost_j     You can use calculate reg_cost_j as reg_cost_j = w[j]**2</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost :&quot;</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="c1"># UNIT TEST</span>
<span class="n">compute_cost_reg_test</span><span class="p">(</span><span class="n">compute_cost_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 6</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> lambda_ <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-fg">----&gt; 6</span> cost <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">compute_cost_reg</span>(X_mapped, y_train, initial_w, initial_b, lambda_)
<span class="ansi-green-intense-fg ansi-bold">      8</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Regularized cost :</span><span style="color: rgb(175,0,0)">&#34;</span>, cost)
<span class="ansi-green-intense-fg ansi-bold">     10</span> <span style="color: rgb(95,135,135)"># UNIT TEST    </span>

<span class="ansi-red-fg">NameError</span>: name &#39;compute_cost_reg&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost :</p>
</td><td><p>0.6618252552483948</p>
</td></tr></table></section>
<section id="3.5-Gradient-for-regularized-logistic-regression">
<h2>3.5 Gradient for regularized logistic regression<a class="headerlink" href="#3.5-Gradient-for-regularized-logistic-regression" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the gradient for regularized logistic regression.</p>
<p>The gradient of the regularized cost function has two components. The first, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is a scalar, the other is a vector with the same shape as the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, where the <span class="math notranslate nohighlight">\(j^\mathrm{th}\)</span> element is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \left( \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>As you can see,<span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is the same, the difference is the following term in <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Exercise 6</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function below to modify the code below to calculate the following term</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>The starter code will add this term to the <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span> returned from <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> above to get the gradient for the regularized cost function.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[83]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C6</span>
<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n))   variable such as house size</span>
<span class="sd">      y : (ndarray Shape (m,))    actual value</span>
<span class="sd">      w : (ndarray Shape (n,))    values of parameters of the model</span>
<span class="sd">      b : (scalar)                value of parameter of the model</span>
<span class="sd">      lambda_ : (scalar,float)    regularization constant</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient_reg(X, y, w, b, lambda_ = 1): m, n = X.shape</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db, dj_dw = compute_gradient(X, y, w, b)

### START CODE HERE ###
# Loop over the elements of w
for j in range(n):

    dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]

    # Add the regularization term  to the correspoding element of dj_dw
    dj_dw[j] = dj_dw[j] + dj_dw_j_reg

### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">dj_dw_j_reg</span></code></p>
<details><p>Hint to calculate dj_dw_j_reg     You can use calculate dj_dw_j_reg as dj_dw_j_reg = (lambda_ / m) * w[j]</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[84]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First few elements of regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_reg_test</span><span class="p">(</span><span class="n">compute_gradient_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db: 0.07138288792343662
First few elements of regularized dj_dw:
 [-0.010386028450548701, 0.011409852883280122, 0.0536273463274574, 0.003140278267313462]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db:0.07138288792343656</p>
</td></tr><tr><td><p>First few elements of regularized dj_dw:</p>
</td></tr><tr><td><p>[[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]]</p>
</td></tr></table></section>
<section id="3.6-Learning-parameters-using-gradient-descent">
<h2>3.6 Learning parameters using gradient descent<a class="headerlink" href="#3.6-Learning-parameters-using-gradient-descent" title="Permalink to this headline"></a></h2>
<p>Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>. - If you have completed the cost and gradient for regularized logistic regression correctly, you should be able to step through the next cell to learn the parameters <span class="math notranslate nohighlight">\(w\)</span>. - After training our parameters, we will use it to plot the decision boundary.</p>
<p><strong>Note</strong></p>
<p>The code block below takes quite a while to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, run for 100,000 iterations to see better results.</p>
<p>Regularised Gradient Descent</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_7350/3281116716.py:84: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[-9.33e-05  1.40e-03  2.20e-03  4.21e-03  2.22e-03  2.82e-03  5.85e-03
  5.76e-03  1.76e-03  2.81e-03  6.51e-03  6.15e-03  6.38e-03  1.66e-03
  3.68e-03  6.78e-03  6.91e-03  6.66e-03  6.74e-03  1.83e-03  3.54e-03
  7.08e-03  6.94e-03  7.32e-03  6.77e-03  7.03e-03  1.64e-03]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-2.01e-03  4.82e-04  4.12e-04  2.61e-03  1.09e-03  1.44e-03  5.01e-03
  4.80e-03  4.66e-04  1.10e-03  5.78e-03  5.42e-03  5.54e-03  1.32e-04
  2.39e-03  6.16e-03  6.38e-03  6.07e-03  6.03e-03  3.60e-04  2.03e-03
  6.56e-03  6.42e-03  6.87e-03  6.22e-03  6.41e-03  1.51e-05]
[-2.17e-03  4.23e-04  2.28e-04  2.45e-03  9.85e-04  1.32e-03  4.92e-03
  4.70e-03  3.56e-04  9.28e-04  5.70e-03  5.34e-03  5.46e-03 -9.45e-06
  2.27e-03  6.10e-03  6.32e-03  6.01e-03  5.96e-03  2.28e-04  1.88e-03
  6.50e-03  6.36e-03  6.82e-03  6.16e-03  6.34e-03 -1.35e-04]
[-2.31e-03  3.71e-04  4.65e-05  2.29e-03  8.85e-04  1.21e-03  4.83e-03
  4.61e-03  2.51e-04  7.59e-04  5.63e-03  5.27e-03  5.37e-03 -1.47e-04
  2.15e-03  6.03e-03  6.27e-03  5.95e-03  5.89e-03  1.02e-04  1.73e-03
  6.45e-03  6.30e-03  6.78e-03  6.10e-03  6.28e-03 -2.80e-04]
[-2.45e-03  3.27e-04 -1.32e-04  2.14e-03  7.87e-04  1.10e-03  4.75e-03
  4.51e-03  1.52e-04  5.94e-04  5.56e-03  5.19e-03  5.29e-03 -2.79e-04
  2.04e-03  5.97e-03  6.21e-03  5.89e-03  5.81e-03 -1.91e-05  1.59e-03
  6.40e-03  6.25e-03  6.73e-03  6.04e-03  6.22e-03 -4.21e-04]
[-2.58e-03  2.88e-04 -3.08e-04  1.98e-03  6.92e-04  9.95e-04  4.67e-03
  4.42e-03  5.69e-05  4.32e-04  5.49e-03  5.12e-03  5.20e-03 -4.08e-04
  1.94e-03  5.91e-03  6.16e-03  5.83e-03  5.74e-03 -1.35e-04  1.45e-03
  6.35e-03  6.19e-03  6.68e-03  5.99e-03  6.15e-03 -5.57e-04]
[-2.69e-03  2.55e-04 -4.82e-04  1.84e-03  6.00e-04  8.97e-04  4.59e-03
  4.33e-03 -3.31e-05  2.73e-04  5.42e-03  5.04e-03  5.12e-03 -5.33e-04
  1.83e-03  5.84e-03  6.11e-03  5.77e-03  5.67e-03 -2.47e-04  1.31e-03
  6.30e-03  6.14e-03  6.64e-03  5.93e-03  6.09e-03 -6.88e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.    0.    0.01  0.
  0.01 -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-2.90e-03  2.07e-04 -8.22e-04  1.55e-03  4.22e-04  7.17e-04  4.43e-03
  4.16e-03 -2.00e-04 -3.44e-05  5.28e-03  4.90e-03  4.96e-03 -7.72e-04
  1.64e-03  5.72e-03  6.01e-03  5.66e-03  5.53e-03 -4.58e-04  1.05e-03
  6.20e-03  6.03e-03  6.55e-03  5.82e-03  5.97e-03 -9.38e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.22e-03  1.67e-04 -1.47e-03  1.00e-03  8.84e-05  4.13e-04  4.12e-03
  3.83e-03 -4.86e-04 -6.15e-04  5.03e-03  4.61e-03  4.66e-03 -1.21e-03
  1.30e-03  5.49e-03  5.82e-03  5.43e-03  5.26e-03 -8.32e-04  5.58e-04
  6.01e-03  5.82e-03  6.38e-03  5.60e-03  5.73e-03 -1.39e-03]
[-3.28e-03  1.68e-04 -1.63e-03  8.75e-04  9.12e-06  3.48e-04  4.05e-03
  3.75e-03 -5.49e-04 -7.53e-04  4.97e-03  4.54e-03  4.59e-03 -1.31e-03
  1.23e-03  5.43e-03  5.77e-03  5.38e-03  5.20e-03 -9.17e-04  4.42e-04
  5.96e-03  5.77e-03  6.34e-03  5.55e-03  5.68e-03 -1.50e-03]
[-3.33e-03  1.73e-04 -1.79e-03  7.49e-04 -6.88e-05  2.87e-04  3.98e-03
  3.68e-03 -6.09e-04 -8.89e-04  4.91e-03  4.47e-03  4.52e-03 -1.41e-03
  1.16e-03  5.37e-03  5.72e-03  5.33e-03  5.14e-03 -9.99e-04  3.28e-04
  5.92e-03  5.72e-03  6.30e-03  5.50e-03  5.62e-03 -1.60e-03]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.46e-03  2.06e-04 -2.24e-03  3.87e-04 -2.95e-04  1.25e-04  3.77e-03
  3.46e-03 -7.72e-04 -1.28e-03  4.74e-03  4.27e-03  4.32e-03 -1.70e-03
  9.53e-04  5.21e-03  5.59e-03  5.17e-03  4.95e-03 -1.23e-03  2.43e-06
  5.79e-03  5.57e-03  6.18e-03  5.34e-03  5.46e-03 -1.89e-03]
[-3.49e-03  2.24e-04 -2.39e-03  2.71e-04 -3.68e-04  7.81e-05  3.70e-03
  3.39e-03 -8.21e-04 -1.41e-03  4.68e-03  4.20e-03  4.25e-03 -1.79e-03
  8.91e-04  5.16e-03  5.55e-03  5.12e-03  4.89e-03 -1.30e-03 -1.02e-04
  5.74e-03  5.52e-03  6.14e-03  5.29e-03  5.41e-03 -1.99e-03]
[-3.52e-03  2.44e-04 -2.54e-03  1.58e-04 -4.40e-04  3.39e-05  3.63e-03
  3.32e-03 -8.68e-04 -1.53e-03  4.63e-03  4.14e-03  4.19e-03 -1.88e-03
  8.32e-04  5.10e-03  5.51e-03  5.07e-03  4.83e-03 -1.37e-03 -2.04e-04
  5.70e-03  5.47e-03  6.10e-03  5.24e-03  5.36e-03 -2.08e-03]
[-3.54e-03  2.67e-04 -2.68e-03  4.64e-05 -5.11e-04 -7.22e-06  3.57e-03
  3.25e-03 -9.12e-04 -1.65e-03  4.57e-03  4.07e-03  4.12e-03 -1.97e-03
  7.75e-04  5.05e-03  5.46e-03  5.02e-03  4.78e-03 -1.43e-03 -3.04e-04
  5.66e-03  5.42e-03  6.06e-03  5.19e-03  5.31e-03 -2.16e-03]
[-3.55e-03  2.92e-04 -2.82e-03 -6.24e-05 -5.82e-04 -4.56e-05  3.50e-03
  3.19e-03 -9.54e-04 -1.77e-03  4.52e-03  4.01e-03  4.06e-03 -2.05e-03
  7.20e-04  5.00e-03  5.42e-03  4.97e-03  4.72e-03 -1.50e-03 -4.01e-04
  5.62e-03  5.38e-03  6.03e-03  5.14e-03  5.26e-03 -2.25e-03]
[-3.57e-03  3.19e-04 -2.96e-03 -1.69e-04 -6.51e-04 -8.13e-05  3.44e-03
  3.12e-03 -9.94e-04 -1.89e-03  4.47e-03  3.94e-03  4.00e-03 -2.14e-03
  6.67e-04  4.95e-03  5.38e-03  4.92e-03  4.66e-03 -1.56e-03 -4.97e-04
  5.58e-03  5.33e-03  5.99e-03  5.10e-03  5.21e-03 -2.33e-03]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-3.36e-03  9.29e-04 -4.89e-03 -1.54e-03 -1.61e-03 -3.75e-04  2.58e-03
  2.28e-03 -1.41e-03 -3.45e-03  3.79e-03  3.04e-03  3.19e-03 -3.22e-03
  7.68e-05  4.24e-03  4.83e-03  4.24e-03  3.90e-03 -2.28e-03 -1.75e-03
  5.04e-03  4.65e-03  5.47e-03  4.41e-03  4.55e-03 -3.38e-03]
[-3.32e-03  9.78e-04 -5.01e-03 -1.62e-03 -1.67e-03 -3.83e-04  2.53e-03
  2.23e-03 -1.43e-03 -3.54e-03  3.75e-03  2.98e-03  3.15e-03 -3.28e-03
  4.77e-05  4.20e-03  4.80e-03  4.19e-03  3.85e-03 -2.32e-03 -1.82e-03
  5.01e-03  4.61e-03  5.44e-03  4.37e-03  4.51e-03 -3.43e-03]
[-3.29e-03  1.03e-03 -5.13e-03 -1.70e-03 -1.73e-03 -3.89e-04  2.48e-03
  2.18e-03 -1.44e-03 -3.64e-03  3.71e-03  2.93e-03  3.10e-03 -3.34e-03
  1.96e-05  4.16e-03  4.76e-03  4.15e-03  3.81e-03 -2.35e-03 -1.89e-03
  4.98e-03  4.57e-03  5.41e-03  4.33e-03  4.47e-03 -3.49e-03]
[-3.25e-03  1.08e-03 -5.24e-03 -1.78e-03 -1.79e-03 -3.95e-04  2.43e-03
  2.14e-03 -1.46e-03 -3.73e-03  3.67e-03  2.87e-03  3.05e-03 -3.40e-03
 -7.66e-06  4.12e-03  4.73e-03  4.11e-03  3.76e-03 -2.39e-03 -1.96e-03
  4.95e-03  4.53e-03  5.38e-03  4.28e-03  4.43e-03 -3.55e-03]
[-3.22e-03  1.13e-03 -5.35e-03 -1.85e-03 -1.85e-03 -3.99e-04  2.38e-03
  2.09e-03 -1.47e-03 -3.81e-03  3.63e-03  2.82e-03  3.01e-03 -3.47e-03
 -3.40e-05  4.07e-03  4.70e-03  4.07e-03  3.72e-03 -2.42e-03 -2.03e-03
  4.92e-03  4.49e-03  5.35e-03  4.24e-03  4.39e-03 -3.60e-03]
[-3.18e-03  1.18e-03 -5.47e-03 -1.93e-03 -1.90e-03 -4.03e-04  2.33e-03
  2.04e-03 -1.49e-03 -3.90e-03  3.60e-03  2.76e-03  2.96e-03 -3.52e-03
 -5.95e-05  4.03e-03  4.67e-03  4.03e-03  3.68e-03 -2.46e-03 -2.10e-03
  4.89e-03  4.45e-03  5.32e-03  4.20e-03  4.35e-03 -3.66e-03]
[-3.14e-03  1.24e-03 -5.58e-03 -2.00e-03 -1.96e-03 -4.06e-04  2.28e-03
  2.00e-03 -1.50e-03 -3.99e-03  3.56e-03  2.71e-03  2.92e-03 -3.58e-03
 -8.42e-05  3.99e-03  4.63e-03  3.99e-03  3.63e-03 -2.49e-03 -2.17e-03
  4.85e-03  4.41e-03  5.29e-03  4.16e-03  4.32e-03 -3.71e-03]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.34  0.18 -0.73 -0.28 -0.27 -0.05  0.24  0.2  -0.19 -0.53  0.4   0.28
  0.31 -0.46 -0.03  0.44  0.53  0.44  0.4  -0.32 -0.3   0.55  0.49  0.6
  0.46  0.48 -0.47] 0.3499985218344349 260.61872511912435
[[ 5.13e-02  7.00e-01  2.63e-03 ...  6.29e-04  8.59e-03  1.17e-01]
 [-9.27e-02  6.85e-01  8.60e-03 ...  1.89e-03 -1.40e-02  1.03e-01]
 [-2.14e-01  6.92e-01  4.57e-02 ...  1.05e-02 -3.40e-02  1.10e-01]
 ...
 [-4.84e-01  9.99e-01  2.35e-01 ...  2.34e-01 -4.83e-01  9.96e-01]
 [-6.34e-03  9.99e-01  4.01e-05 ...  4.00e-05 -6.31e-03  9.96e-01]
 [ 6.33e-01 -3.06e-02  4.00e-01 ...  3.51e-07 -1.70e-08  8.23e-10]] [0.51 0.53 0.52 0.59 0.58 0.58 0.59 0.58 0.56 0.54 0.48 0.44 0.41 0.35
 0.5  0.53 0.55 0.44 0.52 0.54 0.53 0.56 0.56 0.56 0.56 0.51 0.46 0.36
 0.58 0.46 0.26 0.45 0.54 0.59 0.6  0.6  0.58 0.58 0.57 0.53 0.54 0.48
 0.41 0.5  0.45 0.48 0.46 0.55 0.51 0.51 0.55 0.59 0.57 0.6  0.6  0.6
 0.59 0.57 0.34 0.47 0.53 0.5  0.52 0.51 0.47 0.35 0.37 0.22 0.22 0.26
 0.22 0.24 0.24 0.27 0.37 0.44 0.48 0.49 0.46 0.49 0.52 0.56 0.54 0.56
 0.47 0.53 0.51 0.45 0.51 0.3  0.3  0.49 0.47 0.34 0.23 0.1  0.2  0.47
 0.54 0.54 0.52 0.13 0.54 0.52 0.57 0.55 0.57 0.59 0.55 0.51 0.5  0.5
 0.47 0.5  0.56 0.18 0.24 0.43] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e8b160ad4a674fac9aeeb8e1b5c3bd1d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(27,)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[85]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize fitting parameters</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                    <span class="n">compute_cost_reg</span><span class="p">,</span> <span class="n">compute_gradient_reg</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div></div>
</div>
<details><p>Expected Output: Cost &lt; 0.5 (Click for details)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Using the following settings
#np.random.seed(1)
#initial_w = np.random.rand(X_mapped.shape[1])-0.5
#initial_b = 1.
#lambda_ = 0.01;
#iterations = 10000
#alpha = 0.01
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div>
</div>
</section>
<section id="3.7-Plotting-the-decision-boundary">
<h2>3.7 Plotting the decision boundary<a class="headerlink" href="#3.7-Plotting-the-decision-boundary" title="Permalink to this headline"></a></h2>
<p>To help you visualize the model learned by this classifier, we will use our <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary</span></code> function which plots the (non-linear) decision boundary that separates the positive and negative examples.</p>
<ul class="simple">
<li><p>In the function, we plotted the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then drew a contour plot of where the predictions change from y = 0 to y = 1.</p></li>
<li><p>After learning the parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>, the next step is to plot a decision boundary similar to Figure 4.</p></li>
</ul>
<p><img alt="527b2396f79f41fabc9bfc6240fc6af6" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure4.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[111]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[5.50e-04 9.54e-04 1.55e-03 ... 8.74e-12 1.50e-13 1.42e-15]
 [8.40e-04 1.47e-03 2.42e-03 ... 2.46e-11 4.51e-13 4.57e-15]
 [1.21e-03 2.14e-03 3.55e-03 ... 6.07e-11 1.18e-12 1.28e-14]
 ...
 [6.07e-11 1.68e-10 3.98e-10 ... 4.08e-22 8.97e-24 1.24e-25]
 [9.91e-13 2.80e-12 6.75e-12 ... 2.68e-24 5.64e-26 7.42e-28]
 [8.07e-15 2.33e-14 5.73e-14 ... 8.60e-27 1.72e-28 2.16e-30]] (50, 50)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec69572a491941fc866b2028c415a2e7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[110]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sig</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Credit to dibgerge on Github for this plotting code</span>

    <span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HI&quot;</span><span class="p">)</span>
        <span class="n">plot_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])])</span>
        <span class="n">plot_y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">plot_x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)))</span>

        <span class="c1"># Evaluate z = theta*x over the grid</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)):</span>
                <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

        <span class="c1"># important to transpose z before calling contour</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># Plot z = 0</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="3.8-Evaluating-regularized-logistic-regression-model">
<h2>3.8 Evaluating regularized logistic regression model<a class="headerlink" href="#3.8-Evaluating-regularized-logistic-regression-model" title="Permalink to this headline"></a></h2>
<p>You will use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[87]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on the training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 82.203390
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Train Accuracy:~ 80%</p>
</td></tr></table></section>
<section id="My-Solution">
<h2>My Solution<a class="headerlink" href="#My-Solution" title="Permalink to this headline"></a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
<span class="n">x_train</span><span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">w</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">dcw_reg</span><span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="c1">#print(dcw_reg)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">dcw</span><span class="o">+</span><span class="n">dcw_reg</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="c1">#if i&gt;1:</span>
        <span class="c1">#    if cost_i[i]&gt;cost_i[i-1]:</span>
        <span class="c1">#        alpha/=2</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">50</span>==0:
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The cost is&quot;</span><span class="p">,</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">lam_in</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam_in</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>





<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>



<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">y_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">z_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">y_plot</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_plot</span><span class="p">)):</span>
        <span class="n">z_plot</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">x_plot</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">y_plot</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span><span class="n">y_plot</span><span class="p">,</span><span class="n">z_plot</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="c1">#ax.plot((-bf/wf[0],0),(0,-bf/wf[1]),label=&quot;Predicted model&quot;)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">0.1</span><span class="p">,</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">0.1</span> <span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">0.1</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">z_predict</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">z_predict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z_predict</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">theta_f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 2.0028840493199764
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_6366/2829149365.py:82: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 0.6893248648821225
The cost is 0.5947327181536963
The cost is 0.5594964140290553
The cost is 0.5439204525169686
The cost is 0.5365941656209957
The cost is 0.5329892209020973
The cost is 0.5311518234465994
The cost is 0.5301878305603884
The cost is 0.5296693737410955
The cost is 0.5293843854020753
The cost is 0.5292246537838604
The cost is 0.5291335540683393
The cost is 0.5290807827627604
The cost is 0.5290497884840468
The cost is 0.5290313609587369
The cost is 0.5290202869125287
The cost is 0.5290135693988894
The cost is 0.5290094612865763
The cost is 0.5290069311969307
[ 0.62  1.18 -2.02 -0.92 -1.43  0.13 -0.37 -0.36 -0.17 -1.46 -0.05 -0.61
 -0.27 -1.19 -0.24 -0.2  -0.04 -0.27 -0.29 -0.46 -1.04  0.03 -0.28  0.02
 -0.32 -0.14 -0.93] 1.2715618096527268 0.5290053880660263
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 0.000000
(array([ 0.62,  1.18, -2.02, -0.92, -1.43,  0.13, -0.37, -0.36, -0.17,
       -1.46, -0.05, -0.61, -0.27, -1.19, -0.24, -0.2 , -0.04, -0.27,
       -0.29, -0.46, -1.04,  0.03, -0.28,  0.02, -0.32, -0.14, -0.93]), 1.2715618096527268)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "14a9467b4ad24181b1016046645ee3f1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span><span class="nn">os</span><span class="o">,</span><span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">subprocess</span><span class="o">,</span><span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="n">home_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">())</span>
<span class="n">proj_path</span><span class="o">=</span><span class="n">home_path</span><span class="o">+</span><span class="s2">&quot;/my_web/Machine-Learning-Andrew-Ng/source/source_files/Supervised_Machine_Learning_Regression_and_Classification</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">home_path</span><span class="si">}</span><span class="s2">/week3/C1W3A1/&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</section>
</section>
<section id="id5">
<h1>2 - Logistic Regression<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h1>
<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p>
<section id="id6">
<h2>2.1 Problem Statement<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<p>Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. * You have historical data from previous applicants that you can use as a training set for logistic regression. * For each training example, you have the applicant’s scores on two exams and the admissions decision. * Your task is to build a classification model that estimates an applicant’s probability of admission based on the
scores from those two exams.</p>
</section>
<section id="id7">
<h2>2.2 Loading and visualizing the data<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>You will start by loading the dataset for this task. - The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> - <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains exam scores on two exams for a student - <code class="docutils literal notranslate"><span class="pre">y_train</span></code> is the admission decision - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the student was admitted - <code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the student was not admitted - Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data1.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>View the variables</p>
<div class="line-block">
<div class="line">Let’s get more familiar with your dataset.</div>
<div class="line">- A good place to start is to just print out each variable and see what it contains.</div>
</div>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and the type of the variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[57]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in X_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in X_train are:
 [[34.62 78.02]
 [30.29 43.89]
 [35.85 72.9 ]
 [60.18 86.31]
 [79.03 75.34]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Now print the first five values of <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First five elements in y_train are:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First five elements in y_train are:
 [0. 0. 0. 1. 1.]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[59]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape</span>
<span class="n">of</span> <span class="n">y_train</span> <span class="ow">is</span><span class="p">:</span> <span class="s1">&#39; + str(y_train.shape))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-cyan-fg">  Cell </span><span class="ansi-green-fg">In[59], line 2</span>
<span class="ansi-red-fg">    print (&#39;The shape</span>
           ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> unterminated string literal (detected at line 2)

</pre></div></div>
</div>
<p>Visualize your data</p>
<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible. - The code below displays the data on a 2D plot (as shown below), where the axes are the two exam scores, and the positive and negative examples are shown with different markers. - We use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to generate this plot.</p>
<p><img alt="ba167ecc8e824108aa9d2a1063b60363" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure1.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Admitted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Not admitted&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_120_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_120_0.png" />
</div>
</div>
<p>Your goal is to build a logistic regression model to fit this data. - With this model, you can then predict if a new student will be admitted based on their scores on the two exams.</p>
</section>
<section id="id8">
<h2>2.3 Sigmoid function<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h2>
<p>Recall that for logistic regression, the model is represented as</p>
<div class="math notranslate nohighlight">
\[f_{\mathbf{w},b}(x) = g(\mathbf{w}\cdot \mathbf{x} + b)\]</div>
<p>where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. The sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Let’s implement the sigmoid function first, so it can be used by the rest of this assignment.</p>
<p>Exercise 1</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function to calculate</p>
<div class="math notranslate nohighlight">
\[g(z) = \frac{1}{1+e^{-z}}\]</div>
<p>Note that - <code class="docutils literal notranslate"><span class="pre">z</span></code> is not always a single number, but can also be an array of numbers. - If the input is an array of numbers, we’d like to apply the sigmoid function to each value in the input array.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[61]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C1</span>
<span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Args:</span>
<span class="sd">        z (ndarray): A scalar, numpy array of any size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        g (ndarray): sigmoid(z), with the same shape as z</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    <span class="c1">### END SOLUTION ###</span>

    <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<p><code class="docutils literal notranslate"><span class="pre">numpy</span></code> has a function called <code class="docutils literal notranslate"><span class="pre">`np.exp()</span></code> &lt;<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">https://numpy.org/doc/stable/reference/generated/numpy.exp.html</a>&gt;`__, which offers a convinient way to calculate the exponential ( <span class="math notranslate nohighlight">\(e^{z}\)</span>) of all elements in the input array (<code class="docutils literal notranslate"><span class="pre">z</span></code>).</p>
<details><p>Click for more hints</p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">np.exp(-z)</span></code></p>
<ul>
<li><p>You can translate <span class="math notranslate nohighlight">\(1/e^{-z}\)</span> into code as <code class="docutils literal notranslate"><span class="pre">1/np.exp(-z)</span></code></p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">g</span></code></p>
<details><p>Hint to calculate g g = 1 / (1 + np.exp(-z))</p>
</details></li>
</ul>
</li>
</ul>
</details><p>When you are finished, try testing a few values by calling <code class="docutils literal notranslate"><span class="pre">sigmoid(x)</span></code> in the cell below. - For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. - Evaluating <code class="docutils literal notranslate"><span class="pre">sigmoid(0)</span></code> should give you exactly 0.5.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[62]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid(0) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid(0) = 0.5
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid(0)</p>
</td><td><p>0.5</p>
</td></tr></table><ul class="simple">
<li><p>As mentioned before, your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[63]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid([ -1, 0, 1, 2]) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))))</span>

<span class="c1"># UNIT TESTS</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
sigmoid([ -1, 0, 1, 2]) = [0.27 0.5  0.73 0.88]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>sigmoid([-1, 0, 1, 2])</p>
</td><td><p>[0.26894142 0.5 0.73105858 0.88079708]</p>
</td></tr></table></section>
<section id="id9">
<h2>2.4 Cost function for logistic regression<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the cost function for logistic regression.</p>
<p>Exercise 2</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function using the equations below.</p>
<p>Recall that for logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) \right] \tag{1}\]</div>
<p>where * m is the number of training examples in the dataset</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})\)</span> is the cost for a single data point, which is -</p>
<div class="math notranslate nohighlight">
\[loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \tag{2}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, which is the actual label</p></li>
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x^{(i)}} + b)\)</span> where function <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function.</p>
<ul class="simple">
<li><p>It might be helpful to first calculate an intermediate variable <span class="math notranslate nohighlight">\(z_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is the number of features, before calculating <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span></p></li>
</ul>
</li>
</ul>
<p>Note: * As you are doing this, remember that the variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are not scalar values but matrices of shape (<span class="math notranslate nohighlight">\(m, n\)</span>) and (<span class="math notranslate nohighlight">\(𝑚\)</span>,1) respectively, where <span class="math notranslate nohighlight">\(𝑛\)</span> is the number of features and <span class="math notranslate nohighlight">\(𝑚\)</span> is the number of training examples. * You can use the sigmoid function that you implemented above for this part.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[64]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C2</span>
<span class="c1"># GRADED FUNCTION: compute_cost</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : scalar Values of bias parameter of the model</span>
<span class="sd">      lambda_: unused placeholder</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">+=</span> <span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f_wb</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">f_wb</span><span class="p">)</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>You can represent a summation operator eg: <span class="math notranslate nohighlight">\(h = \sum\limits_{i = 0}^{m-1} 2i\)</span> in code as follows: <code class="docutils literal notranslate"><span class="pre">python</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">0</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(m):</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">h</span> <span class="pre">=</span> <span class="pre">h</span> <span class="pre">+</span> <span class="pre">2*i</span></code></p>
<ul class="simple">
<li><p>In this case, you can iterate over all the examples in <code class="docutils literal notranslate"><span class="pre">X</span></code> using a for loop and add the <code class="docutils literal notranslate"><span class="pre">loss</span></code> from each iteration to a variable (<code class="docutils literal notranslate"><span class="pre">loss_sum</span></code>) initialized outside the loop.</p></li>
<li><p>Then, you can return the <code class="docutils literal notranslate"><span class="pre">total_cost</span></code> as <code class="docutils literal notranslate"><span class="pre">loss_sum</span></code> divided by <code class="docutils literal notranslate"><span class="pre">m</span></code>.</p></li>
</ul>
<details><p>Click for more hints</p>
<ul class="simple">
<li><p>Here’s how you can structure the overall implementation for this function</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Loop over each training example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>

        <span class="c1"># First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb_ij</span> <span class="o">=</span> <span class="c1"># Your code here to calculate w[j] * X[i][j]</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">z_wb_ij</span> <span class="c1"># equivalent to z_wb = z_wb + z_wb_ij</span>
        <span class="c1"># Add the bias term to z_wb</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span> <span class="c1"># equivalent to z_wb = z_wb + b</span>

        <span class="n">f_wb</span> <span class="o">=</span> <span class="c1"># Your code here to calculate prediction f_wb for a training example</span>
        <span class="n">loss</span> <span class="o">=</span>  <span class="c1"># Your code here to calculate loss for a training example</span>

        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># equivalent to loss_sum = loss_sum + loss</span>

    <span class="n">total_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">loss_sum</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">z_wb_ij</span></code>, <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">cost</span></code>.</p>
<details><p>Hint to calculate z_wb_ij     z_wb_ij = w[j]*X[i][j]</p>
</details><details><p>Hint to calculate f_wb     <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(z_{\mathbf{w},b}(\mathbf{x}^{(i)}))\)</span> where <span class="math notranslate nohighlight">\(g\)</span> is the sigmoid function. You can simply call the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function implemented above.</p>
<details><p>    More hints to calculate f     You can compute f_wb as f_wb = sigmoid(z_wb)</p>
</details></details><details><p>Hint to calculate loss     You can use the np.log function to calculate the log</p>
<details><p>    More hints to calculate loss     You can compute loss as loss = -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</p>
</details></details></details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[65]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Compute and display cost with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at initial w (zeros): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at initial w (zeros): 0.693
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at initial w (zeros)</p>
</td><td><p>0.693</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">24.</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cost at test w,b: </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>


<span class="c1"># UNIT TESTS</span>
<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cost at test w,b: 0.218
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Cost at test w,b</p>
</td><td><p>0.218</p>
</td></tr></table></section>
<section id="id10">
<h2>2.5 Gradient for logistic regression<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the gradient for logistic regression.</p>
<p>Recall that the gradient descent algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; b := b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \newline       \; &amp; w_j := w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; &amp; \text{for j := 0..n-1}\newline &amp; \rbrace\end{align*}\]</div>
<p>where, parameters <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(w_j\)</span> are all updated simultaniously</p>
<p>Exercise 3</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function to compute <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> from equations (2) and (3) below.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>* m is the number of training examples in the dataset</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x^{(i)})\)</span> is the model’s prediction, while <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the actual label</p></li>
<li><p><strong>Note</strong>: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of <span class="math notranslate nohighlight">\(f_{\mathbf{w},b}(x)\)</span>.</p></li>
</ul>
<p>As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[67]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C3</span>
<span class="c1"># GRADED FUNCTION: compute_gradient</span>
<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for logistic regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n)) variable such as house size</span>
<span class="sd">      y : (array_like Shape (m,1)) actual value</span>
<span class="sd">      w : (array_like Shape (n,1)) values of parameters of the model</span>
<span class="sd">      b : (scalar)                 value of parameter of the model</span>
<span class="sd">      lambda_: unused placeholder.</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.</span>
<span class="sd">      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">f_wb_i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">err_i</span>  <span class="o">=</span> <span class="n">f_wb_i</span>  <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">err_i</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
        <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span> <span class="o">+</span> <span class="n">err_i</span>
    <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="o">/</span><span class="n">m</span>
    <span class="n">dj_db</span> <span class="o">=</span> <span class="n">dj_db</span><span class="o">/</span><span class="n">m</span>

    <span class="c1">### END CODE HERE ###</span>


    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient(X, y, w, b, lambda_=None): m, n = X.shape dj_dw = np.zeros(w.shape) dj_db = 0.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
for i in range(m):
    # Calculate f_wb (exactly as you did in the compute_cost function above)
    f_wb =

    # Calculate the  gradient for b from this example
    dj_db_i = # Your code here to calculate the error

    # add that to dj_db
    dj_db += dj_db_i

    # get dj_dw for each attribute
    for j in range(n):
        # You code here to calculate the gradient from the i-th example for j-th attribute
        dj_dw_ij =
        dj_dw[j] += dj_dw_ij

# divide dj_db and dj_dw by total number of examples
dj_dw = dj_dw / m
dj_db = dj_db / m
### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code>, <code class="docutils literal notranslate"><span class="pre">dj_db_i</span></code> and <code class="docutils literal notranslate"><span class="pre">dj_dw_ij</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate dj_db_i     You can calculate dj_db_i as dj_db_i = f_wb - y[i]</p>
</details><details><p>Hint to calculate dj_dw_ij     You can calculate dj_dw_ij as dj_dw_ij = (f_wb - y[i])* X[i][j]</p>
</details></li>
</ul>
</details><p>Run the cells below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> function with two different initializations of the parameters <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[68]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display gradient with w initialized to zeroes</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_db at initial w (zeros):</span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dj_dw at initial w (zeros):</span><span class="si">{</span><span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at initial w (zeros):-0.1
dj_dw at initial w (zeros):[-12.00921658929115, -11.262842205513591]
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.1</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-12.00921658929115, -11.262842205513591]</p>
</td></tr></table><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[69]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute and display cost and gradient with non-zero w</span>
<span class="n">test_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">test_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">24</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>  <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_w</span><span class="p">,</span> <span class="n">test_b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_db at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_db</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dj_dw at test_w:&#39;</span><span class="p">,</span> <span class="n">dj_dw</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_test</span><span class="p">(</span><span class="n">compute_gradient</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db at test_w: -0.5999999999991071
dj_dw at test_w: [-44.831353617873795, -44.37384124953978]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db at initial w (zeros)</p>
</td><td><p>-0.5999999999991071</p>
</td></tr><tr><td><p>ddj_dw at initial w (zeros):</p>
</td><td><p>[-44.8313536178737957, -44.37384124953978]</p>
</td></tr></table></section>
<section id="id11">
<h2>2.6 Learning parameters using gradient descent<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h2>
<p>Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. - You don’t need to implement anything for this part. Simply run the cells below.</p>
<ul class="simple">
<li><p>A good way to verify that gradient descent is working correctly is to look at the value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> and check that it is decreasing with each step.</p></li>
<li><p>Assuming you have implemented the gradient and computed the cost correctly, your value of <span class="math notranslate nohighlight">\(J(\mathbf{w},b)\)</span> should never increase, and should converge to a steady value by the end of the algorithm.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[70]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">,</span> <span class="n">gradient_function</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs batch gradient descent to learn theta. Updates theta by taking</span>
<span class="sd">    num_iters gradient steps with learning rate alpha</span>

<span class="sd">    Args:</span>
<span class="sd">      X :    (array_like Shape (m, n)</span>
<span class="sd">      y :    (array_like Shape (m,))</span>
<span class="sd">      w_in : (array_like Shape (n,))  Initial values of parameters of the model</span>
<span class="sd">      b_in : (scalar)                 Initial value of parameter of the model</span>
<span class="sd">      cost_function:                  function to compute cost</span>
<span class="sd">      alpha : (float)                 Learning rate</span>
<span class="sd">      num_iters : (int)               number of iterations to run gradient descent</span>
<span class="sd">      lambda_ (scalar, float)         regularization constant</span>

<span class="sd">    Returns:</span>
<span class="sd">      w : (array_like Shape (n,)) Updated values of parameters of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">      b : (scalar)                Updated value of parameter of the model after</span>
<span class="sd">          running gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># number of training examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># An array to store cost J and w&#39;s at each iteration primarily for graphing later</span>
    <span class="n">J_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>

        <span class="c1"># Calculate the gradient and update the parameters</span>
        <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

        <span class="c1"># Update Parameters using w, b, alpha and gradient</span>
        <span class="n">w_in</span> <span class="o">=</span> <span class="n">w_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_dw</span>
        <span class="n">b_in</span> <span class="o">=</span> <span class="n">b_in</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dj_db</span>

        <span class="c1"># Save cost J at each iteration</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">100000</span><span class="p">:</span>      <span class="c1"># prevent resource exhaustion</span>
            <span class="n">cost</span> <span class="o">=</span>  <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
            <span class="n">J_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

        <span class="c1"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">num_iters</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_iters</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_in</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">4</span><span class="si">}</span><span class="s2">: Cost </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">J_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">w_in</span><span class="p">,</span> <span class="n">b_in</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span> <span class="n">w_history</span> <span class="c1">#return w and J,w history for graphing</span>
</pre></div>
</div>
</div>
<p>Now let’s run the gradient descent algorithm above to learn the parameters for our dataset.</p>
<p><strong>Note</strong></p>
<p>The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[71]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">intial_w</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span>


<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span> <span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                   <span class="n">compute_cost</span><span class="p">,</span> <span class="n">compute_gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div></div>
</div>
<details><p>Expected Output: Cost 0.30, (Click to see details):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># With the following settings
np.random.seed(1)
intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)
initial_b = -8
iterations = 10000
alpha = 0.001
#
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Iteration    0: Cost     1.01
Iteration 1000: Cost     0.31
Iteration 2000: Cost     0.30
Iteration 3000: Cost     0.30
Iteration 4000: Cost     0.30
Iteration 5000: Cost     0.30
Iteration 6000: Cost     0.30
Iteration 7000: Cost     0.30
Iteration 8000: Cost     0.30
Iteration 9000: Cost     0.30
Iteration 9999: Cost     0.30
</pre></div>
</div>
</section>
<section id="id12">
<h2>2.7 Plotting the decision boundary<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line">We will now use the final parameters from gradient descent to plot the linear fit. If you implemented the previous parts correctly, you should see the following plot:</div>
<div class="line"><img alt="eab96bc805764952b70c087a1834f421" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure2.png" style="width: 450px; height: 450px;" /></div>
</div>
<p>We will use a helper function in the <code class="docutils literal notranslate"><span class="pre">utils.py</span></code> file to create this plot.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_153_0.png" src="../../_images/source_files_Supervised_Machine_Learning_Regression_and_Classification_Practice_153_0.png" />
</div>
</div>
</section>
<section id="id13">
<h2>2.8 Evaluating logistic regression<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h2>
<p>We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set.</p>
<p>You will implement the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function below to do this.</p>
</section>
<section id="id14">
<h2>Exercise 4<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h2>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function to produce <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code> predictions given a dataset and a learned parameter vector <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. - First you need to compute the prediction from the model <span class="math notranslate nohighlight">\(f(x^{(i)}) = g(w \cdot x^{(i)})\)</span> for every example - You’ve implemented this before in the parts above - We interpret the output of the model (<span class="math notranslate nohighlight">\(f(x^{(i)})\)</span>) as the probability that <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span> given <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and parameterized by <span class="math notranslate nohighlight">\(w\)</span>. - Therefore, to get a
final prediction (<span class="math notranslate nohighlight">\(y^{(i)}=0\)</span> or <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span>) from the logistic regression model, you can use the following heuristic -</p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &gt;= 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=1\)</span></p>
<p>if <span class="math notranslate nohighlight">\(f(x^{(i)}) &lt; 0.5\)</span>, predict <span class="math notranslate nohighlight">\(y^{(i)}=0\)</span></p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[73]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C4</span>
<span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict whether the label is 0 or 1 using learned logistic</span>
<span class="sd">    regression parameters w</span>

<span class="sd">    Args:</span>
<span class="sd">    X : (ndarray Shape (m, n))</span>
<span class="sd">    w : (array_like Shape (n,))      Parameters of the model</span>
<span class="sd">    b : (scalar, float)              Parameter of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    p: (ndarray (m,1))</span>
<span class="sd">        The predictions for X using a threshold at 0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of training examples</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="c1"># Loop over each example</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">z_wb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">w</span><span class="p">)</span>
        <span class="c1"># Loop over each feature</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="c1"># Add the corresponding term to z_wb</span>
            <span class="n">z_wb</span> <span class="o">+=</span> <span class="mi">0</span>

        <span class="c1"># Add bias term</span>
        <span class="n">z_wb</span> <span class="o">+=</span> <span class="n">b</span>

        <span class="c1"># Calculate the prediction for this example</span>
        <span class="n">f_wb</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z_wb</span><span class="p">)</span>

        <span class="c1"># Apply the threshold</span>
        <span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">f_wb</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><div class="line-block">
<div class="line">Here’s how you can structure the overall implementation for this function ```python def predict(X, w, b): # number of training examples m, n = X.shape</div>
<div class="line">p = np.zeros(m)</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>### START CODE HERE ###
# Loop over each example
for i in range(m):

    # Calculate f_wb (exactly how you did it in the compute_cost function above)
    # using a couple of lines of code
    f_wb =

    # Calculate the prediction for that training example
    p[i] = # Your code here to calculate the prediction based on f_wb

### END CODE HERE ###
return p
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">f_wb</span></code> and <code class="docutils literal notranslate"><span class="pre">p[i]</span></code></p>
<details><p>Hint to calculate f_wb     Recall that you calculated f_wb in compute_cost above — for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise</p>
<details><p>    More hints to calculate f_wb     You can calculate f_wb as</p>
<pre><div class="line-block">
<div class="line">for i in range(m):</div>
<div class="line"># Calculate f_wb (exactly how you did it in the compute_cost function above) z_wb = 0 # Loop over each feature for j in range(n): # Add the corresponding term to z_wb z_wb_ij = X[i, j] * w[j] z_wb += z_wb_ij</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Add bias term
             z_wb += b

             # Calculate the prediction from the model
             f_wb = sigmoid(z_wb)
</pre></div>
</div>
</details></details><details><p>Hint to calculate p[i]     As an example, if you’d like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as x = y &lt; 3 . Now do the same for p[i] = 1 if f_wb &gt;= 0.5 and 0 otherwise.</p>
<details><p>    More hints to calculate p[i]     You can compute p[i] as p[i] = f_wb &gt;= 0.5</p>
</details></details></li>
</ul>
</details><p>Once you have completed the function <code class="docutils literal notranslate"><span class="pre">predict</span></code>, let’s run the code below to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[74]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test your predict code</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tmp_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tmp_b</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">tmp_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="n">tmp_p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">tmp_X</span><span class="p">,</span> <span class="n">tmp_w</span><span class="p">,</span> <span class="n">tmp_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Output of predict: shape </span><span class="si">{</span><span class="n">tmp_p</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, value </span><span class="si">{</span><span class="n">tmp_p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Output of predict: shape (4,), value [0. 1. 1. 1.]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected output</strong></p>
<table><tr><td><p>Output of predict: shape (4,),value [0. 1. 1. 1.]</p>
</td></tr></table><p>Now let’s use this to compute the accuracy on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[75]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on our training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 92.000000
</pre></div></div>
</div>
<table><tr><td><p>Train Accuracy (approx):</p>
</td><td><p>92.00</p>
</td></tr></table></section>
</section>
<section id="id15">
<h1>3 - Regularized Logistic Regression<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h1>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<section id="id16">
<h2>3.1 Problem Statement<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h2>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. - From these two tests, you would like to determine whether the microchips should be accepted or rejected. - To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
</section>
<section id="id17">
<h2>3.2 Loading and visualizing the data<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h2>
<p>Similar to previous parts of this exercise, let’s start by loading the dataset for this task and visualizing it.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">load_dataset()</span></code> function shown below loads the data into variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">X_train</span></code> contains the test results for the microchips from two tests</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span></code> contains the results of the QA</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">1</span></code> if the microchip was accepted</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span> <span class="pre">=</span> <span class="pre">0</span></code> if the microchip was rejected</p></li>
</ul>
</li>
<li><p>Both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> are numpy arrays.</p></li>
</ul>
</li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>View the variables</p>
<p>The code below prints the first five values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and the type of the variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print X_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of X_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># print y_train</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Type of y_train:&quot;</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train: [[ 0.05  0.7 ]
 [-0.09  0.68]
 [-0.21  0.69]
 [-0.38  0.5 ]
 [-0.51  0.47]]
Type of X_train: &lt;class &#39;numpy.ndarray&#39;&gt;
y_train: [1. 1. 1. 1. 1.]
Type of y_train: &lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div></div>
</div>
<p>Check the dimensions of your variables</p>
<p>Another useful way to get familiar with your data is to view its dimensions. Let’s print the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and see how many training examples we have in our dataset.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of y_train is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;We have m = </span><span class="si">%d</span><span class="s1"> training examples&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of X_train is: (118, 2)
The shape of y_train is: (118,)
We have m = 118 training examples
</pre></div></div>
</div>
<p>Visualize your data</p>
<p>The helper function <code class="docutils literal notranslate"><span class="pre">plot_data</span></code> (from <code class="docutils literal notranslate"><span class="pre">utils.py</span></code>) is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>
<p><img alt="fc87af0f31a24fd897e7970d815892f4" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure3.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[78]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot examples</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:],</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;Accepted&quot;</span><span class="p">,</span> <span class="n">neg_label</span><span class="o">=</span><span class="s2">&quot;Rejected&quot;</span><span class="p">)</span>

<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 2&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Microchip Test 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec69572a491941fc866b2028c415a2e7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p>
</section>
<section id="id18">
<h2>3.3 Feature mapping<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h2>
<p>One way to fit the data better is to create more features from each data point. In the provided function <code class="docutils literal notranslate"><span class="pre">map_feature</span></code>, we will map the features into all polynomial terms of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> up to the sixth power.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{map\_feature}(x) =
\left[\begin{array}{c}
x_1\\
x_2\\
x_1^2\\
x_1 x_2\\
x_2^2\\
x_1^3\\
\vdots\\
x_1 x_2^5\\
x_2^6\end{array}\right]\end{split}\]</div>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.</p>
<ul class="simple">
<li><p>A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.</p></li>
<li><p>We have provided the <code class="docutils literal notranslate"><span class="pre">map_feature</span></code> function for you in utils.py.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[79]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original shape of data:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">mapped_X</span> <span class="o">=</span>  <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape after feature mapping:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Original shape of data: (118, 2)
Shape after feature mapping: (118, 27)
</pre></div></div>
</div>
<p>Let’s also print the first elements of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">mapped_X</span></code> to see the tranformation.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[80]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train[0]:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mapped X_train[0]:&quot;</span><span class="p">,</span> <span class="n">mapped_X</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
X_train[0]: [0.05 0.7 ]
mapped X_train[0]: [5.13e-02 7.00e-01 2.63e-03 3.59e-02 4.89e-01 1.35e-04 1.84e-03 2.51e-02
 3.42e-01 6.91e-06 9.43e-05 1.29e-03 1.76e-02 2.39e-01 3.54e-07 4.83e-06
 6.59e-05 9.00e-04 1.23e-02 1.68e-01 1.82e-08 2.48e-07 3.38e-06 4.61e-05
 6.29e-04 8.59e-03 1.17e-01]
</pre></div></div>
</div>
<p>While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
</section>
<section id="id19">
<h2>3.4 Cost function for regularized logistic regression<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h2>
<p>In this part, you will implement the cost function for regularized logistic regression.</p>
<p>Recall that for regularized logistic regression, the cost function is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Compare this to the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}.b) = \frac{1}{m}\sum_{i=0}^{m-1} \left[ (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\right]\]</div>
<p>The difference is the regularization term, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(b\)</span> parameter is not regularized.</p>
<p>Exercise 5</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function below to calculate the following term for each element in <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2\]</div>
<p>The starter code then adds this to the cost without regularization (which you computed above in <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code>) to calculate the cost with regulatization.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[81]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C5</span>
<span class="k">def</span> <span class="nf">compute_cost_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cost over all examples</span>
<span class="sd">    Args:</span>
<span class="sd">      X : (array_like Shape (m,n)) data, m examples by n features</span>
<span class="sd">      y : (array_like Shape (m,)) target value</span>
<span class="sd">      w : (array_like Shape (n,)) Values of parameters of the model</span>
<span class="sd">      b : (array_like Shape (n,)) Values of bias parameter of the model</span>
<span class="sd">      lambda_ : (scalar, float)    Controls amount of regularization</span>
<span class="sd">    Returns:</span>
<span class="sd">      total_cost: (scalar)         cost</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calls the compute_cost function that you implemented above</span>
    <span class="n">cost_without_reg</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1"># You need to calculate this value</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="n">reg_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Add the regularization cost to get the total cost</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">cost_without_reg</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">reg_cost</span>

    <span class="k">return</span> <span class="n">total_cost</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_cost_reg(X, y, w, b, lambda_ = 1):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   m, n = X.shape

    # Calls the compute_cost function that you implemented above
    cost_without_reg = compute_cost(X, y, w, b)

    # You need to calculate this value
    reg_cost = 0.

    ### START CODE HERE ###
    for j in range(n):
        reg_cost_j = # Your code here to calculate the cost from w[j]
        reg_cost = reg_cost + reg_cost_j

    ### END CODE HERE ###

    # Add the regularization cost to get the total cost
    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost

return total_cost
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">reg_cost_j</span></code></p>
<details><p>Hint to calculate reg_cost_j     You can use calculate reg_cost_j as reg_cost_j = w[j]**2</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_cost_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regularized cost :&quot;</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>

<span class="c1"># UNIT TEST</span>
<span class="n">compute_cost_reg_test</span><span class="p">(</span><span class="n">compute_cost_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 6</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> initial_b <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> lambda_ <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">0.5</span>
<span class="ansi-green-fg">----&gt; 6</span> cost <span style="color: rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">compute_cost_reg</span>(X_mapped, y_train, initial_w, initial_b, lambda_)
<span class="ansi-green-intense-fg ansi-bold">      8</span> <span style="color: rgb(0,135,0)">print</span>(<span style="color: rgb(175,0,0)">&#34;</span><span style="color: rgb(175,0,0)">Regularized cost :</span><span style="color: rgb(175,0,0)">&#34;</span>, cost)
<span class="ansi-green-intense-fg ansi-bold">     10</span> <span style="color: rgb(95,135,135)"># UNIT TEST    </span>

<span class="ansi-red-fg">NameError</span>: name &#39;compute_cost_reg&#39; is not defined
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Regularized cost :</p>
</td><td><p>0.6618252552483948</p>
</td></tr></table></section>
<section id="id20">
<h2>3.5 Gradient for regularized logistic regression<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h2>
<p>In this section, you will implement the gradient for regularized logistic regression.</p>
<p>The gradient of the regularized cost function has two components. The first, <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is a scalar, the other is a vector with the same shape as the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, where the <span class="math notranslate nohighlight">\(j^\mathrm{th}\)</span> element is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b} = \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j} = \left( \frac{1}{m}  \sum_{i=0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form</p>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial b}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)}) \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial J(\mathbf{w},b)}{\partial w_j}  = \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - \mathbf{y}^{(i)})x_{j}^{(i)} \tag{3}\]</div>
<p>As you can see,<span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial b}\)</span> is the same, the difference is the following term in <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span>, which is</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>Exercise 6</p>
<p>Please complete the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function below to modify the code below to calculate the following term</p>
<div class="math notranslate nohighlight">
\[\frac{\lambda}{m} w_j  \quad\, \mbox{for $j=0...(n-1)$}\]</div>
<p>The starter code will add this term to the <span class="math notranslate nohighlight">\(\frac{\partial J(\mathbf{w},b)}{\partial w}\)</span> returned from <code class="docutils literal notranslate"><span class="pre">compute_gradient</span></code> above to get the gradient for the regularized cost function.</p>
<p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[83]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNQ_C6</span>
<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient for linear regression</span>

<span class="sd">    Args:</span>
<span class="sd">      X : (ndarray Shape (m,n))   variable such as house size</span>
<span class="sd">      y : (ndarray Shape (m,))    actual value</span>
<span class="sd">      w : (ndarray Shape (n,))    values of parameters of the model</span>
<span class="sd">      b : (scalar)                value of parameter of the model</span>
<span class="sd">      lambda_ : (scalar,float)    regularization constant</span>
<span class="sd">    Returns</span>
<span class="sd">      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.</span>
<span class="sd">      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="c1">### START CODE HERE ###</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dj_dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda_</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span>
</pre></div>
</div>
</div>
<details><p>Click for hints</p>
<ul>
<li><p>Here’s how you can structure the overall implementation for this function ```python def compute_gradient_reg(X, y, w, b, lambda_ = 1): m, n = X.shape</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dj_db, dj_dw = compute_gradient(X, y, w, b)

### START CODE HERE ###
# Loop over the elements of w
for j in range(n):

    dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]

    # Add the regularization term  to the correspoding element of dj_dw
    dj_dw[j] = dj_dw[j] + dj_dw_j_reg

### END CODE HERE ###

return dj_db, dj_dw
</pre></div>
</div>
<p>```</p>
<p>If you’re still stuck, you can check the hints presented below to figure out how to calculate <code class="docutils literal notranslate"><span class="pre">dj_dw_j_reg</span></code></p>
<details><p>Hint to calculate dj_dw_j_reg     You can use calculate dj_dw_j_reg as dj_dw_j_reg = (lambda_ / m) * w[j]</p>
</details></details></li>
</ul>
</details><p>Run the cell below to check your implementation of the <code class="docutils literal notranslate"><span class="pre">compute_gradient_reg</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[84]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_mapped</span> <span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">dj_db</span><span class="p">,</span> <span class="n">dj_dw</span> <span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dj_db: </span><span class="si">{</span><span class="n">dj_db</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First few elements of regularized dj_dw:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">dj_dw</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">)</span>

<span class="c1"># UNIT TESTS</span>
<span class="n">compute_gradient_reg_test</span><span class="p">(</span><span class="n">compute_gradient_reg</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
dj_db: 0.07138288792343662
First few elements of regularized dj_dw:
 [-0.010386028450548701, 0.011409852883280122, 0.0536273463274574, 0.003140278267313462]
<span class="ansi-green-intense-fg">All tests passed!</span>
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>dj_db:0.07138288792343656</p>
</td></tr><tr><td><p>First few elements of regularized dj_dw:</p>
</td></tr><tr><td><p>[[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]]</p>
</td></tr></table></section>
<section id="id21">
<h2>3.6 Learning parameters using gradient descent<a class="headerlink" href="#id21" title="Permalink to this headline"></a></h2>
<p>Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>. - If you have completed the cost and gradient for regularized logistic regression correctly, you should be able to step through the next cell to learn the parameters <span class="math notranslate nohighlight">\(w\)</span>. - After training our parameters, we will use it to plot the decision boundary.</p>
<p><strong>Note</strong></p>
<p>The code block below takes quite a while to run, especially with a non-vectorized version. You can reduce the <code class="docutils literal notranslate"><span class="pre">iterations</span></code> to test your implementation and iterate faster. If you have time, run for 100,000 iterations to see better results.</p>
<p>Regularised Gradient Descent</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_7350/3281116716.py:84: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]
[0.   0.   0.01 0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.01 0.01 0.01 0.01 0.01 0.   0.01 0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.01 0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[0.   0.   0.   0.   0.   0.   0.01 0.01 0.   0.   0.01 0.01 0.01 0.
 0.   0.01 0.01 0.01 0.01 0.   0.   0.01 0.01 0.01 0.01 0.01 0.  ]
[-9.33e-05  1.40e-03  2.20e-03  4.21e-03  2.22e-03  2.82e-03  5.85e-03
  5.76e-03  1.76e-03  2.81e-03  6.51e-03  6.15e-03  6.38e-03  1.66e-03
  3.68e-03  6.78e-03  6.91e-03  6.66e-03  6.74e-03  1.83e-03  3.54e-03
  7.08e-03  6.94e-03  7.32e-03  6.77e-03  7.03e-03  1.64e-03]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.    0.01  0.01
  0.01  0.    0.    0.01  0.01  0.01  0.01  0.    0.    0.01  0.01  0.01
  0.01  0.01  0.  ]
[-2.01e-03  4.82e-04  4.12e-04  2.61e-03  1.09e-03  1.44e-03  5.01e-03
  4.80e-03  4.66e-04  1.10e-03  5.78e-03  5.42e-03  5.54e-03  1.32e-04
  2.39e-03  6.16e-03  6.38e-03  6.07e-03  6.03e-03  3.60e-04  2.03e-03
  6.56e-03  6.42e-03  6.87e-03  6.22e-03  6.41e-03  1.51e-05]
[-2.17e-03  4.23e-04  2.28e-04  2.45e-03  9.85e-04  1.32e-03  4.92e-03
  4.70e-03  3.56e-04  9.28e-04  5.70e-03  5.34e-03  5.46e-03 -9.45e-06
  2.27e-03  6.10e-03  6.32e-03  6.01e-03  5.96e-03  2.28e-04  1.88e-03
  6.50e-03  6.36e-03  6.82e-03  6.16e-03  6.34e-03 -1.35e-04]
[-2.31e-03  3.71e-04  4.65e-05  2.29e-03  8.85e-04  1.21e-03  4.83e-03
  4.61e-03  2.51e-04  7.59e-04  5.63e-03  5.27e-03  5.37e-03 -1.47e-04
  2.15e-03  6.03e-03  6.27e-03  5.95e-03  5.89e-03  1.02e-04  1.73e-03
  6.45e-03  6.30e-03  6.78e-03  6.10e-03  6.28e-03 -2.80e-04]
[-2.45e-03  3.27e-04 -1.32e-04  2.14e-03  7.87e-04  1.10e-03  4.75e-03
  4.51e-03  1.52e-04  5.94e-04  5.56e-03  5.19e-03  5.29e-03 -2.79e-04
  2.04e-03  5.97e-03  6.21e-03  5.89e-03  5.81e-03 -1.91e-05  1.59e-03
  6.40e-03  6.25e-03  6.73e-03  6.04e-03  6.22e-03 -4.21e-04]
[-2.58e-03  2.88e-04 -3.08e-04  1.98e-03  6.92e-04  9.95e-04  4.67e-03
  4.42e-03  5.69e-05  4.32e-04  5.49e-03  5.12e-03  5.20e-03 -4.08e-04
  1.94e-03  5.91e-03  6.16e-03  5.83e-03  5.74e-03 -1.35e-04  1.45e-03
  6.35e-03  6.19e-03  6.68e-03  5.99e-03  6.15e-03 -5.57e-04]
[-2.69e-03  2.55e-04 -4.82e-04  1.84e-03  6.00e-04  8.97e-04  4.59e-03
  4.33e-03 -3.31e-05  2.73e-04  5.42e-03  5.04e-03  5.12e-03 -5.33e-04
  1.83e-03  5.84e-03  6.11e-03  5.77e-03  5.67e-03 -2.47e-04  1.31e-03
  6.30e-03  6.14e-03  6.64e-03  5.93e-03  6.09e-03 -6.88e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.    0.    0.01  0.
  0.01 -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-2.90e-03  2.07e-04 -8.22e-04  1.55e-03  4.22e-04  7.17e-04  4.43e-03
  4.16e-03 -2.00e-04 -3.44e-05  5.28e-03  4.90e-03  4.96e-03 -7.72e-04
  1.64e-03  5.72e-03  6.01e-03  5.66e-03  5.53e-03 -4.58e-04  1.05e-03
  6.20e-03  6.03e-03  6.55e-03  5.82e-03  5.97e-03 -9.38e-04]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.22e-03  1.67e-04 -1.47e-03  1.00e-03  8.84e-05  4.13e-04  4.12e-03
  3.83e-03 -4.86e-04 -6.15e-04  5.03e-03  4.61e-03  4.66e-03 -1.21e-03
  1.30e-03  5.49e-03  5.82e-03  5.43e-03  5.26e-03 -8.32e-04  5.58e-04
  6.01e-03  5.82e-03  6.38e-03  5.60e-03  5.73e-03 -1.39e-03]
[-3.28e-03  1.68e-04 -1.63e-03  8.75e-04  9.12e-06  3.48e-04  4.05e-03
  3.75e-03 -5.49e-04 -7.53e-04  4.97e-03  4.54e-03  4.59e-03 -1.31e-03
  1.23e-03  5.43e-03  5.77e-03  5.38e-03  5.20e-03 -9.17e-04  4.42e-04
  5.96e-03  5.77e-03  6.34e-03  5.55e-03  5.68e-03 -1.50e-03]
[-3.33e-03  1.73e-04 -1.79e-03  7.49e-04 -6.88e-05  2.87e-04  3.98e-03
  3.68e-03 -6.09e-04 -8.89e-04  4.91e-03  4.47e-03  4.52e-03 -1.41e-03
  1.16e-03  5.37e-03  5.72e-03  5.33e-03  5.14e-03 -9.99e-04  3.28e-04
  5.92e-03  5.72e-03  6.30e-03  5.50e-03  5.62e-03 -1.60e-03]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.    0.   -0.    0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.01  0.01  0.01  0.01 -0.    0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-3.46e-03  2.06e-04 -2.24e-03  3.87e-04 -2.95e-04  1.25e-04  3.77e-03
  3.46e-03 -7.72e-04 -1.28e-03  4.74e-03  4.27e-03  4.32e-03 -1.70e-03
  9.53e-04  5.21e-03  5.59e-03  5.17e-03  4.95e-03 -1.23e-03  2.43e-06
  5.79e-03  5.57e-03  6.18e-03  5.34e-03  5.46e-03 -1.89e-03]
[-3.49e-03  2.24e-04 -2.39e-03  2.71e-04 -3.68e-04  7.81e-05  3.70e-03
  3.39e-03 -8.21e-04 -1.41e-03  4.68e-03  4.20e-03  4.25e-03 -1.79e-03
  8.91e-04  5.16e-03  5.55e-03  5.12e-03  4.89e-03 -1.30e-03 -1.02e-04
  5.74e-03  5.52e-03  6.14e-03  5.29e-03  5.41e-03 -1.99e-03]
[-3.52e-03  2.44e-04 -2.54e-03  1.58e-04 -4.40e-04  3.39e-05  3.63e-03
  3.32e-03 -8.68e-04 -1.53e-03  4.63e-03  4.14e-03  4.19e-03 -1.88e-03
  8.32e-04  5.10e-03  5.51e-03  5.07e-03  4.83e-03 -1.37e-03 -2.04e-04
  5.70e-03  5.47e-03  6.10e-03  5.24e-03  5.36e-03 -2.08e-03]
[-3.54e-03  2.67e-04 -2.68e-03  4.64e-05 -5.11e-04 -7.22e-06  3.57e-03
  3.25e-03 -9.12e-04 -1.65e-03  4.57e-03  4.07e-03  4.12e-03 -1.97e-03
  7.75e-04  5.05e-03  5.46e-03  5.02e-03  4.78e-03 -1.43e-03 -3.04e-04
  5.66e-03  5.42e-03  6.06e-03  5.19e-03  5.31e-03 -2.16e-03]
[-3.55e-03  2.92e-04 -2.82e-03 -6.24e-05 -5.82e-04 -4.56e-05  3.50e-03
  3.19e-03 -9.54e-04 -1.77e-03  4.52e-03  4.01e-03  4.06e-03 -2.05e-03
  7.20e-04  5.00e-03  5.42e-03  4.97e-03  4.72e-03 -1.50e-03 -4.01e-04
  5.62e-03  5.38e-03  6.03e-03  5.14e-03  5.26e-03 -2.25e-03]
[-3.57e-03  3.19e-04 -2.96e-03 -1.69e-04 -6.51e-04 -8.13e-05  3.44e-03
  3.12e-03 -9.94e-04 -1.89e-03  4.47e-03  3.94e-03  4.00e-03 -2.14e-03
  6.67e-04  4.95e-03  5.38e-03  4.92e-03  4.66e-03 -1.56e-03 -4.97e-04
  5.58e-03  5.33e-03  5.99e-03  5.10e-03  5.21e-03 -2.33e-03]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.01  0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.01 -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.01  0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.01  0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.   -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.01  0.    0.01
  0.    0.   -0.  ]
[-3.36e-03  9.29e-04 -4.89e-03 -1.54e-03 -1.61e-03 -3.75e-04  2.58e-03
  2.28e-03 -1.41e-03 -3.45e-03  3.79e-03  3.04e-03  3.19e-03 -3.22e-03
  7.68e-05  4.24e-03  4.83e-03  4.24e-03  3.90e-03 -2.28e-03 -1.75e-03
  5.04e-03  4.65e-03  5.47e-03  4.41e-03  4.55e-03 -3.38e-03]
[-3.32e-03  9.78e-04 -5.01e-03 -1.62e-03 -1.67e-03 -3.83e-04  2.53e-03
  2.23e-03 -1.43e-03 -3.54e-03  3.75e-03  2.98e-03  3.15e-03 -3.28e-03
  4.77e-05  4.20e-03  4.80e-03  4.19e-03  3.85e-03 -2.32e-03 -1.82e-03
  5.01e-03  4.61e-03  5.44e-03  4.37e-03  4.51e-03 -3.43e-03]
[-3.29e-03  1.03e-03 -5.13e-03 -1.70e-03 -1.73e-03 -3.89e-04  2.48e-03
  2.18e-03 -1.44e-03 -3.64e-03  3.71e-03  2.93e-03  3.10e-03 -3.34e-03
  1.96e-05  4.16e-03  4.76e-03  4.15e-03  3.81e-03 -2.35e-03 -1.89e-03
  4.98e-03  4.57e-03  5.41e-03  4.33e-03  4.47e-03 -3.49e-03]
[-3.25e-03  1.08e-03 -5.24e-03 -1.78e-03 -1.79e-03 -3.95e-04  2.43e-03
  2.14e-03 -1.46e-03 -3.73e-03  3.67e-03  2.87e-03  3.05e-03 -3.40e-03
 -7.66e-06  4.12e-03  4.73e-03  4.11e-03  3.76e-03 -2.39e-03 -1.96e-03
  4.95e-03  4.53e-03  5.38e-03  4.28e-03  4.43e-03 -3.55e-03]
[-3.22e-03  1.13e-03 -5.35e-03 -1.85e-03 -1.85e-03 -3.99e-04  2.38e-03
  2.09e-03 -1.47e-03 -3.81e-03  3.63e-03  2.82e-03  3.01e-03 -3.47e-03
 -3.40e-05  4.07e-03  4.70e-03  4.07e-03  3.72e-03 -2.42e-03 -2.03e-03
  4.92e-03  4.49e-03  5.35e-03  4.24e-03  4.39e-03 -3.60e-03]
[-3.18e-03  1.18e-03 -5.47e-03 -1.93e-03 -1.90e-03 -4.03e-04  2.33e-03
  2.04e-03 -1.49e-03 -3.90e-03  3.60e-03  2.76e-03  2.96e-03 -3.52e-03
 -5.95e-05  4.03e-03  4.67e-03  4.03e-03  3.68e-03 -2.46e-03 -2.10e-03
  4.89e-03  4.45e-03  5.32e-03  4.20e-03  4.35e-03 -3.66e-03]
[-3.14e-03  1.24e-03 -5.58e-03 -2.00e-03 -1.96e-03 -4.06e-04  2.28e-03
  2.00e-03 -1.50e-03 -3.99e-03  3.56e-03  2.71e-03  2.92e-03 -3.58e-03
 -8.42e-05  3.99e-03  4.63e-03  3.99e-03  3.63e-03 -2.49e-03 -2.17e-03
  4.85e-03  4.41e-03  5.29e-03  4.16e-03  4.32e-03 -3.71e-03]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.    0.   -0.01 -0.   -0.   -0.    0.    0.   -0.   -0.    0.    0.
  0.   -0.   -0.    0.    0.    0.    0.   -0.   -0.    0.    0.    0.01
  0.    0.   -0.  ]
[-0.34  0.18 -0.73 -0.28 -0.27 -0.05  0.24  0.2  -0.19 -0.53  0.4   0.28
  0.31 -0.46 -0.03  0.44  0.53  0.44  0.4  -0.32 -0.3   0.55  0.49  0.6
  0.46  0.48 -0.47] 0.3499985218344349 260.61872511912435
[[ 5.13e-02  7.00e-01  2.63e-03 ...  6.29e-04  8.59e-03  1.17e-01]
 [-9.27e-02  6.85e-01  8.60e-03 ...  1.89e-03 -1.40e-02  1.03e-01]
 [-2.14e-01  6.92e-01  4.57e-02 ...  1.05e-02 -3.40e-02  1.10e-01]
 ...
 [-4.84e-01  9.99e-01  2.35e-01 ...  2.34e-01 -4.83e-01  9.96e-01]
 [-6.34e-03  9.99e-01  4.01e-05 ...  4.00e-05 -6.31e-03  9.96e-01]
 [ 6.33e-01 -3.06e-02  4.00e-01 ...  3.51e-07 -1.70e-08  8.23e-10]] [0.51 0.53 0.52 0.59 0.58 0.58 0.59 0.58 0.56 0.54 0.48 0.44 0.41 0.35
 0.5  0.53 0.55 0.44 0.52 0.54 0.53 0.56 0.56 0.56 0.56 0.51 0.46 0.36
 0.58 0.46 0.26 0.45 0.54 0.59 0.6  0.6  0.58 0.58 0.57 0.53 0.54 0.48
 0.41 0.5  0.45 0.48 0.46 0.55 0.51 0.51 0.55 0.59 0.57 0.6  0.6  0.6
 0.59 0.57 0.34 0.47 0.53 0.5  0.52 0.51 0.47 0.35 0.37 0.22 0.22 0.26
 0.22 0.24 0.24 0.27 0.37 0.44 0.48 0.49 0.46 0.49 0.52 0.56 0.54 0.56
 0.47 0.53 0.51 0.45 0.51 0.3  0.3  0.49 0.47 0.34 0.23 0.1  0.2  0.47
 0.54 0.54 0.52 0.13 0.54 0.52 0.57 0.55 0.57 0.59 0.55 0.51 0.5  0.5
 0.47 0.5  0.56 0.18 0.24 0.43] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e8b160ad4a674fac9aeeb8e1b5c3bd1d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(27,)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[85]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize fitting parameters</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">initial_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X_mapped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Set regularization parameter lambda_ to 1 (you can try varying this)</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="c1"># Some gradient descent settings</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="n">J_history</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">initial_w</span><span class="p">,</span> <span class="n">initial_b</span><span class="p">,</span>
                                    <span class="n">compute_cost_reg</span><span class="p">,</span> <span class="n">compute_gradient_reg</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div></div>
</div>
<details><p>Expected Output: Cost &lt; 0.5 (Click for details)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Using the following settings
#np.random.seed(1)
#initial_w = np.random.rand(X_mapped.shape[1])-0.5
#initial_b = 1.
#lambda_ = 0.01;
#iterations = 10000
#alpha = 0.01
Iteration    0: Cost     0.72
Iteration 1000: Cost     0.59
Iteration 2000: Cost     0.56
Iteration 3000: Cost     0.53
Iteration 4000: Cost     0.51
Iteration 5000: Cost     0.50
Iteration 6000: Cost     0.48
Iteration 7000: Cost     0.47
Iteration 8000: Cost     0.46
Iteration 9000: Cost     0.45
Iteration 9999: Cost     0.45
</pre></div>
</div>
</section>
<section id="id22">
<h2>3.7 Plotting the decision boundary<a class="headerlink" href="#id22" title="Permalink to this headline"></a></h2>
<p>To help you visualize the model learned by this classifier, we will use our <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary</span></code> function which plots the (non-linear) decision boundary that separates the positive and negative examples.</p>
<ul class="simple">
<li><p>In the function, we plotted the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then drew a contour plot of where the predictions change from y = 0 to y = 1.</p></li>
<li><p>After learning the parameters <span class="math notranslate nohighlight">\(w\)</span>,<span class="math notranslate nohighlight">\(b\)</span>, the next step is to plot a decision boundary similar to Figure 4.</p></li>
</ul>
<p><img alt="aed5eea361ae47acb4b3761ebc8e063c" class="no-scaled-link" src="source_files/Supervised_Machine_Learning_Regression_and_Classification/week3/C1W3A1/images/figure4.png" style="width: 450px; height: 450px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[111]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_mapped</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[5.50e-04 9.54e-04 1.55e-03 ... 8.74e-12 1.50e-13 1.42e-15]
 [8.40e-04 1.47e-03 2.42e-03 ... 2.46e-11 4.51e-13 4.57e-15]
 [1.21e-03 2.14e-03 3.55e-03 ... 6.07e-11 1.18e-12 1.28e-14]
 ...
 [6.07e-11 1.68e-10 3.98e-10 ... 4.08e-22 8.97e-24 1.24e-25]
 [9.91e-13 2.80e-12 6.75e-12 ... 2.68e-24 5.64e-26 7.42e-28]
 [8.07e-15 2.33e-14 5.73e-14 ... 8.60e-27 1.72e-28 2.16e-30]] (50, 50)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec69572a491941fc866b2028c415a2e7", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[110]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sig</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Credit to dibgerge on Github for this plotting code</span>

    <span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HI&quot;</span><span class="p">)</span>
        <span class="n">plot_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])])</span>
        <span class="n">plot_y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">plot_x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)))</span>

        <span class="c1"># Evaluate z = theta*x over the grid</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)):</span>
                <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

        <span class="c1"># important to transpose z before calling contour</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># Plot z = 0</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="id23">
<h2>3.8 Evaluating regularized logistic regression model<a class="headerlink" href="#id23" title="Permalink to this headline"></a></h2>
<p>You will use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[87]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Compute accuracy on the training set</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_mapped</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 82.203390
</pre></div></div>
</div>
<p><strong>Expected Output</strong>:</p>
<table><tr><td><p>Train Accuracy:~ 80%</p>
</td></tr></table></section>
<section id="id24">
<h2>My Solution<a class="headerlink" href="#id24" title="Permalink to this headline"></a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/OptionalLabs&quot;</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;week3/OptionalLabs/deeplearning.mplstyle&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">plt_overfit</span> <span class="kn">import</span> <span class="n">overfit_example</span><span class="p">,</span> <span class="n">output</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s2">&quot;week3/C1W3A1/data/ex2data2.txt&quot;</span><span class="p">)</span>
<span class="n">x_train</span><span class="o">=</span> <span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">sigmoid</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">!=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of W and X dosn&#39;t match&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">()</span>
        <span class="n">sigmoid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">sigmoid</span>

<span class="k">def</span> <span class="nf">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dmodel_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="n">w</span>

<span class="k">def</span> <span class="nf">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">cf</span><span class="o">=</span>  <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">dcost_w_result</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">wi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)):</span>
        <span class="n">dcost_w_result</span><span class="p">[</span><span class="n">wi</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)[:,</span><span class="n">wi</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span>  <span class="n">dcost_w_result</span>

<span class="k">def</span> <span class="nf">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">w</span>

<span class="k">def</span> <span class="nf">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">dmodel_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">dcost_b</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dcost_w_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gradient_decent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam</span><span class="p">,</span><span class="n">niter</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">theta</span>
    <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">cost_i</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">niter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.05</span><span class="p">:</span>
                <span class="n">alpha</span><span class="o">/=</span><span class="mi">2</span>
        <span class="n">dcw</span><span class="p">,</span><span class="n">dcb</span><span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">dcw_reg</span><span class="o">=</span> <span class="n">compute_gradient_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="c1">#print(dcw_reg)</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">dcw</span><span class="o">+</span><span class="n">dcw_reg</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="n">alpha</span><span class="o">*</span><span class="n">dcb</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">w</span><span class="p">,</span><span class="n">b</span>
        <span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">cost_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">lam</span><span class="p">)</span>
        <span class="c1">#if i&gt;1:</span>
        <span class="c1">#    if cost_i[i]&gt;cost_i[i-1]:</span>
        <span class="c1">#        alpha/=2</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">50</span>==0:
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The cost is&quot;</span><span class="p">,</span><span class="n">cost_i</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1">#print(theta)</span>
    <span class="k">return</span> <span class="n">cost_i</span><span class="p">,</span><span class="n">theta</span>



<span class="n">niter</span><span class="o">=</span><span class="mi">1000</span>
<span class="n">Win</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Bin</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">lam_in</span><span class="o">=</span><span class="mf">1.</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">theta_in</span><span class="o">=</span><span class="n">Win</span><span class="p">,</span><span class="n">Bin</span>
<span class="n">grad_dec_result</span><span class="p">,</span><span class="n">theta_f</span><span class="o">=</span><span class="n">gradient_decent</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">theta_in</span><span class="p">,</span><span class="n">alpha</span><span class="p">,</span><span class="n">lam_in</span><span class="p">,</span><span class="n">niter</span><span class="p">)</span>

<span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="o">=</span><span class="n">theta_f</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wf</span><span class="p">,</span><span class="n">bf</span><span class="p">,</span><span class="n">grad_dec_result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>





<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">niter</span><span class="p">),</span><span class="n">grad_dec_result</span><span class="p">,</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;No of steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>



<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">y_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">z_plot</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">y_plot</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_plot</span><span class="p">)):</span>
        <span class="n">z_plot</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">x_plot</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">y_plot</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span><span class="n">y_plot</span><span class="p">,</span><span class="n">z_plot</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
<span class="c1">#plt.plot(x_train, model(x_train,theta_f), c = &quot;g&quot;,label=&quot;Predcited model&quot;)</span>
<span class="c1">#ax.plot((-bf/wf[0],0),(0,-bf/wf[1]),label=&quot;Predicted model&quot;)</span>
<span class="n">pos</span><span class="o">=</span><span class="n">y_train</span><span class="o">&gt;</span><span class="mf">0.5</span>
<span class="n">neg</span><span class="o">=</span><span class="n">y_train</span><span class="o">&lt;</span><span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">pos</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">pos</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">][</span><span class="n">neg</span><span class="p">],</span><span class="n">x_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">][</span><span class="n">neg</span><span class="p">]</span> <span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_0$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">0.1</span><span class="p">,</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mf">0.1</span> <span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mf">0.1</span><span class="p">])</span>
<span class="c1"># Set the title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model fit&quot;</span><span class="p">)</span>
<span class="c1"># Set the y-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;training data&#39;</span><span class="p">)</span>
<span class="c1"># Set the x-axis label</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">z_predict</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])):</span>
    <span class="n">z_predict</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">map_feature</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">+</span><span class="n">theta_f</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z_predict</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">theta_f</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 2.0028840493199764
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/tmp/ipykernel_6366/2829149365.py:82: RuntimeWarning: divide by zero encountered in scalar divide
  if np.abs((cost_i[i]-cost_i[i-1])/cost_i[i])&lt;0.05:
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The cost is 0.6893248648821225
The cost is 0.5947327181536963
The cost is 0.5594964140290553
The cost is 0.5439204525169686
The cost is 0.5365941656209957
The cost is 0.5329892209020973
The cost is 0.5311518234465994
The cost is 0.5301878305603884
The cost is 0.5296693737410955
The cost is 0.5293843854020753
The cost is 0.5292246537838604
The cost is 0.5291335540683393
The cost is 0.5290807827627604
The cost is 0.5290497884840468
The cost is 0.5290313609587369
The cost is 0.5290202869125287
The cost is 0.5290135693988894
The cost is 0.5290094612865763
The cost is 0.5290069311969307
[ 0.62  1.18 -2.02 -0.92 -1.43  0.13 -0.37 -0.36 -0.17 -1.46 -0.05 -0.61
 -0.27 -1.19 -0.24 -0.2  -0.04 -0.27 -0.29 -0.46 -1.04  0.03 -0.28  0.02
 -0.32 -0.14 -0.93] 1.2715618096527268 0.5290053880660263
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train Accuracy: 0.000000
(array([ 0.62,  1.18, -2.02, -0.92, -1.43,  0.13, -0.37, -0.36, -0.17,
       -1.46, -0.05, -0.61, -0.27, -1.19, -0.24, -0.2 , -0.04, -0.27,
       -0.29, -0.46, -1.04,  0.03, -0.28,  0.02, -0.32, -0.14, -0.93]), 1.2715618096527268)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "14a9467b4ad24181b1016046645ee3f1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Andrew Ng.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>